{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS CELL WHICH CUSTOMIZES LAYOUT AND STYLING OF THE NOTEBOOK !\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings = lambda *a, **kw: None\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(open(\"../documents/custom.html\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<span style=\"background:#f0f0e0;padding:1em\">Copyright (c) 2020-2022 ETH Zurich, Scientific IT Services. This work is licensed under <a href=\"https://creativecommons.org/licenses/by-nc/4.0/\">CC BY-NC 4.0</a></span><br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 2.5em; font-weight: bold;\">Section 6b: Parallel Computing on an HPC Cluster</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pythonic solutions for Scaling Up and Out\n",
    "\n",
    "<p>\n",
    "<img src=\"./images/Scale_Up_Out.png\" width=\"640\">\n",
    "</p>\n",
    "\n",
    "**Main Idea**: The general strategy for scaling in a sustainable way is to develop on a laptop (several cores) and deploy on an HPC cluster. \n",
    "\n",
    "On one hand we can run the PC code directly **locally** on a single node of the HPC cluster but on a bigger node, which is **scaled up**.  \n",
    "\n",
    "On the other hand there are several mature solutions that allow us to start on a PC and also run **distributed** over more nodes of the cluster, which is called **to scale out**:\n",
    "- [IPython Parallel](https://ipyparallel.readthedocs.io/en/latest/) - `ipyparallel`\n",
    "    - mature solution that provides a high flexibility\n",
    "- [Dask](https://docs.dask.org/en/latest/) - `dask`\n",
    "    - newer solution that is focused around tasks and comes with batteries included (e.g. interface for NumPy, Pandas, scikit-learn, ...)\n",
    "- [MPI for Python](https://mpi4py.readthedocs.io/en/stable/) - `mpi4py`  (not covered in this course)\n",
    "    - for HPC-heavy users \n",
    "- [mpipool](https://github.com/mpipool/mpipool) - `mpipool` (not covered in this course)\n",
    "    - a wrapper of `mpi4py` that mimics the interface provided by the multiprocessing \"pools\" available in the Python standard library, i.e. `concurrent.futures.ProcessPoolExecutor` and `multiprocessing.Pool`\n",
    "- [Ray](https://www.ray.io/) - `ray` (not covered in this course)\n",
    "    - a tool focused mainly on machine learning tasks\n",
    "    \n",
    "[MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) stands for Message Passing Interface. It is the standard that defines the API (in C and Fortran) and the semantics for the communication between processes and it is used by many applications able to scale out from a PC to supercomputers; especially if the underlying processing cannot easily be split into independent tasks. There are several implementations, e.g. [Open MPI](https://www.open-mpi.org/), [MPICH](https://www.mpich.org/). For further information on MPI on Euler, see the [scicomp wiki](https://scicomp.ethz.ch/wiki/MPI_on_Euler)\n",
    "\n",
    "Alternatively one can use a workflow management system that has HPC integration. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scaling Up - Using PC code on an HPC Cluster Node\n",
    "\n",
    "In order to run our code developed on our PC to the HPC cluster and profit from scaling up we should make sure that:\n",
    "1. the computing environment is compatible,\n",
    "2. the job is submitted to only one node.\n",
    "\n",
    "Compatible **computing environment** implies compatible:\n",
    "- platform (x86-64 on Euler) \n",
    "- operating system (Linux on Euler)\n",
    "- system applications\n",
    "- Python version \n",
    "- Python packages \n",
    "\n",
    "The platform and the operating system are given in Euler and there is not so much flexibility. \n",
    "The system applications, Python version and packages provide more flexibility and we can achieve a good compatibility by using ([containers](https://scicomp.ethz.ch/wiki/Singularity) are also an option):\n",
    "- on **our PC** a **Python version** available on Euler modules (e.g. Python 3.8, 3.9, or 3.10),\n",
    "- **virtual environments** `venv` on both our PC and Euler, where we can install the desired Python packages.\n",
    "\n",
    "In case important applications or system libraries are missing on Euler, please contact the HPC team (cluster-support@id.ethz.ch).\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Warning!</strong>\n",
    "    \n",
    "  conda environments [are not recommended by the HPC team](https://scicomp.ethz.ch/wiki/Conda), since these environments consist of **many** small files which put pressure on the parallel file system.\n",
    "</div>\n",
    "\n",
    "Next, one has to make sure that the submitted job to LSF is scheduled to a **single node**.  \n",
    "This can be guaranteed using `--nodes 1`. At the moment, the maximum number of cores per node is 128 on Euler VI and Euler VII (please check https://scicomp.ethz.ch/wiki/Euler).  \n",
    "In case we need 32 cores on a single node we can request them using:\n",
    "\n",
    "```bash\n",
    "# 32 cores on the same node\n",
    "$ sbatch -n 32 --nodes 1 --wrap \"COMMAND TO EXECUTE\"\n",
    "```\n",
    "\n",
    "We would need more than one core only if our code can profit from parallelization. Otherwise we should ask for only one core `-n 1`. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Warning!</strong> \n",
    "  \n",
    "  `os.cpu_count()` or `multiprocessing.cpu_count()` provides the number of cores available on the **node**, and **not** the ones available to the **submitted job**. Therefore, for a small number of cores, we will encounter a high oversubscription.</br>\n",
    "    We should use the environment variable `SLURM_NTASKS` which provides the number of processors allocated to the job (the number after `sbatch -n`).\n",
    "</div>\n",
    "\n",
    "**Select a CPU model**\n",
    "\n",
    "In case we want a specific node, we can request it using `--constraint=model_name`, where the models corresponding to the nodes are:\n",
    "- Euler IV: `XeonGold_6150`\n",
    "- Euler V: `XeonGold_5118`\n",
    "- Euler VI: `EPYC_7742`\n",
    "- Euler VII: `EPYC_7H12`\n",
    "\n",
    "```bash\n",
    "# 12 cores on a single node on Euler VI\n",
    "$ sbatch -n 12 --constraint=EPYC_7742 --nodes 1 --wrap ....\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Scaling Up \"Compute Pi\" on Euler (Hands-on) [10min]\n",
    "\n",
    "**You should replace `<...>` with something sensible.**\n",
    "\n",
    "In this exercise we will run the \"Compute Pi\" exercise from Section 5, where \n",
    "\n",
    "```python\n",
    "num_workers = int(os.environ.get(\"SLURM_NTASKS\", os.cpu_count()))\n",
    "```\n",
    "\n",
    "that is, if `SLURM_NTASKS` environment variable is available it is used, otherwise `os.cpu_count()` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycat euler_scripts/pi_scale_up_hpc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Login to the cluster and load the new environment system\n",
    "\n",
    "```bash\n",
    "$ ssh <username>@euler.ethz.ch\n",
    "\n",
    "# on the cluster\n",
    "$ env2lmod\n",
    "```\n",
    "\n",
    "2. Load the fast-python module and activate the corresponding virtual environment\n",
    "\n",
    "```bash\n",
    "$ module load gcc/8.2.0 fast_python_workshop_cpu/2022.1.0\n",
    "$ venv_cpu_init\n",
    "```\n",
    "\n",
    "3. Identify `~/euler_scripts/pi_scale_up_hpc.py` and submit it `sbatch --wrap ...` option to run on 1 and 3 cores \n",
    "\n",
    "```bash\n",
    "$ sbatch <...> \"python ~/euler_scripts/pi_scale_up_hpc.py\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "**Solution**\n",
    "\n",
    "```bash\n",
    "## 1 core (-n 1 is optional)\n",
    "$ sbatch -n 1 --nodes 1 --time 00:02:00 --wrap \"python ~/euler_scripts/pi_scale_up_hpc.py\"\n",
    "\n",
    "## 3 cores (don't forget --nodes 1)\n",
    "$ sbatch -n 3 --nodes 1 --time 00:02:00 --wrap python \"~/euler_scripts/pi_scale_up_hpc.py\"\n",
    "\n",
    "## Optional: What happens if we use -n 4 --ntasks-per-node 2\n",
    "$ sbatch -n 4 --ntasks-per-node 2 --time 00:02:00 --wrap \"python ~/euler_scripts/pi_scale_up_hpc.py\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scaling Out - General Considerations\n",
    "\n",
    "Scaling out means using multiple nodes at the same time.  \n",
    "Even using efficiently several cores on one node can be challenging, so dealing with multiple nodes can be **demanding**.  \n",
    "So, before starting to scale out our code we should check carefully whether we really have to do it.   \n",
    "Moreover we should keep in mind the message from Amdahl's and Gustafson's laws and therefore focus on the weak scaling case, i.e. **increase the number of processors together with the problem size**. \n",
    "\n",
    "Again the **computing environment** has to be compatible on all nodes. On Euler cluster this is done automatically, so we have to follow the advice given in the previous subsection.\n",
    "\n",
    "The solutions that we will present next - IPython Parallel and Dask - have a similar pattern for the parallelism called **Manager-Workers** approach:\n",
    "- the **Manager** is performing some initial calculation and **schedules** work to the workers,\n",
    "- the **Workers** are doing the work.\n",
    "\n",
    "![](./images/manager_worker.svg)\n",
    "\n",
    "One key thing that we will have to do when scaling out is to **share data** between nodes (at least between the manager and the worker).\n",
    "\n",
    "There are two main ways of sharing data between two nodes:\n",
    "- via the network,\n",
    "- using the parallel file system (write to a file from one node and read from the file from another node).\n",
    "\n",
    "Using the network is generally faster (see below), so it is highly recommended to use it (or to use a library that relies on it).\n",
    "\n",
    "Once we have the code ready, we should test it on a **small test job**, e.g. submit to 2-3 nodes, with 1-2 cores per node.  \n",
    "This can be achieved using `--nodes\"` option:\n",
    "\n",
    "```bash\n",
    "# 9 cores, 3 cores per node\n",
    "$ sbatch -n 9 --nodes 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Recap: Latency Comparison Numbers (~2012)\n",
    "\n",
    "```\n",
    "                                             Real time             \n",
    "L1 cache reference ......................... 0.5 ns                \n",
    "Execute typical instruction ................   1 ns                \n",
    "L2 cache reference ........................... 7 ns                \n",
    "Main memory reference ...................... 100 ns                \n",
    "Send 2K bytes over 1 Gbps network ....... 20,000 ns  =  20 µs      \n",
    "SSD random read ........................ 150,000 ns  = 150 µs      \n",
    "Read 1 MB sequentially from memory ..... 250,000 ns  = 250 µs      \n",
    "Read 1 MB sequentially from SSD  ..... 1,000,000 ns  =   1 ms      \n",
    "Send packet CA->Netherlands->CA .... 150,000,000 ns  = 150 ms      \n",
    "```\n",
    "\n",
    "Source: https://gist.github.com/jboner/2841832"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Out - IPython Parallel\n",
    "\n",
    "[IPython Parallel](https://ipyparallel.readthedocs.io/en/latest/intro.html) is a mature library, formally part of the IPython package, that\n",
    "> enables all types of parallel applications to be developed, executed, debugged, and monitored interactively. Hence, the `I` in IPython. \n",
    "\n",
    "**Note**: We will focus on **programmatic ways** of using IPython Parallel since they allow us to scale out to Euler and run batch jobs.  \n",
    "**Note**: We will use new features of IPython Parallel available starting from **version 7**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo IPython Parallel \"Hello World\" on multi-node\n",
    "We will test a simple script `euler_scripts/ipyparallel_demo.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycat euler_scripts/ipyparallel_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To run on Euler**\n",
    "\n",
    "```bash\n",
    "$ env2lmod\n",
    "$ module load gcc/8.2.0 fast_python_workshop_cpu/2022.1.0\n",
    "$ venv_cpu_init\n",
    "$ sbatch -n 2 --nodes 2 --wrap \"python ~/euler_scripts/ipyparallel_demo.py\"\n",
    "$ less slurm-....out\n",
    "\n",
    "INFO:ipyparallel.cluster.cluster.1665571093-7pqt:Starting 2 engines with <class 'ipyparallel.cluster.launcher.MPIEngineSetLauncher'>\n",
    "INFO:ipyparallel.cluster.cluster.1665571093-7pqt:Stopping controller\n",
    "INFO:ipyparallel.cluster.cluster.1665571093-7pqt:Stopping engine(s): 1665571094\n",
    "[{'cwd': '/cluster/home/chadhat',\n",
    "  'hostname': 'eu-a2p-278',\n",
    "  'pid': 88425,\n",
    "  'python': '3.10.4 (main, May 30 2022, 08:01:42) [GCC 8.2.0]'},\n",
    " {'cwd': '/cluster/home/chadhat',\n",
    "  'hostname': 'eu-a2p-279',\n",
    "  'pid': 1664,\n",
    "  'python': '3.10.4 (main, May 30 2022, 08:01:42) [GCC 8.2.0]'}]\n",
    "```\n",
    "\n",
    "By checking the **`hostname`** we notice the job was running on 2 different nodes with the hostnames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "IPython Parallel relies on an **IPython Cluster** (which follows the **Manager-Workers** pattern) consists of: \n",
    "- **IPython Engine (Workers)**:  kernel(s) able to run Python code enabling therefore parallel and distributed computing (each kernel is a **process**). The engines cannot communicate with each other to share data.\n",
    "- **IPython Controller (Manager)**: an interface and single point of contact for working with the engine(s)\n",
    "    - **IPython Hub**: keeps track of everything (engine connection, schedulers, clients, task requests and results)\n",
    "    - **IPython Schedulers**: all actions that run on an engine go through it, and it provides a non-blocking interface to the engines, i.e. it returns immediately\n",
    "\n",
    "The connection to the cluster is done via the object `Client`. For the execution a `View` is used:\n",
    " - `DirectView` class for explicit access\n",
    " - `LoadBalancedView` class for a `pool`-like interface \n",
    "<p>\n",
    "<img src=\"./images/ipython_parallel.png\" width=\"400\">\n",
    "<div>Source: <a href=https://ipyparallel.readthedocs.io/en/latest/tutorial/intro.html#architecture-overview>IPython Parallel - Architecture Overview</a></div>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### IPython Cluster on PC and Euler\n",
    "\n",
    "We can use IPython clusters both on on our PC and on Euler.\n",
    "\n",
    "The main actions that we can perform are:\n",
    "- configure,\n",
    "- start,\n",
    "- stop.\n",
    "\n",
    "JupyterLab and Jupyter Notebook provide extensions to create it using the graphical user interface.  \n",
    "IPython Parallel provides a command line interface to start a cluster.\n",
    "\n",
    "However, next we will focus on the programmatic approach introduced in version 7 that can be done directly in a Python script or Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure the IPython Cluster on our PC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "\n",
    "print(f\"{ipp.version_info=}\")\n",
    "\n",
    "n_engines = 3\n",
    "\n",
    "## Configure\n",
    "cluster = ipp.Cluster(n=n_engines)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- by convention `ipyparallel` is imported as `ipp`\n",
    "- we define the IPython Cluster that we want to start, and it is good practice to already provide the desired number of engines, e.g. `n=3`.\n",
    "\n",
    "So far we configured the cluster, and as we can see it has an auto-generated identifiers named `cluster_id` and a predefined `profile` called `default`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Configure the IPython Cluster on Euler**\n",
    "\n",
    "```python\n",
    "import ipyparallel as ipp\n",
    "\n",
    "n_engines = int(os.environ.get(\"SLURM_NTASKS\", os.cpu_count()))\n",
    "\n",
    "cluster = ipp.Cluster(\n",
    "    n=n_engines,\n",
    "    controller_ip=\"*\",\n",
    "    engine_launcher_class=\"MPI\",\n",
    "    location=\"server.local\",\n",
    ")\n",
    "```\n",
    "\n",
    "The arguments `controller_ip=\"*\"`, `engine_launcher_class=\"MPI\"`, and `location=\"server.local\"` are needed to run on Euler on multiple nodes.  \n",
    "It requires the [`mpi4py` package](https://mpi4py.readthedocs.io/en/stable/) and an MPI implementation, e.g. the `openmpi/4.1.4` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start the IPython Cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start\n",
    "cluster.start_cluster_sync()\n",
    "\n",
    "## Connect a client\n",
    "client = cluster.connect_client_sync()\n",
    "\n",
    "## Make sure all engines are connected\n",
    "client.wait_for_engines(n=n_engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop the IPython Cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stop the cluster\n",
    "cluster.stop_cluster_sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Direct Interface - `DirectView` 1/2\n",
    "\n",
    "The direct interface is used to provide us direct access to the engines - the workhorse of our IPyparallel cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "\n",
    "n_engines = 3\n",
    "\n",
    "## Configure\n",
    "cluster = ipp.Cluster(n=n_engines)\n",
    "\n",
    "## Start\n",
    "cluster.start_cluster_sync()\n",
    "\n",
    "## Connect a client\n",
    "client = cluster.connect_client_sync()\n",
    "\n",
    "## Make sure all engines are connected\n",
    "client.wait_for_engines(n=n_engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`client.block = True` will make sure that all calls will block until all engines are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.block = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **engines** are identified by integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can create a `DirectView` using the **list access** to the client.\n",
    "This can be done on a given engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DirectView associated with a given engine, e.g. 0\n",
    "client[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even with all engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview = client[:]\n",
    "dview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calling functions** - `apply()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DirectView is similar to the `ProcessPoolExecutor` from the `concurrent.futures`.   \n",
    "We can use it to call functions, and in that case each engine that is associated with the view will call it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a self-contained function that we will run both locally and on engines.\n",
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Warning!</strong> \n",
    "  The engines should run the imports as well!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(secs=1):\n",
    "    import os\n",
    "    import sys\n",
    "    import time\n",
    "\n",
    "    time.sleep(secs)\n",
    "    print(f\"I slept for {secs} seconds\")\n",
    "    return secs\n",
    "\n",
    "    return {\n",
    "        \"cwd\": os.getcwd(),\n",
    "        \"python\": sys.version,\n",
    "        \"hostname\": os.uname().nodename,\n",
    "        \"pid\": os.getpid(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the function **locally**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or on **engine 0** (using the list access) and `apply` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client[0].apply(summary, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the value is returned.\n",
    "\n",
    "We can call a generic function `f(*args,**kwargs)` using `view.apply(f, *args, **kwargs)`. \n",
    "\n",
    "We can call it on all engines with the help of the previously defined `dview = client[:]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview.apply(summary, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the values are returned in a list, where for each engine the corresponding value is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we run with `block = False`, it immediately returns an `AsyncResult` - a `concurrent.futures.Future` subclass.  \n",
    "So we can reuse the methods that we learned in the previous section, e.g. `done()` or `result()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview.block = False\n",
    "result = dview.apply(summary, 4)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.done()  ## or .ready() only for IPython Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.result()  ## or .get() only for IPython Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the engine output is missing. In order to get it we can use `.display_outputs()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.display_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview.block = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallel `map()`**  \n",
    "\n",
    "The DirectView `map()` method can be used to parallelize the `map()` built-in function, where the order of the gathered results is kept by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview.map(lambda x: x**2, range(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, for `block=True` the result of the `map` is returned as a list of values, which is similar to the `map` from `concurrent.futures`.  \n",
    "However, for non-blocking views, an `AsyncMapResult` is returned, which is an iterable of `AsyncResult`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to check more carefully what `map` is actually doing. We define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def sleeping(secs, time_start):\n",
    "    import time\n",
    "\n",
    "    time.sleep(secs)\n",
    "    return (secs, int(time.time() - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "%time dview.map(sleeping, range(8,0,-1),8*[time_start])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/Direct_Interface.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this is not the most efficient way of running these tasks.  \n",
    "The **scheduler** is scattering the data on the engines, running the function, and the results are returned back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can monitor the progress (done tasks) of the non-blocking case using the client method `wait_interactive`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview.map(sleeping, range(8, 0, -1), 8 * [time_start], block=False)\n",
    "client.wait_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can we do it better?**\n",
    "\n",
    "IPython Parallel provides a way to schedule such tasks in a better way via the `LoadBalancedView`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task interface - `LoadBalancedView`\n",
    "\n",
    "The task interface can be used to have dynamic load balancing and is constructed from the client view using the `load_balanced_view()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lview = client.load_balanced_view()\n",
    "lview.block = True\n",
    "lview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be used for the parallelized version of `map()`.  \n",
    "It is recommended for functions where the execution time per item varies significantly.  \n",
    "It should be preferred compared to the direct view because the tasks are **dynamically load balanced**.  \n",
    "The IPython Scheduler, which is part of the IPython Controller, is responsible for assigning the jobs. The default scheme used to schedule the jobs is called **least load** and assigns tasks to the engine with the fewest outstanding tasks. By default only one task can be outstanding on each engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def sleeping(secs, time_start):\n",
    "    import time\n",
    "\n",
    "    time.sleep(secs)\n",
    "    return (secs, int(time.time() - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "%time lview.map(sleeping, range(8,0,-1),8*[time_start])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/Task_Interface.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also monitor this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lview.map(sleeping, range(8, 0, -1), 8 * [time_start], block=False)\n",
    "client.wait_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in this case, we really monitor the 8 tasks, and not the engines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Dependencies**\n",
    "\n",
    "The task interface allows one to build dependencies between functions, and therefore to construct a directed acyclic graph (DAG).  \n",
    "This is an advanced topic that we do not cover in this course.  \n",
    "In case you are interested in this topic please see https://ipyparallel.readthedocs.io/en/latest/tutorial/task.html#dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Interface - `DirectView` 2/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will try to understand better what `DirectView` is doing in `map`. In fact, it takes care of the following tasks:\n",
    "- **share data** between the current kernel and the engines (the scatter and the gather), and\n",
    "- **execute** code or functions on engines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sharing data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sharing Python objects one can use a dictionary-style interface where one can update a variable using the key (see the code below `dview[\"b\"] = 3`).  \n",
    "IPython Parallel will pickle the object (serialize it), send it to the engine and unpickle it.  \n",
    "The values of a variable are returned in a list, where for each engine the corresponding value is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview[\"b\"] = 3\n",
    "dview[\"b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case multiple variables are updated one can store them in a dictionary and update (`push`) them once.  \n",
    "To return (`pull`) them, a tuple with the variable names is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview.push({\"x\": 10, \"y\": 11})\n",
    "dview.pull((\"x\", \"y\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scatter and Gather**\n",
    "\n",
    "In case a bigger object needs to be partitioned to different engines, IPython Parallel provides `scatter`. For the inverse operation, i.e. returning scattered partitions, one can use `gather`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(range(8, 0, -1))\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview.scatter(\"l\", l)\n",
    "dview[\"l\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview.gather(\"l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we understand how `DirectView.map()` works.\n",
    "\n",
    "![](./images/Direct_Interface.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Warning!</strong> \n",
    "    \n",
    "  Sharing big objects can be very expensive. We should try to generate the data directly on the engine. For instance, instead of passing the contents of a file from the local kernel to the engine we can pass only the file path and read the file directly on the engine.</br>\n",
    "  NumPy arrays are not copied and are read-only. In case we want to modify them we have to copy them explicitly. \n",
    "  \n",
    "  See: https://ipyparallel.readthedocs.io/en/6.2.0/details.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "my_array = np.zeros((4,4))\n",
    "\n",
    "\n",
    "def set_value(a):\n",
    "    a[0, :] = 1\n",
    "    return a\n",
    "\n",
    "\n",
    "client[0].apply_sync(set_value, my_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array.flags.writeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array = np.zeros((4,4))\n",
    "\n",
    "\n",
    "def set_value(a):\n",
    "    if not a.flags.writeable:\n",
    "        a = a.copy()\n",
    "    a[0, :] = 1\n",
    "    return a\n",
    "\n",
    "\n",
    "client[0].apply_sync(set_value, my_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we used the `DirectView` to apply functions with `dview.apply()` and `dview.map()`.  \n",
    "Moreover we can use the `DirectView` also to:\n",
    "- execute code: `dview.execute(\"code\")`, which corresponds to the `exec()` built-in function executed on the desired engines, and\n",
    "- run code from a file: `dview.run(path)`, which reads the contents of a file and calls the previous `dview.execute` .\n",
    "\n",
    "Next we will execute an assignment directly on the engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview.execute(\"a=3\")\n",
    "dview[\"a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And next we will execute the code from a file.  \n",
    "We check the content using `%pycat` magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat ./now.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview.run(\"./now.py\")\n",
    "dview[\"d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "dview.execute(\"print(numpy.__version__)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dview.execute(\"import numpy; print(numpy.__version__)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.display_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Warning!</strong> \n",
    "  All packages need to be inside the function or loaded beforehand by the engine.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executor API - `ViewExecutor`\n",
    "\n",
    "Every `View` has an `.executor` property to provide the API from `concurrent.futures`.  \n",
    "Moreover the client has an `executor()`, that will provide an executor corresponding precisely to `LoadBalancedView`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = client.executor()\n",
    "executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "time_start = time.time()\n",
    "results = executor.map(sleeping, range(8, 0, -1), 8 * [time_start])\n",
    "list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Practice\n",
    "\n",
    "\n",
    "When we use IPython Parallel we have to take care of the `import` statements since each kernel should know what has to be imported (and how in case of renaming).\n",
    "Instead of adding this to each function (the way that we did it so far) we can do all imports using the direct view.\n",
    "\n",
    "The benefit is that we can focus on the parallelization and minimize the changes to the original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stop the previous cluster\n",
    "cluster.stop_cluster_sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use this approach for the new `sleeping` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import ipyparallel as ipp\n",
    "\n",
    "\n",
    "## we want to use IPython Parallel with this function\n",
    "def sleeping(secs, time_start):\n",
    "\n",
    "    time.sleep(secs)\n",
    "    return (secs, int(time.time() - time_start))\n",
    "\n",
    "\n",
    "n_engines = 3\n",
    "\n",
    "## Configure the cluster\n",
    "cluster = ipp.Cluster(n=n_engines)\n",
    "\n",
    "## Start the ipython parallel cluster\n",
    "cluster.start_cluster_sync()\n",
    "\n",
    "## Connect a client to the cluster\n",
    "client = cluster.connect_client_sync()\n",
    "\n",
    "## Make sure all engines are connected\n",
    "client.wait_for_engines(n=n_engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We `execute` the imports on all engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview = client[:]\n",
    "dview.block = True\n",
    "dview.execute(\"import time\", block=False)\n",
    "client.wait_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lview = client.load_balanced_view()\n",
    "lview.block = True\n",
    "time_start = time.time()\n",
    "results = lview.map(sleeping, range(8, 0, -1), 8 * [time_start], block=False)\n",
    "client.wait_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips**\n",
    "\n",
    "Use both interfaces: \n",
    "- the direct interface is used to load libraries, assign values to engine-local variables, etc;\n",
    "- the task interface is used to benefit from the dynamic load balancing.  \n",
    "\n",
    "**Note**: IPython Parallel can be used for heterogeneous computing. `UnmetDependency` from `ipyparallel.error` can instruct the schedule to not run tasks on some engines (see [here](https://ipyparallel.readthedocs.io/en/latest/tutorial/task.html?highlight=unmet#functional-dependencies))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise Scaling Out \"Compute Pi\" using IPython Parallel (Hands-on) [20min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt the previous example example \"euler_scripts/pi_scale_up_hpc.py\" to use IPython Parallel and test it locally and then on Euler using 3 engines.  \n",
    "Try to keep the function `approx_pi` unchanged.  \n",
    "Which interface (`DirectView` vs `LoadBalancedView`) do you want to use to replace the `ProcessPoolExecutor`?  \n",
    "*Note*: In case you want to increase the number of processors you should increase the number of points as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycat euler_scripts/pi_scale_up_hpc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "import ipyparallel as ipp\n",
    "\n",
    "ON_EULER = False\n",
    "\n",
    "n_engines = int(os.environ.get(\"SLURM_NTASKS\", os.cpu_count()))\n",
    "\n",
    "if ON_EULER is False:\n",
    "    cluster = ipp.Cluster(n=n_engines)\n",
    "elif ON_EULER is True:\n",
    "    cluster = ipp.Cluster(\n",
    "        n=n_engines,\n",
    "        controller_ip=\"*\",\n",
    "        engine_launcher_class=\"MPI\",\n",
    "        location=\"server.local\",\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"{ON_EULER=} should be boolean\")\n",
    "\n",
    "## Start the ipython parallel cluster\n",
    "cluster.start_cluster_sync()\n",
    "\n",
    "## Connect a client to the cluster\n",
    "client = cluster.connect_client_sync()\n",
    "\n",
    "## Make sure all engines are connected\n",
    "client.wait_for_engines(n=n_engines)\n",
    "\n",
    "\n",
    "def approx_pi(n_attempts):\n",
    "    n_hits = 0\n",
    "    for _ in range(n_attempts):\n",
    "        x = uniform(-1.0, 1.0)\n",
    "        y = uniform(-1.0, 1.0)\n",
    "        if x**2 + y**2 <= 1.0:\n",
    "            n_hits += 1\n",
    "    return n_hits\n",
    "\n",
    "\n",
    "dview = client[:]\n",
    "\n",
    "dview.execute(\"from random import uniform\")\n",
    "\n",
    "num_points = 2_000_000\n",
    "num_workers = n_engines\n",
    "\n",
    "num_points_worker = [int(num_points / num_workers) for _ in range(num_workers)]\n",
    "num_points = sum(num_points_worker)\n",
    "\n",
    "started = time.time()\n",
    "results = dview.map(approx_pi, num_points_worker)\n",
    "client.wait_interactive()\n",
    "\n",
    "duration = time.time() - started\n",
    "print(\"The estimates value of Pi is: {}\".format(sum(results) * 4 / num_points))\n",
    "print(f\"Execution time: {duration:.3f}s using {num_workers=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Save the file on Euler, change the line `ON_EULER = True`, and run it as a quick test: \n",
    "\n",
    "```bash\n",
    "$ env2lmod\n",
    "$ module load gcc/8.2.0 fast_python_workshop_cpu/2022.1.0\n",
    "$ venv_cpu_init\n",
    "\n",
    "$ sbatch -n 3 --nodes 1 -wrap \"python ...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "- https://ipyparallel.readthedocs.io/en/latest/tutorial/index.html\n",
    "- SciPy 2014 Tutorial (the API is slightly outdated):\n",
    "    - Video: [Part 1](https://www.youtube.com/watch?v=y4hgalfhc1Y&t=1146s&ab_channel=Enthought), [Part 2](https://www.youtube.com/watch?v=-9ijnHPCYhY&ab_channel=Enthought), [Part 3](https://www.youtube.com/watch?v=U5mhpKkIx2Y&t=2637s&ab_channel=Enthought)\n",
    "    - https://github.com/minrk/IPython-parallel-tutorial/blob/master/Index.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scaling Out - Dask\n",
    "> Dask is a flexible library for parallel computing in Python.\n",
    "\n",
    "Dask consists of a \n",
    "- **dynamic task scheduler** that is responsible for scheduling tasks.\n",
    "- \"Big Data\" **collections** that provide a similar interface for NumPy, Pandas, Python, iterators.  \n",
    "\n",
    "Dask allows one to perform out-of-memory computation (using the collections) or to even use distributed environments like HPC clusters via the `dask.distributed` package . \n",
    "\n",
    "*Dask also provides a single machine scheduler in the `dask` package but it does not scale out, and therefore the `dask.distributed` schedule is the recommended one.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo Dask \"Hello World\" on multi-node\n",
    "We will first test a simple script: `euler_scripts/dask_demo.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycat euler_scripts/dask_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To run on Euler**\n",
    "\n",
    "```bash\n",
    "$ env2lmod\n",
    "$ module load gcc/8.2.0 fast_python_workshop_cpu/2022.1.0\n",
    "$ venv_cpu_init\n",
    "$ sbatch -n 4 --nodes 2 --wrap \"mpirun python ~/euler_scripts/dask_demo.py\"\n",
    "\n",
    "[{'cwd': '/cluster/home/chadhat',\n",
    "  'hostname': 'eu-a2p-277',\n",
    "  'pid': 79574,\n",
    "  'python': '3.10.4 (main, May 30 2022, 08:01:42) [GCC 8.2.0]'},\n",
    " {'cwd': '/cluster/home/chadhat',\n",
    "  'hostname': 'eu-a2p-278',\n",
    "  'pid': 124228,\n",
    "  'python': '3.10.4 (main, May 30 2022, 08:01:42) [GCC 8.2.0]'},\n",
    " {'cwd': '/cluster/home/chadhat',\n",
    "  'hostname': 'eu-a2p-277',\n",
    "  'pid': 79574,\n",
    "  'python': '3.10.4 (main, May 30 2022, 08:01:42) [GCC 8.2.0]'},\n",
    " {'cwd': '/cluster/home/chadhat',\n",
    "  'hostname': 'eu-a2p-278',\n",
    "  'pid': 124228,\n",
    "```\n",
    "\n",
    "By checking **`hostname`** we see that the job was running on 2 different nodes with the hostnames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Architecture of `dask.distributed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Dask Distributed Cluster** (which follows the **Manager-Workers** pattern) consists of: \n",
    "- **Dask Workers**: they compute tasks, store and serve computed results (each worker is a **process** on its own that can have several threads, and it sends work to a `concurrent.futures.ThreadPoolExecutor`). They can communicate with each other to share data.\n",
    "- **Dask Scheduler (Manager)**: it sends tasks to run on workers.\n",
    "\n",
    "In case we decide to use threads we have to make sure that: \n",
    "1. our code is not already using multithreading, and\n",
    "2. the problem can take advantage of multithreading (e.g. IO bounded, the GIL is released)\n",
    "\n",
    "The connection to the cluster is done via the `Client` object which submits tasks to the scheduler to be executed.\n",
    "\n",
    "<p>\n",
    "<img src=\"./images/dask_architecture.png\" width=\"600\">\n",
    "<div>Source: <a href=https://github.com/dask/dask/issues/4471#issuecomment-896799678>Dask Distributed - Architecture Diagram</a></div>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dask Distributed Cluster on PC and Euler\n",
    "\n",
    "We can use Dask clusters both on our PC and on Euler.\n",
    "\n",
    "The main actions that we can perform are:\n",
    "- configure and start, and\n",
    "- stop.\n",
    "\n",
    "JupyterLab provides extensions to create a Dask cluster using a graphical user interface.\n",
    "\n",
    "Next we will focus on the programmatic approach that can be used directly from a Python script or Jupyter Notebook.  \n",
    "On Euler we will have to use also the command line interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure and Start the Dask Distributed Cluster on our PC - `LocalCluster`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "## Configure and start\n",
    "cluster = LocalCluster(n_workers=3, threads_per_worker=1)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client.wait_for_workers(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a local cluster with 3 workers, each worker with 1 thread.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the distributed cluster is unresponsive we can reset it using the `restart()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Configure and Start the Dask Distributed Cluster on Euler - `dask-mpi`**\n",
    "\n",
    "In order to have a good integration with the Euler cluster we have used [Dask-MPI](http://mpi.dask.org/en/latest/). \n",
    "\n",
    "For **batch jobs** you have to run them as [MPI jobs on Euler](https://scicomp.ethz.ch/wiki/Using_the_batch_system#MPI), e.g. one has to submit a Python script `script.py` on Euler with\n",
    "```bash\n",
    "$ sbatch [options] mpirun python script.py\n",
    "```\n",
    "\n",
    "The script needs to call the [`initialize()` method](http://mpi.dask.org/en/latest/generated/dask_mpi.core.initialize.html#dask-mpi-core-initialize) provided by Dask-MPI.\n",
    "\n",
    "\n",
    ">Initialize a Dask cluster using mpi4py.\n",
    "Using mpi4py, MPI rank 0 launches the Scheduler, MPI rank 1 passes through to the client script, and all other MPI ranks launch workers.\n",
    "source: http://mpi.dask.org/en/latest/batch.html\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "from dask_mpi import initialize\n",
    "\n",
    "## memory in bytes on Euler\n",
    "mem = (\n",
    "    1024 * 1024 * int(os.environ[\"SLURM_MEM_PER_CPU\"])\n",
    "    if os.environ.get(\"SLURM_MEM_PER_CPU\")\n",
    "    else \"auto\"\n",
    ")\n",
    "\n",
    "# run within MPI env\n",
    "initialize(nthreads=1, memory_limit=mem, local_directory=\"~/dask-mpi-workers\")\n",
    "```\n",
    "The first proces (MPI rank 0)  is used for the scheduler, the next for the client script and all others are used for actual workers.\n",
    "\n",
    "Therefore the number of workers is equal with the number of processors (passed via `-n` to SLURM) minus 2.\n",
    "\n",
    "The [`dask-mpi` Python package](http://mpi.dask.org/) is required to run on Euler on multiple nodes. It requires the [`mpi4py` package](https://mpi4py.readthedocs.io/en/stable/) and an MPI implementation, e.g. the `openmpi/4.1.4` module.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Warning!</strong> \n",
    "  In case you want to use Dask-MPI with interactive jobs you have to use a different approach - please see  <a href=\"http://mpi.dask.org/en/latest/interactive.html\">Dask-MPI with Interactive Jobs</a>. <br />\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Warning!</strong> \n",
    "  Do not use  <a href=\"http://jobqueue.dask.org/en/latest/\">Dask-Jobqueue</a>. <br />\n",
    "  Each worker runs as a separate batch job, and therefore we can easily submit many small batch jobs that can impact the entire cluster negatively.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Arrays - `dask.array`\n",
    "\n",
    "> Dask Array implements a subset of the NumPy ndarray interface using blocked algorithms, cutting up the large array into many small arrays. This lets us compute on arrays that would not fit in the memory using all of our cores. \n",
    "\n",
    "It is one of Dask's built-in **collections**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Dask Arrays**  \n",
    "\n",
    "We start with a familiar NumPy 2-dimensional array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define and start the cluster\n",
    "cluster = LocalCluster(n_workers=3, threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_np = np.random.rand(9, 12)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use **Dask Array** we have to replace `numpy` with `dask.array`. \n",
    "We can create a Dask Array from a NumPy array using `from_array()` method.\n",
    "\n",
    "Next we should **chunk** the initial array into many smaller NumPy-like arrays.\n",
    "\n",
    "These arrays will then be used smartly by **Dask Array**. This precisely corresponds to a **domain decomposition**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "x_da = da.from_array(x_np, chunks=(3, 4))\n",
    "x_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`chunks=(3, 4)` represents the **size** along each of the dimensions:\n",
    "- 3 chunks, each of **size** 3 along the first dimension of size 9, and\n",
    "- 3 chunks, each of **size** 4 along the second dimension of size 12.\n",
    "\n",
    "One can specify even the sizes of each block (see below):\n",
    "- 3 chunks of: size 1, size 4, and size 4 along the first dimension of size 9, and\n",
    "- 3 chunks of: size 3, size 3, and size 6 along the second dimension of size 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_da = da.from_array(x_np, chunks=((1, 4, 4), (3, 3, 6)))\n",
    "x_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.config.get('array.chunk-size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T09:54:19.288027Z",
     "iopub.status.busy": "2021-01-11T09:54:19.287791Z",
     "iopub.status.idle": "2021-01-11T09:54:19.294817Z",
     "shell.execute_reply": "2021-01-11T09:54:19.293281Z",
     "shell.execute_reply.started": "2021-01-11T09:54:19.288005Z"
    }
   },
   "source": [
    "This allows for a high flexibility.\n",
    "\n",
    "\n",
    "<p>\n",
    "<img src=\"./images/dask-array-black-text.svg\" width=\"400\">\n",
    "<div>Source: <a href=https://docs.dask.org/en/latest/array.html#design>Dask Arrays</a></div>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunk size**: In practice the chunk size should be adjusted such that the entire work can be done in the memory, close to the upper boundary (see more [here](https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes)).\n",
    "\n",
    "Dask Arrays can be loaded from or stored in diverse sources, e.g. HDF5, a file format relevant for Big Data use cases.  \n",
    "This is an advanced topic that we do not cover in this course.  \n",
    "In case you are interested in this topic please see https://docs.dask.org/en/latest/array-creation.html ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computation with Dask Array**\n",
    "\n",
    "Dask Arrays support a subset of the NumPy methods. So we can expect to do the main Numpy operations with Dask Arrays, but not all.\n",
    "\n",
    "For example, we can easily do a mean with NumPy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_np = x_np.mean()\n",
    "res_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same method also exists for Dask Arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_da = x_da.mean()\n",
    "res_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, `Dask` is **lazy**.  \n",
    "Dask will first plan the split and parallelization of the computation. It creates a so-called **task graph**.  This step automatically takes care of potential **functional decomposition**.\n",
    "However, up till this point the computation does not start. \n",
    "\n",
    "The computation can be performed with the `.compute()` method which on completion returns the results.\n",
    "\n",
    "The task graph created by Dask can be visualized using the `.visualize()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_da.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_da.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see a Dask Array is able to split the computation in tasks. Next we will show how to check that indeed the tasks are parallelized by using the Dask Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop the Dask Cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stop the cluster\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T13:39:52.902462Z",
     "iopub.status.busy": "2021-01-11T13:39:52.902224Z",
     "iopub.status.idle": "2021-01-11T13:39:52.913851Z",
     "shell.execute_reply": "2021-01-11T13:39:52.912269Z",
     "shell.execute_reply.started": "2021-01-11T13:39:52.902440Z"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Warning!</strong> \n",
    "  Keep in mind that Dask Array API is a subset of NumPy API. <br />\n",
    "  Check <a href=\"https://docs.dask.org/en/latest/array-api.html\">dask.array API</a> for the details.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T10:26:56.960345Z",
     "iopub.status.busy": "2021-01-11T10:26:56.960096Z",
     "iopub.status.idle": "2021-01-11T10:26:56.965436Z",
     "shell.execute_reply": "2021-01-11T10:26:56.963741Z",
     "shell.execute_reply.started": "2021-01-11T10:26:56.960321Z"
    }
   },
   "source": [
    "### Dask Dashboard - JupyterLab extension\n",
    "\n",
    "The main tool to perform live diagnostics is the dask dashboard.    \n",
    "The dashboard is accessible from the `client.dashboard_link` property. Pay attention to the link below corresponding to the `Dashboard`, e.g. [http://127.0.0.1:8787/status]( http://127.0.0.1:8787/status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JupyterLab extensions allows us to integrate the dashboard directly in JupyterLab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <strong>Exercise: Dask Dashboard [5 min] - Task Interface</strong>\n",
    "    \n",
    "1. Organize your workspace to mimic the image provided below (insert the link in the `DASK DASHBOARD URL`).\n",
    "</div>\n",
    "\n",
    "![](./images/dask_dashboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see plots in action we will define a more intensive computation on a larger dask array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.random.random((20000, 20000), chunks=(2000, 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = (da.sin(x) + da.cos(x.T)).sum()\n",
    "\n",
    "res.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sometimes problems do not fit into one of the collections like `dask.array` or `dask.dataframe`. In these cases, users can parallelize custom algorithms using the simpler `dask.delayed` interface. This allows one to create graphs directly with a simple annotation of normal python code.\n",
    "\n",
    "It is one of the built-in Dask **collections**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def sleeping(secs):\n",
    "\n",
    "    time.sleep(secs)\n",
    "    return secs\n",
    "\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a simple dependency between three tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sleeping(1)\n",
    "y = sleeping(2)\n",
    "z = add(x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we annotate such a computation and get the \"lazy\" dask task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "x = dask.delayed(sleeping)(1)\n",
    "y = dask.delayed(sleeping)(2)\n",
    "z = dask.delayed(add)(x, y)\n",
    "z.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore Dask Delayed allows us to express the functional decomposition explicitly.  \n",
    "This can be easily combined with **domain decomposition**.  \n",
    "\n",
    "Suppose that we want to increment all numbers in a list and compute their `sum` in the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = range(8, 0, -1)\n",
    "\n",
    "output = []\n",
    "for x in data:\n",
    "    a = dask.delayed(sleeping)(x)\n",
    "    output.append(a)\n",
    "\n",
    "total = dask.delayed(sum)(output)\n",
    "total.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-11T14:03:39.327765Z",
     "iopub.status.busy": "2021-01-11T14:03:39.327283Z",
     "iopub.status.idle": "2021-01-11T14:03:39.650064Z",
     "shell.execute_reply": "2021-01-11T14:03:39.636429Z",
     "shell.execute_reply.started": "2021-01-11T14:03:39.327735Z"
    }
   },
   "source": [
    "**`@dask.delayed` decorator**\n",
    "\n",
    "Alternatively one can decorate the functions directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def sleeping(secs):\n",
    "\n",
    "    time.sleep(secs)\n",
    "    return secs\n",
    "\n",
    "\n",
    "data = range(8, 0, -1)\n",
    "\n",
    "output = []\n",
    "for x in data:\n",
    "    a = sleeping(x)\n",
    "    output.append(a)\n",
    "\n",
    "total = dask.delayed(sum)(output)\n",
    "total.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(range(8, 0, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Futures\n",
    "> Dask supports a real-time task framework that extends Python’s `concurrent.futures` interface. This interface is good for arbitrary task scheduling like `dask.delayed`, but is **immediate** rather than **lazy**, which provides some more flexibility in situations where the computations may evolve over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dask `Client` implements the `concurrent.futures` interface that we introduced in the previous section.  \n",
    "So we can simply use:\n",
    "- `client.submit()` to pass a function to the workers for execution (it returns immediately),\n",
    "- `.result()` to get the result locally in the master process represented by the notebook.\n",
    "\n",
    "On HPC clusters the code execution can happen even on a different node, so we have to keep in mind that getting the data locally can be \"expensive\" in terms of execution-time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def sleeping(secs):\n",
    "\n",
    "    time.sleep(secs)\n",
    "    return secs\n",
    "\n",
    "\n",
    "future = client.submit(sleeping, 4)\n",
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "future = client.submit(sleeping, 5)\n",
    "future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "future = client.submit(sleeping, 5)\n",
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not 4 seconds?\n",
    "\n",
    "**Pure functions**\n",
    "\n",
    "By default, `dask.distributed` assumes that the functions are [**pure**](https://toolz.readthedocs.io/en/latest/purity.html) (same for delayed), i.e.:\n",
    "- the function depends only on its inputs (no hidden states),\n",
    "- the evaluation does not cause side effects, e.g. update global variable. \n",
    "\n",
    "The scheduler avoids redundant computation based on this assumption - if the result is already in memory it will be used. Therefore it is using the memoization that we learned in Section 3 (Caching and memoization).\n",
    "\n",
    "We can change the default behavior by using the `pure=False` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "future = client.submit(sleeping, 5, pure=False)\n",
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical case of a non-pure function is the random function.  \n",
    "We create a function that creates a list of random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "\n",
    "def my_random(size):\n",
    "    return [random() for i in range(size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_random(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_random(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results are different.  \n",
    "Next we will submit it twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.submit(my_random, 3).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.submit(my_random, 3).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we got the same result, since dask treated the function as pure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Warning!</strong>\n",
    "    \n",
    "  By default, Dask assumes that the function as are [**pure**](https://toolz.readthedocs.io/en/latest/purity.html).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallel `map()`**  \n",
    "\n",
    "Dask Futures provides a `client.map()` method which can be used to parallelize the `map()` built-in function. \n",
    "\n",
    "`map()` returns a list of Futures, and similar to the `concurrent.futures` case we can use `as_completed` to get them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def sleeping(secs, time_start):\n",
    "    time.sleep(secs)\n",
    "    return (secs, int(time.time() - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from dask.distributed import as_completed\n",
    "\n",
    "time_start = time.time()\n",
    "results = client.map(sleeping, range(8, 0, -1), 8 * [time_start], pure=False)\n",
    "for tmp in as_completed(results):\n",
    "    print(tmp.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the results can be gathered potentially more efficiently using the `Client.gather` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "time_start = time.time()\n",
    "results = client.map(sleeping, range(8, 0, -1), 8 * [time_start], pure=False)\n",
    "client.gather(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise Scaling Out \"Compute Pi\" using Dask Futures (Hands-on) [20min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt the previous example \"euler_scripts/pi_scale_up_hpc.py\" to use Dask Futures and test it locally and then on Euler using 3 workers (5 processors).    \n",
    "Try to keep `approx_pi` function unchanged.  \n",
    "*Note*: In case you want to increase the number of processors you should increase the number of points as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycat euler_scripts/pi_scale_up_hpc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "ON_EULER = False\n",
    "\n",
    "\n",
    "if ON_EULER is False:\n",
    "    num_workers = os.cpu_count()\n",
    "    cluster = LocalCluster(n_workers=num_workers, threads_per_worker=1)\n",
    "    client = Client(cluster)\n",
    "\n",
    "\n",
    "elif ON_EULER is True:\n",
    "    from dask_mpi import initialize\n",
    "\n",
    "    num_workers = int(os.environ.get(\"SLURM_NTASKS\", os.cpu_count())) - 2\n",
    "\n",
    "    ## memory in bytes on Euler\n",
    "    mem = (\n",
    "        1024 * 1024 * int(os.environ[\"SLURM_MEM_PER_CPU\"])\n",
    "        if os.environ.get(\"SLURM_MEM_PER_CPU\")\n",
    "        else \"auto\"\n",
    "    )\n",
    "    # run within MPI env\n",
    "    initialize(nthreads=1, memory_limit=mem, local_directory=\"~/dask-mpi-workers\")\n",
    "\n",
    "    client = Client()\n",
    "\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"{ON_EULER=} should be boolean\")\n",
    "\n",
    "## Define and start the cluster\n",
    "\n",
    "\n",
    "def approx_pi(n_attempts):\n",
    "    n_hits = 0\n",
    "    for _ in range(n_attempts):\n",
    "        x = uniform(-1.0, 1.0)\n",
    "        y = uniform(-1.0, 1.0)\n",
    "        if x**2 + y**2 <= 1.0:\n",
    "            n_hits += 1\n",
    "    return n_hits\n",
    "\n",
    "\n",
    "num_points = 2_000_000\n",
    "\n",
    "num_points_worker = [int(num_points / num_workers) for _ in range(num_workers)]\n",
    "num_points = sum(num_points_worker)\n",
    "\n",
    "started = time.time()\n",
    "\n",
    "future = client.map(approx_pi, num_points_worker, pure=False)\n",
    "results = client.gather(future)\n",
    "\n",
    "duration = time.time() - started\n",
    "print(\"The estimates value of Pi is: {}\".format(sum(results) * 4 / num_points))\n",
    "print(f\"Execution time: {duration:.3f}s using {num_workers=}\")\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Save the file on Euler, change the line `ON_EULER = True`, and run it as a quick test: \n",
    "\n",
    "```bash\n",
    "$ env2lmod\n",
    "$ module load gcc/8.2.0 fast_python_workshop_cpu/2022.1.0\n",
    "$ venv_cpu_init\n",
    "\n",
    "$ sbatch -n 5 --nodes 1 --wrap \"mpirun python ...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Further Reading\n",
    "- https://dask.org/\n",
    "- https://docs.dask.org/en/latest/array.html\n",
    "- https://docs.dask.org/en/latest/delayed.html\n",
    "- https://distributed.dask.org/en/latest/\n",
    "- https://docs.dask.org/en/latest/array.html\n",
    "- https://tutorial.dask.org/\n",
    "- https://www.youtube.com/watch?v=_u0OQm9qf_A&t=3583s&ab_channel=Dask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
