{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# IGNORE THIS CELL WHICH CUSTOMIZES LAYOUT AND STYLING OF THE NOTEBOOK !\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings = lambda *a, **kw: None\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(open(\"../documents/custom.html\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"font-size: 2.5em; font-weight: bold;\">Section 6a: Introduction to the Euler HPC Cluster</p>\n",
    "<br/>\n",
    "<span style=\"background:#f0f0e0;padding:1em\">Copyright (c) 2020-2021 ETH Zurich, Scientific IT Services. This work is licensed under <a href=\"https://creativecommons.org/licenses/by-nc/4.0/\">CC BY-NC 4.0</a></span><br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> **Target audience:** this presentation is intended for beginners and intermediate cluster users and for users that are new to SLURM (but are familiar with HPC systems). If you are confident you can answer the \"check questions\" at the end of this part, feel free to skip this presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# HPC and Python\n",
    "* An HPC cluster represents a **collection of tens, hundreds, or thousands of fast computers** able to perform a lot more computations than a single computer. \n",
    "* An HPC cluster is usually **shared** between users and research groups.  \n",
    "* Use cases:\n",
    "    * Traditionally used in the simulation of physical systems (e.g. quantum systems, weather, climate or fluid dynamics)\n",
    "    * Nowadays HPC clusters are accomondating Big Data workloads from diverse fields that cannot be run on a PC due to resource requirements. \n",
    "* The increased popularity of Python in research (and industry) motivated the development of Python libraries that take advantage of HPC clusters (e.g. IPython Parallel, Dask).\n",
    "\n",
    "In this section we will learn:\n",
    "- how to use the ETHZ Euler cluster, and \n",
    "- which pythonic solutions are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to the ETHZ HPC Clusters\n",
    "\n",
    "The [Scientific IT Services](https://sis.id.ethz.ch) provide two [research IT environments](https://sis.id.ethz.ch/services/hpc/):\n",
    "\n",
    "\n",
    "\n",
    "| **Euler**                                                           |        **LeoMed**                                                             |\n",
    "|---------------------------------------------------------------------|-------------------------------------------------------------------------------|\n",
    "| large CPU & GPU cluster                                             | multiple CPU & GPU clusters                                                   |\n",
    "| designed for high-performance and high-throughput applications      | designed for biomedical application with sensitive data; includes very high IT security controls               |\n",
    "| provided by SIS's High Performance Computing group                  | provided by SIS's Research IT Platforms group                                 |\n",
    "| shareholder model + a small public share for all ETH members (free) | shareholders only                                                             |\n",
    "| https://scicomp.ethz.ch                                             | https://unlimited.ethz.ch/display/LeoMed2/Leonhard+Med+Intro+for+shareholders |\n",
    "\n",
    "\n",
    "**In this tutorial we will focus on the Euler cluster**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##Â The scicomp wiki\n",
    "* https://scicomp.ethz.ch/\n",
    "* Documentation on all aspects of Euler\n",
    "\n",
    "<p>\n",
    "<img src=\"./images/scicomp.png\" width=\"700\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## [Euler cluster](https://scicomp.ethz.ch/wiki/Euler)\n",
    "- regularly expanded since 2013\n",
    "- shareholder model \n",
    "  - over 180 research groups from almost all departments of ETH invested in Euler)\n",
    "  - a small public share (only CPUs), with **free** limited access for all ETH members (up to 48 cores and 128 GB of memory)\n",
    "  - GPU nodes (only for shareholders)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using the Euler cluster\n",
    "\n",
    "* An HPC cluster is shared between multiple users and its mission is to allow the users to run computing tasks, which we call **jobs**.  \n",
    "* The jobs are running on so-called **compute** nodes. However, in order to have the jobs scheduled on a compute node we have to submit them. \n",
    "* For this purpose we have the so-called **login** nodes. These allow the users to login from their PC and submit the jobs.\n",
    "* login: `ssh <username>@euler.ethz.ch` (see [scicomp wiki](https://scicomp.ethz.ch/wiki/Accessing_the_cluster))\n",
    "* The submitted jobs are scheduled by the **batch system**, and once there are available resources the jobs are started on the compute nodes. Depending on the resources that you requested and on their availability you might have to wait shorter or longer (or even forever). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<p>\n",
    "<img src=\"./images/Cluster.png\" width=\"1000\">\n",
    "<div>Source: <a href=https://scicomp.ethz.ch/wiki/File:Accessing_the_clusters.png>ETHZ scicomp wiki</a></div>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cluster architecture\n",
    "\n",
    "The main components of the cluster are:\n",
    "- Login nodes (for accessing the cluster)\n",
    "- Compute nodes (where jobs are executed)\n",
    "- Storage (where data is stored)\n",
    "- Environment - Modules (centrally installed applications, libraries, compilers, ...)\n",
    "- Batch system (manages jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Login vs Compute Nodes\n",
    "- **login** nodes: allow users to login to the cluster and submit jobs\n",
    "- **compute** nodes: the place where jobs are running\n",
    "\n",
    "**Do not run computation on the login node:**\n",
    "\n",
    "- The batch system balances loads on the compute nodes\n",
    "- Running on the login node circumvents this mechanism\n",
    "- As a result, you might make the login node unusable for you and others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Storage and data management\n",
    "\n",
    "Data can be stored in different locations (see also the [scicomp wiki](https://scicomp.ethz.ch/wiki/Storage_systems)):\n",
    "\n",
    "<p>\n",
    "<img src=\"./images/Storage.png\" width=\"500\">\n",
    "<div>Source: <a href=https://scicomp.ethz.ch/wiki/File:Storage.png>ETHZ scicomp wiki</a></div>\n",
    "</p>\n",
    "\n",
    "* **Personal storage for all users**\n",
    "    * Home\n",
    "    * Global Scratch\n",
    "    * Local Scratch\n",
    "* **Group storage for shareholders**\n",
    "    * Project\n",
    "    * Work\n",
    "* **External Storage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p>\n",
    "<img src=\"./images/Storage.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "### Personal storage for all users\n",
    "* Home\n",
    "  * `/cluster/home/username`, `$HOME`, `~`\n",
    "  * safe, long-term storage \n",
    "  * for critical data (program source, scripts, etc.)\n",
    "* Global Scratch\n",
    "  * `/cluster/scratch/username`, `$SCRATCH`\n",
    "  * fast, short-term storage \n",
    "  * for computations running on the cluster\n",
    "* Local Scratch\n",
    "  * `/scratch`, `$TMPDIR` on each compute node\n",
    "  * very short-term: deleted automatically when the job ends\n",
    "  * for serial, I/O-intensive applications. \n",
    "  * https://scicomp.ethz.ch/wiki/Using_local_scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p>\n",
    "<img src=\"./images/Storage.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "\n",
    "### Group storage for shareholders\n",
    "Shareholders can buy the space on Project and Work as much as they need, and manage access rights\n",
    "* Project\n",
    "  * `/cluster/project/groupname`\n",
    "  * similar to $HOME, but for groups\n",
    "* Work\n",
    "  * `/cluster/work/groupname`\n",
    "  * Similar to global scratch, but without purge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p>\n",
    "<img src=\"./images/Storage.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "### External Storage\n",
    "* **Central NAS:**\n",
    "    - [Central Network Attached Storage (NAS)](https://ethz.ch/services/en/it-services/catalogue/storage/nas.html)\n",
    "    - can be mounted via NFS (not CIFS)\n",
    "* **Other NAS:**\n",
    "    - it needs to support NFSv3 (the only NFS version supported by the cluster)\n",
    "    - contact cluster-support@id.ethz.ch for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview\n",
    "### Life span\n",
    "\n",
    "|                | Personal storage<br>for all users | Group storage<br>for shareholders |\n",
    "|----------------|:----------------------------------:|:----------------------------------:|\n",
    "| <b>long-term  | Home                               | Project                            |\n",
    "| <b>short-term | Global Scratch<br>Local Scratch    | Work                               |\n",
    "\n",
    "\n",
    "### File size\n",
    "\n",
    "\n",
    "|  File system   |    Small files  |  Large files  |\n",
    "|:--------------|:---------------:|:-------------:|\n",
    "| Home           |         â       |       o       |\n",
    "| Global Scratch |        o        |        ââ     |\n",
    "|  Local Scratch |        ââ       |       o       |\n",
    "| Project        |         â       |       â       |\n",
    "| Work           |         o       |       ââ      |\n",
    "|  Central NAS   |         â       |       â       |\n",
    "\n",
    ">  For a more in-depth comparison of the file systems, see the [scicomp wiki](https://scicomp.ethz.ch/wiki/Storage_systems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quotas\n",
    "\n",
    "* The availability of disk resources is linked to your user account and the shareholder group.\n",
    "* You can check the available disk space using `lquota`.\n",
    "* There are limitations (quotas) for \n",
    "  * the numbers of files created \n",
    "  * the total storage space used\n",
    "* Soft and hard quota\n",
    "    * exceeding **soft quota**:\n",
    "      * you'll have 5 days (\"grace period\") to reduce your disk usage\n",
    "      * if you still exceed soft quota after grace period, you can no longer write\n",
    "    * **hard quota** cannot be exceeded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "```bash\n",
    "$ lquota\n",
    "\n",
    "+-----------------------------+-------------+------------------+------------------+------------------+\n",
    "| Storage location:           | Quota type: | Used:            | Soft quota:      | Hard quota:      |\n",
    "+-----------------------------+-------------+------------------+------------------+------------------+\n",
    "| /cluster/home/<username>    | space       |          3.36 GB |         17.18 GB |         21.47 GB |\n",
    "| /cluster/home/<username>    | files       |            75505 |            80000 |           100000 |\n",
    "+-----------------------------+-------------+------------------+------------------+------------------+\n",
    "| /cluster/shadow             | space       |          8.19 kB |          2.15 GB |          2.15 GB |\n",
    "| /cluster/shadow             | files       |                3 |            50000 |            50000 |\n",
    "+-----------------------------+-------------+------------------+------------------+------------------+\n",
    "| /cluster/scratch/<username> | space       |         85.12 MB |          2.50 TB |          2.70 TB |\n",
    "| /cluster/scratch/<username> | files       |            20779 |          1000000 |          1500000 |\n",
    "+-----------------------------+-------------+------------------+------------------+------------------+\n",
    "```\n",
    "\n",
    "Remark: *`shadow` is internally used by the batch system to temporarily store the output from your compute jobs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Copying data from/to the cluster:**\n",
    "\n",
    "In case you want to transfer data to the cluster from the outside you need to copy data on your own. The most common way is to use:\n",
    "\n",
    "```bash\n",
    "$ scp [options] source destination\n",
    "```\n",
    "Alternatives are: \n",
    "- command line tools: `rsync`, `sftp`, `svn`, `git`, `wget`, ...\n",
    "- tools with a graphical user interface: [FileZilla](https://filezilla-project.org/), [Cyberduck](https://cyberduck.io/), [WinSCP](https://winscp.net/eng/index.php), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Environment - Modules\n",
    "\n",
    "The clusters provide modules to configure our computing environment for specific tools, e.g.:\n",
    "- Development tools \n",
    "- Scientific libraries \n",
    "- Communication libraries (MPI)\n",
    "- Third-party applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Advantages:\n",
    "- User do not need to install software, they can just load existing modules\n",
    "- Different versions of the same software can co-exist and can be selected explicitly\n",
    "- We can easily try out different tools and  switch between versions to find out which one works best for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**The New Software Stack**  \n",
    "\n",
    "\n",
    "* The HPC team introduced a new system in 2020 to manage the modules, the so called *new software stack*.\n",
    "    * The old stack still is the default but we recommend to use the new software stack.\n",
    "    * To permanently switch to the new software stack (more information [here](https://scicomp.ethz.ch/wiki/New_SPACK_software_stack_on_Euler)):\n",
    "    `/cluster/apps/local/set_software_stack.sh new`\n",
    "    * You can temporarily switch from \n",
    "      * new -> old stack: `lmod2env`\n",
    "      * old -> new stack: `env2lmod`\n",
    "* For a list of available Python modules, see [Python on Euler](https://scicomp.ethz.ch/wiki/Python_on_Euler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Module command**\n",
    "\n",
    "* Software orgainzed into modules\n",
    "* Modules can be managed (listed, loaded...) using the `module` commmand\n",
    "\n",
    "\n",
    "| Command            \t | Description                                         \t  |\n",
    "|----------------------- |------------------------------------------------------- |\n",
    "| `module`            \t | get info about module sub-commands                  \t  |\n",
    "| `module spider`      \t | list all modules available on the cluster           \t  |\n",
    "| `module spider <name>` |Â list all modules that match `<name>`                   |\n",
    "| `module avail`       \t | list all modules that can be loaded in the current environment  |\n",
    "| `module avail <name>`  |Â list all modules that match `<name>`                   |\n",
    "| `module key <keyword>` | list all modules whose description contains `<keyword>`|\n",
    "| `module help <name>`   | get information about module `<name> `                 |\n",
    "| `module show <name>`   | show what module `<name>` does (without loading it)    |\n",
    "| `module load <name>`   | load module `<name>`                                   |\n",
    "| `module list`        \t | list all currently loaded modules                   \t  |\n",
    "| `module unload <name>` | unload module `<name>`                                 |\n",
    "| `module purge`       \t | unload all modules at once                          \t  |\n",
    "\n",
    "* These commands are the same for both the old and the new software stack. \n",
    "* The main difference between the stacks are the available modules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**\n",
    "\n",
    "```bash\n",
    "\n",
    "$ env2lmod\n",
    "Current modulesystem is already LMOD modules, nothing to change for env2lmod\n",
    "\n",
    "$ module list\n",
    "Currently Loaded Modules:\n",
    "  1) StdEnv   2) gcc/4.8.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```bash\n",
    "$ module spider python\n",
    "\n",
    "--------------------\n",
    "  python:\n",
    "--------------------\n",
    "\n",
    "     Versions:\n",
    "        python/2.7.14\n",
    "        ...\n",
    "        python/3.10.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```bash\n",
    "$ module spider python/3.10.4\n",
    "...\n",
    "You will need to load all module(s) on any one of the lines below \n",
    "before the \"python/3.10.4\" module is available to load.\n",
    "\n",
    "      gcc/8.2.0\n",
    "...\n",
    "      \n",
    "$ module load gcc/8.2.0 python/3.10.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```bash\n",
    "$ which python\n",
    "/cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/bin/python\n",
    "\n",
    "$ python --version\n",
    "Python 3.10.4\n",
    "\n",
    "$ module list\n",
    "Currently Loaded Modules:\n",
    "  1) StdEnv   2) gcc/8.2.0   3) openblas/0.3.15   4) python/3.10.4\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Python ecosystem on Euler\n",
    "\n",
    "* Conda is not recommended on HPC systems (amongst other things, because it creates a lot of small files which most HPC storage is not optimized for; see [here](https://scicomp.ethz.ch/wiki/Conda))\n",
    "* When available, use centrally installed packages (installed as part of a Python module; see [Python on Euler](https://scicomp.ethz.ch/wiki/Python_on_Euler))\n",
    "* Additional packages can be installed in `virtualenvs` (see [here](https://scicomp.ethz.ch/wiki/Python_virtual_environment))\n",
    "* You can also ask for a package to be installed via a request to cluster-support@id.ethz.ch\n",
    "\n",
    "To get a list of centrally installed packges:\n",
    "```bash\n",
    "$ module load gcc/8.2.0 python/3.10.4\n",
    "$ pip list \n",
    "Package                         Version\n",
    "------------------------------- ----------------------\n",
    "absl-py                         1.0.0\n",
    "AccessControl                   5.3.1\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1: login, modules, data transfer [20min]\n",
    "\n",
    "### Account\n",
    "* In the next exercises, you should replace `<username>` with the training account and use the corresponding password.\n",
    "* After the workshop, you could use your nethz account (the user that you use to login to [mail.ethz.ch](https://mail.ethz.ch/)). The `password` is also the same as the one used for your email address.\n",
    "\n",
    "### Goals\n",
    "* login to Euler and familiarize yourself\n",
    "* navigate the file system\n",
    "* explore the module system\n",
    "* transfer some data from your PC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Login to the cluster.\n",
    "\n",
    "```bash\n",
    "$ ssh <username>@euler.ethz.ch\n",
    "\n",
    "# Example:\n",
    "$ ssh msmith@euler.ethz.ch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. Navigate the file system\n",
    "\n",
    "```bash\n",
    "$ ls -a # list all files from your home directory\n",
    "\n",
    "$ cd $SCRATCH # change into the global scratch directory\n",
    "$ pwd\n",
    "/cluster/scratch/username\n",
    "\n",
    "$ cd $TMPDIR\n",
    "$ pwd\n",
    "/cluster/home/username # why is the current directory Home and not local scratch? \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. Get familiar with the python modules available on the cluster.\n",
    "\n",
    "```bash\n",
    "$ env2lmod\n",
    "$ module list\n",
    "$ module spider python\n",
    "$ module load gcc/8.2.0 python/3.10.4\n",
    "$ module list\n",
    "$ python --version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4. Exit from the cluster.\n",
    "\n",
    "```bash\n",
    "$ exit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "5. Locate the `fast-python` repo and copy the `./scripts/section_6/euler_scripts` to the home directory of your cluster.\n",
    "\n",
    "```bash\n",
    "$ cd <fast-python-repo>\n",
    "$ scp -r ./scripts/section_6/euler_scripts <username>@euler.ethz.ch:~\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "6. Login to the cluster again and check the content of the `~/euler_scripts` directory\n",
    "\n",
    "```bash\n",
    "$ ssh <username>@euler.ethz.ch\n",
    "\n",
    "$ ls ~/euler_scripts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch System\n",
    "\n",
    "<p>\n",
    "<img src=\"./images/Cluster.png\" width=\"640\">\n",
    "<div>Source: <a href=https://scicomp.ethz.ch/wiki/File:Accessing_the_clusters.png>ETHZ scicomp wiki</a></div>\n",
    "</p>\n",
    "\n",
    "* The batch system is responsible for scheduling jobs.  \n",
    "* Different batch systems exist: SGE, LSF, SLURM, PBS, HTCondor, etc\n",
    "* Each system comes with its own features and syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ongoing transition to SLURM\n",
    "* The cluster support team is in the process of transitioning the Euler's batch system from **LSF** to **SLURM**\n",
    "* By the end of 2022 the majority of nodes will have been migrated to SLURM\n",
    "* If you are still using LSF, it is recommended to migrate to SLURM as soon as possible \n",
    "* Here, we will only discuss SLURM, not LSF\n",
    "* SLURM docs in the scicomp wiki:\n",
    "  * For more information on the transition, see the \n",
    "  [Transition from LSF to Slurm](https://scicomp.ethz.ch/wiki/Transition_from_LSF_to_Slurm)\n",
    "  * For more information on SLURM commands and options, see the \n",
    "  [LSF to Slurm quick reference](https://scicomp.ethz.ch/wiki/LSF_to_Slurm_quick_reference)\n",
    "  * [LSF/Slurm submission line advisor](https://scicomp.ethz.ch/public/lsla/index2.html) (helper tool to create submission commands or jobscripts for LSF and Slurm; we'll take a look together later)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Check cluster status\n",
    "\n",
    "```bash\n",
    "$ sinfo\n",
    "$ sinfo -Nel # 1 line per node and more info\n",
    "NODELIST        NODES   PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON              \n",
    "...\n",
    "eu-a2p-273          1  normal.24h       mixed 128    2:64:1 249200   842000    200 AMD,EPYC none                \n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Job types\n",
    "\n",
    "* Batch job\n",
    "  * Create job script\n",
    "  * Submit job to cluster\n",
    "  * Do something else until the job has finished\n",
    "  * Inspect results\n",
    "* Interactive job\n",
    "  * Submit interactive job\n",
    "  * Get prompt on compute node\n",
    "  * Run commands interactively\n",
    "  * Exit interactive job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Job types: use cases\n",
    "\n",
    "* Batch job\n",
    "  * Standard approach for data analysis\n",
    "* Interactive job\n",
    "  * Debugging a batch job (you have the same environment as in the batch job and can inspect states and errors interractively)\n",
    "  * GUIs (via X11-forwarding; see [scicomp wiki](https://scicomp.ethz.ch/wiki/X11_forwarding_batch_interactive_jobs))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Batch jobs\n",
    "* Create job file\n",
    "* Submit to cluster (`sbatch`)\n",
    "* SLURM adds your job to the queue (`squeue`) \n",
    "* SLURM will schedule your job and execute it on a compute node once resources \n",
    "are available\n",
    "* Check if your job is complete/failed/pending\n",
    "* Output (stdout/stderr) is written to log file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Create job file\n",
    "    * Include bash shebang (`#!/bin/bash`)\n",
    "    * Include a request for resources (starting with `#SBATCH` pragma; more on job options later)\n",
    "    * Include commands to run (e.g., run Python script)\n",
    "    \n",
    "`job.sh`:\n",
    "```bash \n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH -c 2                              # Number of cpus (default: 1)\n",
    "#SBATCH --time=2:00:00                    # Wall-clock time hours:minutes:seconds (default: 4h)\n",
    "#SBATCH --mem-per-cpu=2G                  # 2 GB\n",
    "#SBATCH --tmp=4000                        # temporary disk space per node (default units MB [M])\n",
    "#SBATCH --job-name=analysis1\n",
    "#SBATCH --output=analysis1.out\n",
    "#SBATCH --error=analysis1.err\n",
    "\n",
    "hostname\n",
    "python ~/data_analysis.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use `sbatch` to submit a job to the batch system\n",
    "```bash\n",
    "$ module load gcc/8.2.0 python/3.10.4\n",
    "$ sbatch job.sh\n",
    "Submitted batch job <jobid>\n",
    "```\n",
    "\n",
    "\n",
    "* SLURM analyzes the job requirements and dispatches it to the job **queue**. \n",
    "* Once there are available resources, the job will start running.\n",
    "* We recommend to load all required modules before submitting a job because the batch system uses module information to better schedule a job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch jobs without job script\n",
    "\n",
    "via the `--wrap` flag:\n",
    "\n",
    "`sbatch --wrap \"python ~/data_analysis.py\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Interactive jobs\n",
    "* Request an interactive job with specific resources\n",
    "* SLURM will schedule your job and execute it on a compute node once resources \n",
    "are available\n",
    "* You receive a prompt on a compute node to run your commands interactively\n",
    "* Output is printed directly in the terminal\n",
    "* Interactive jobs offer\n",
    "* Exit interactive job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use `srun` to submit an interactive job with the default options\n",
    "```bash\n",
    "user@eu-login$ srun --pty bash  # submit interactive job\n",
    "user@eu-compute$ pwd            # run command on compute node\n",
    "/cluster/home/user\n",
    "user@eu-compute$ exit           # exit interactive job\n",
    "exit\n",
    "user@eu-login$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**:\n",
    "\n",
    "`srun --time 1:00:00 -c 2 --mem-per-cpu 2G --pty bash`\n",
    "\n",
    "Requests an interactive job with a max time of 1h, 2 CPUs, and 2x2GB memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Resource requirements\n",
    "The batch system of Euler works like a black box:\n",
    "- We do not need to know anything about queues, hosts, user groups, priorities, etc. to use it.\n",
    "- We only need to specify the resources needed by our job.\n",
    "\n",
    "Resources are requested:\n",
    "- via the `#SBATCH` pragma (directive) in job scripts, or\n",
    "- as options when calling SLURM submission commands (`sbatch` and `srun`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The most important resources are:\n",
    "- **maximal run-time** `--time <HH:MM:SS>` (default 4 hours) \n",
    "- **memory** `--mem-per-cpu <memory>` (default 1024 MB per CPU)\n",
    "- Multiprocessing and parallel jobs\n",
    "  - **number  of CPUs per task** \n",
    "    - `-c <number_of_cpus>`, (or `--cpus-per-task`; default 1)\n",
    "    - Use this option for multi-threaded jobs\n",
    "  - **number of tasks** \n",
    "    - `-n <number_of_tasks>`, (or `--ntasks`; default 1) for parallel jobs\n",
    "    - Use this option for parallel tasks (e.g., MPI jobs)\n",
    "\n",
    "Note that:\n",
    "* Options passed to the command will override the options in the job script.\n",
    "* If you request more resources than are available, the job will queue forever (you can kill it).\n",
    "* Docs\n",
    "    * Further information on resource options can be found in the \n",
    "    [scicomp wiki - LSF to Slurm quick reference](https://scicomp.ethz.ch/wiki/LSF_to_Slurm_quick_reference)\n",
    "    * You can assemble a jobscript with the desired options using the [LSF/Slurm submission line advisor](https://scicomp.ethz.ch/public/lsla/index2.html) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### GPUs\n",
    "\n",
    "To request GPUs:\n",
    "* `--gpus=1`\n",
    "* `--gpus=gtx_1080:1` (for a specific GPU model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 2: submission advisor [5 min]\n",
    "  * Open the [LSF/Slurm submission line advisor](https://scicomp.ethz.ch/public/lsla/index2.html)\n",
    "  * Select \"Slurm\" as \"Batch System\"\n",
    "  * Tweak the options and click \"display command/script\"\n",
    "  * Also switch between \"command\" and \"jobscript\"\n",
    "  \n",
    "  <p>\n",
    "<img src=\"./images/submission-advisor.png\" width=\"640\">\n",
    "</p>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Job queue\n",
    "```\n",
    "$ squeue\n",
    "$ squeue -i 5   # (iterate) refresh all 5 seconds\n",
    "$ squeue -l     # (--long) slightly more verbose output\n",
    "```\n",
    "\n",
    "```\n",
    "$ squeue -l\n",
    "JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n",
    "3266  normal.4h analysis     user  RUNNING       1:10   2:00:00      1 eu-a2p-524\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Resource usage information of running jobs\n",
    "\n",
    "```bash\n",
    "$ sstat -a --format JobID,AveCPU,AveRSS,MaxRSS -j <jobid>.batch\n",
    " Â  Â  Â  Â JobID     AveCPU     AveRSS     MaxRSS \n",
    "-------------- ---------- ---------- ---------- \n",
    "<jobid>.batch   00:14.000   8745040K   8745040K \n",
    "```\n",
    "\n",
    "* `AveCPU`: Average (system + user) CPU time of all tasks in the job. Example: 2 CPUs running 100% for one minute, will result in 2 minutes of `AveCPU` time\n",
    "* `AveRSS`: Average resident set size, i.e., the average memory usage of the job.\n",
    "* `MaxRSS`: Maximum resident set size, i.e., the maximum memory usage of the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Details on past jobs\n",
    "```bash\n",
    "$ sacct -X --format JobID,Elapsed,State,ExitCode -j <jobid>.batch\n",
    "```\n",
    "\n",
    "**Example successful job**\n",
    "```\n",
    "$ sacct -X --format JobID,Elapsed,State,ExitCode -j 3284\n",
    "JobID           Elapsed      State ExitCode \n",
    "------------ ---------- ---------- -------- \n",
    "3284           00:01:43  COMPLETED      0:0 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example failed job (exit code 1)**\n",
    "```\n",
    "$ sacct -X --format JobID,State,Elapsed,ExitCode -j 3286\n",
    "JobID           Elapsed      State ExitCode \n",
    "------------ ---------- ---------- -------- \n",
    "3286           00:00:01     FAILED      1:0 \n",
    "```\n",
    "\n",
    "**Example failed job (out of memory)**\n",
    "```\n",
    "$ sacct -X --format JobID,Elapsed,State,ExitCode -j 11349\n",
    "JobID           Elapsed      State ExitCode \n",
    "------------ ---------- ---------- -------- \n",
    "11349          00:00:05 OUT_OF_ME+    0:125  \n",
    "```\n",
    "\n",
    "For information on the job state codes, see the [official SLURM docs](https://slurm.schedmd.com/sacct.html#SECTION_JOB-STATE-CODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Resource usage information of past jobs**\n",
    "```bash\n",
    "$ sacct --format JobID,AveCPU,AveRSS,MaxRSS -j <jobid>.batch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Output of jobs\n",
    "* SLURM creates an output file for each job. \n",
    "* By default the name of the file is `slurm-<jobid>.out`.\n",
    "* If specified, `stdout` will be written to the file specified in `--output`, \n",
    "`sterr` to the `--error` file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cancelling a job\n",
    "Cancel pending or running jobs:\n",
    "```bash\n",
    "$ scancel <jobid>\n",
    "$ scancel -u $USER  # cancel all your jobs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 3: job submission & management [20 min]\n",
    "\n",
    "\n",
    "### Account\n",
    "* In the next exercises you should replace `<username>` with the training account and use the corresponding password.\n",
    "* After the workshop you could use your nethz account (the user that you use to login to [mail.ethz.ch](https://mail.ethz.ch/)). The `password` is also the same as the one used for your email address.\n",
    "\n",
    "### Goals\n",
    "* login to Euler\n",
    "* inspect the files required to run the job\n",
    "* submit the job\n",
    "* observe its status & output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Login to the cluster.\n",
    "\n",
    "```bash\n",
    "$ ssh <username>@euler.ethz.ch\n",
    "\n",
    "# Example:\n",
    "$ ssh msmith@euler.ethz.ch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. Check the content of the Python script `~/euler_scripts/job_summary.py` and the job scipt .\n",
    "\n",
    "```bash\n",
    "$ cat ~/euler_scripts/job_summary.py \n",
    "$ cat ~/euler_scripts/job.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. Load modules, submit the script, and identify its `<jobid>`\n",
    "\n",
    "```bash\n",
    "$ module load gcc/8.2.0 python/3.10.4\n",
    "$ sbatch ~/euler_scripts/job.sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4. Check the job status and the queue using `squeue` and/or `sstat`.\n",
    "\n",
    "```bash\n",
    "$ squeue -j <jobid>\n",
    "$ sstat -a --format JobID,AveCPU,AveRSS,MaxRSS -j <jobid>.batch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "5. Once the job is done list the files that start with `slurm-`, and check that you have one that includes the `<jobid>` of the previous job.\n",
    "\n",
    "```bash\n",
    "$ ls -l slurm-*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "6.  Display the content of the output file corresponding to your job\n",
    "\n",
    "```bash\n",
    "$ cat slurm-<jobid>*\n",
    "```\n",
    "\n",
    "Note the differences between `SLURM_CPUS_PER_TASK` and `os.cpu_count()`. What's the implication for multi-core python code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "7. Check the job status and exit code. \n",
    "\n",
    "```bash\n",
    "$ sacct -X --format JobID,State,Elapsed,ExitCode -j <jobid>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T14:27:19.433752Z",
     "iopub.status.busy": "2020-11-09T14:27:19.431419Z",
     "iopub.status.idle": "2020-11-09T14:27:19.463988Z",
     "shell.execute_reply": "2020-11-09T14:27:19.456940Z",
     "shell.execute_reply.started": "2020-11-09T14:27:19.432284Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dos and Don'ts\n",
    "\n",
    "**Dos**\n",
    "\n",
    "- Understand what we are doing\n",
    "- Ask for help if we don't understand what we are doing, e.g. write to cluster-support@id.ethz.ch\n",
    "- Use the wiki https://scicomp.ethz.ch\n",
    "- Optimize our workflow to make it as efficient as possible\n",
    "    - Jobs with shorter expected running time (`--time HH:MM:SS`) are likely to get the compute resources faster.\n",
    "- Keep in mind that our clusters are shared by many users\n",
    "- Carefully choose the file system you want to use\n",
    "- Try to have jobs of at least 5 minutes\n",
    "- Before requesting multiple cores, check whether the program supports parallelism and how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-09T14:27:19.433752Z",
     "iopub.status.busy": "2020-11-09T14:27:19.431419Z",
     "iopub.status.idle": "2020-11-09T14:27:19.463988Z",
     "shell.execute_reply": "2020-11-09T14:27:19.456940Z",
     "shell.execute_reply.started": "2020-11-09T14:27:19.432284Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Don'ts**\n",
    "\n",
    "- Don't waste CPU time or disk space\n",
    "- Don't run applications on the login nodes\n",
    "    - The login nodes are shared between many users and we can negatively impact the experience.\n",
    "    - You should run them on compute nodes by using an interactive batch job, e.g.:\n",
    "    ```bash\n",
    "    $ srun --pty bash\n",
    "    ```\n",
    "- Don't write large amounts of data to standard output\n",
    "    - The size of standard output (`/cluster/shadow`) is finite, so you might lose it. Write or redirect to a file (see [here](https://scicomp.ethz.ch/wiki/Too_much_space_is_used_by_your_output_files)).\n",
    "- Don't run module commands within a job or job script\n",
    "    - We strongly recommend to load all needed modules before submitting a job because the batch system uses module information to better schedule a job.\n",
    "- Don't create millions of small files\n",
    "    - The file system used by **Global scratch** and **Work** is not design for a lot of small files. The entire storage can be negatively impacted.\n",
    "- Don't use conda (because it creates a lot of small files)\n",
    "- Don't run hundreds of small jobs if the same work can be done in a single job\n",
    "    - The batch system has to do some work to put the job in the queue. By creating hundreds of small jobs the batch system is negatively impacted.\n",
    "    - Use job arrays if possible (see  [Job Arrays](#JobArrays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Check questions [10min]\n",
    "\n",
    "1. How many users can use an HPC cluster at the same time?\n",
    "2. What is the difference between a compute and a login node?\n",
    "3. Who is scheduling the jobs?\n",
    "4. What is a module?\n",
    "5. How can you get Python 3.10 on Euler?\n",
    "6. How do you test a code / script on Euler?\n",
    "7. Imagine that you are using two different HPC clusters, each of them with a different batch system. What type of problems do you expect to have when you try to move your job from one cluster to another: think about the nodes, data, environment and batch system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "solution"
    ]
   },
   "source": [
    "**Solution**\n",
    "1. A lot since the HPC cluster is shared between the users.\n",
    "2. The login nodes allow users to login to the cluster and submit jobs, and the compute nodes are running the jobs (doing the computation).\n",
    "3. The batch system (SLURM on Euler).\n",
    "4. The way to configure our computing environment for specific tools.\n",
    "5. We activate the new software stack if desired via `env2lmod` and load the modules `module load gcc/8.2.0 python/3.10.4` .\n",
    "6. Via an interactive job. More precisely, in case we want to experiment on a compute node, we can start a new terminal with `srun --pty bash` .\n",
    "7. A lot of problems:\n",
    "    - different nodes so the time to solution might be different, big memory nodes might not be available;\n",
    "    - the storage layouts may differ and be optimized for different kinds of data\n",
    "    - for the environment we have to test whether all applications are available. The environment might work differently, e.g. we have to reload all modules when the jobs are running on the compute node;\n",
    "    - for the batch system we have to check how to do the daily business: submit jobs, monitor, kill, interactive jobs, ask for resources, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further Reading\n",
    "- https://scicomp.ethz.ch\n",
    "- https://slurm.schedmd.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Optional Topics\n",
    "## Batch System: Advanced Topic - Parallel/Dependent Jobs \n",
    "<a name=\"BatchParallel\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Embarrassingly Parallel Jobs\n",
    "\n",
    "![](./images/job_array.svg)\n",
    "\n",
    "Each job submitted to the batch system is introducing a small amount of work to the batch system.  \n",
    "For the case of multiple similar jobs we can avoid this overhead by submitting all jobs at once using the so-called \"Job Arrays\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the following example we submit an job array of 4 jobs:\n",
    "\n",
    "```\n",
    "$ sbatch --array=1-4 --wrap 'echo \"Hello, I am task $SLURM_ARRAY_TASK_ID of $SLURM_ARRAY_TASK_COUNT\"'\n",
    "```\n",
    "* `--wrap` allows to submit a batch job without a job script\n",
    "* The `SLURM_ARRAY_TASK_ID` environmental variable can be used in scripts to select (input files, patient IDs, list entry....)\n",
    "* In Python you can read environment variables using `os.environ` which is a dictionary mapping names of variables to their values as strings.\n",
    "* You can limit the number of simultaneously running tasks using the `%` sign (e.g., `--array=1-4%2` will only run 2 tasks at a time)\n",
    "* You can define the `--array` via the command line or inside a job script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So in case you want to process 1000 files in parallel you could:\n",
    "\n",
    "1. create a text file with the names of the 1000 files\n",
    "2. submit a job array using `sbatch --array=1-1000%20 process.py` to start 1000 jobs, 20 running at the same time\n",
    "3. `process.py` \n",
    "    - reads the list of file names and picks the file name at line `int(os.environ[\"SLURM_ARRAY_TASK_ID\"]) - 1` (Python start index is `0`)\n",
    "    - processes this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Key aspects*\n",
    "\n",
    "- All jobs in an array share the same `<jobid>`.  \n",
    "- To refer to a single task (e.g., `scancel`), use `<jobid>_<taskid>`\n",
    "- Each task can have its own standard output (default: `slurm-<jobid>_<taskid>.out`).\n",
    "\n",
    "*For more information on SLURM job arrays, see*\n",
    "* [LSF to Slurm quick reference](https://scicomp.ethz.ch/wiki/LSF_to_Slurm_quick_reference)\n",
    "* [SLURM docs](https://slurm.schedmd.com/job_array.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dependent Jobs - Job Chaining<a name=\"JobChaining\"></a>\n",
    "\n",
    "\n",
    "![](./images/job_chaining.svg)\n",
    "\n",
    "\n",
    "In case the output of a job is needed as input for another job, one can use `--dependency` to wait until the required job has completed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example we can create a chain `Job_1+Job_2->Job_3->Job_4` and use the job names to specify the dependency:\n",
    "\n",
    "```bash\n",
    "$ sbatch -J Job_1 --wrap 'date; echo \"I am job $SLURM_JOB_ID\"; sleep 20'\n",
    "Submitted batch job 9551\n",
    "$ sbatch -J Job_2 --wrap 'date; echo \"I am job $SLURM_JOB_ID\"; sleep 20'\n",
    "Submitted batch job 9552\n",
    "$ sbatch -J Job_3 --dependency=afterok:9551,afterok:9552 --wrap 'date; echo \"I am job $SLURM_JOB_ID\"; sleep 10'\n",
    "Submitted batch job 9553\n",
    "$ sbatch -J Job_4 --dependency=afterok:9553 --wrap 'date; echo \"I am job $SLURM_JOB_ID\"; sleep 10'\n",
    "Submitted batch job 9554\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "or via the job name <jobid> (simplified example)\n",
    "\n",
    "```bash\n",
    "$ sbatch -J Job_1 --wrap 'date; echo \"I am job $SLURM_JOB_ID\"; sleep 20'\n",
    "$ jobid=$(squeue --noheader --format %i --name Job_1)\n",
    "$ sbatch -J Job_2 --dependency=afterok:$jobid --wrap 'date; echo \"I am job $SLURM_JOB_ID\"; sleep 20'\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Warning!</strong> \n",
    "  Using sophisticated job chaining can introduce a dependency on the batch system. Therefore running our workflow on a cluster with a different batch system might require to rewrite the job chaining part.</br>\n",
    "  Workflow management systems that provide integration with various HPC system can overcome this.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workflow Management System + HPC integration <a name=\"WMS\"></a>\n",
    "\n",
    "As we mentioned in the [Job Chaining](#JobChaining) subsection, the workflow management systems allow us to specify the dependency between the jobs independently of the batch system used.   \n",
    "The jobs (also called tasks or rules) determine a **Directed Acyclic DAG (DAG)**.  \n",
    "In practice one usually defines for each rule the input and output files, and the DAG is determined directly by the workflow management system.  \n",
    "Nodes of this graph are tasks and edges define the dependencies between those tasks.\n",
    "\n",
    "\n",
    "![](./images/DAG.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A simple workflow may consist of multiple scripts that need to be run in a dependent manner, but there are also parts that can profit from parallelization, e.g. `Job_1` and `Job_2` can run concurrently.  \n",
    "\n",
    "Since the input and output files are available, they can be used as checkpoints, i.e. in case `Job_4` fails there is no need to recompute the previous jobs, since their output is available in the corresponding output files.\n",
    "       \n",
    "Examples: [Airflow](https://airflow.apache.org/),[Luigi](https://luigi.readthedocs.io), [Nextflow](https://www.nextflow.io/), [Snakemake](https://snakemake.readthedocs.io/), [Nipype](https://nipype.readthedocs.io/), ...\n",
    "\n",
    "These tools allow for easy scaling up and out on cloud or HPC clusters, e.g. for HPC clusters:\n",
    "- [Snakemake](https://snakemake.readthedocs.io/): SGE, LSF, SLURM, PBS, HTCondor, DRMAA, ...\n",
    "- [Nextflow](https://www.nextflow.io/): SGE, LSF, SLURM, PBS and HTCondor,\n",
    "- [Luigi](https://luigi.readthedocs.io): SGE and LSF.\n",
    "- [Nipype](https://nipype.readthedocs.io/): SGE, PBS, HTCondor, LSF and SLURM\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Warning!</strong> \n",
    "  Wokflow management systems generally do not provide Job Array integration.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "rise": {
   "scroll": true,
   "transition": null
  },
  "toc-autonumbering": true,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "c6615654c775c122ec0c506a731be37c7d340b3aad37a4f4c607fc5395f0682b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
