{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS CELL WHICH CUSTOMIZES LAYOUT AND STYLING OF THE NOTEBOOK !\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings = lambda *a, **kw: None\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(open(\"../documents/custom.html\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<span style=\"background:#f0f0e0;padding:1em\">Copyright (c) 2020-2021 ETH Zurich, Scientific IT Services. This work is licensed under <a href=\"https://creativecommons.org/licenses/by-nc/4.0/\">CC BY-NC 4.0</a></span><br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 2.5em; font-weight: bold;\">Section 5: Parallel computing on a PC</p> \n",
    "\n",
    "In modern scientific computing **parallel computing** is omnipresent.\n",
    "\n",
    "Nowadays, it is common to have multicore processors even on a laptop. This opens up opportunities to solve some of the computational problems faster by leveraging these parallel architectures. \n",
    "\n",
    "Solving a problem means identifying the steps (or set of instructions) that allow us to solve it, i.e. the **algorithm**. If all the steps are executed in a non-parallel fashion, then the algorithm is said to be **serial**.\n",
    "\n",
    "In simple terms, **parallel computing** refers to breaking down of the computational task into smaller \"independent\" chunks and executing these chunks concurrently (may or may not be truly parallel) or in-parallel using different mechanisms. In practice, we might have to reconsider the initial **serial** algorithm for one that allows better parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking parallel\n",
    "\n",
    "Before embarking on a journey to start parallelizing our code, we first need to think if the problem at hand is parallelizable at all. \n",
    "\n",
    "For example, an algorithm in which the computation at any given step depends on the previous steps cannot be parallelized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style=\"font-weight: bold;\"><i class=\"fa fa-info-circle\"></i> A typical serial problem: Fibonacci series (0,1,1,2,3,5,8...)</p>\n",
    "    \n",
    "$F(n) = F(n-1) + F(n-2)$\n",
    "    \n",
    "In the formula stated above the function $F$ at step $n$ depends on its value at steps $n-1$ and $n-2$, respectively. Therefore, we need to compute these values before we can go ahead with the computation at step $n$. This procedure does not lends itself to parallelization.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the problem is parallelizable, then there are two prominent ways we can approach it:\n",
    "\n",
    "* **Domain decomposition**\n",
    "\n",
    "In this approach, one aims to partition the data which needs to be processed such that the tasks/instructions can operate on separate chunks of data, simultaneously.\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"./images/domain_decomp.gif\" width=\"300\" alt=\"domain\"/>\n",
    "    <div style=\"text-align:center;\">Source: <a href=\"https://hpc.llnl.gov/training/tutorials/introduction-parallel-computing-tutorial\">LLNL Introduction to Parallel Computing Tutorial</a></div>\n",
    "</p>\n",
    "\n",
    "\n",
    "* **Functional decomposition**\n",
    "\n",
    "In this approach, the computation task itself is partitioned such that different tasks do parts of the overall work.\n",
    "\n",
    "<p  style=\"text-align:center;\">\n",
    "    <img src=\"./images/functional_decomp.gif\" width=\"400\" alt=\"domain\"/>\n",
    "    <div style=\"text-align:center;\">Source: <a href=\"https://hpc.llnl.gov/training/tutorials/introduction-parallel-computing-tutorial\">LLNL Introduction to Parallel Computing Tutorial</div></a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling: Can I just throw in infinite CPUs to get infinite speedup?\n",
    "\n",
    "**Short answer**: of course, NO!\n",
    "\n",
    "**Detailed answer**\n",
    "\n",
    "In order to give a detailed answer, we first need to set the stage.\n",
    "\n",
    "Let's assume that we have a problem that can be parallelized. Following that we assume that the problem depends linearly on its input size $N$ (called also **problem size**) and can executed on $P$ processors. \n",
    "\n",
    "Let's say we can solve the problem using the **fastest serial** algorithm in time $T_S$. \n",
    "\n",
    "We can also use a parallel algorithm on $P$ processor and solve the same problem in time $T(P,N)$. In other words, problem of size $N$ is solved on $P$ processors in time $T(P,N)$. \n",
    "\n",
    "It is important to notice that generally when using the parallel algorithm on $P=1$ processors the problem is solved in $T(1,N) \\ge T_S$. \n",
    "\n",
    "The parallel algorithm usually contains a fraction $f$ that is inherently serial and cannot be parallelized.\n",
    "\n",
    "To quantify the benefit obtained from parallelization, it is useful to define some metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<div style=\"font-size: 150%;\"></div>\n",
    "<i class=\\\"fa fa-info-circle\\\"></i>\n",
    "\n",
    "**Absolute Speedup**: \n",
    "    \n",
    "    \n",
    "Ratio of the time take by the **best** serial algorithm to solve a particular problem ($T_S$) to the parallel execution time on $P$ processors ($T(P,N)$).\n",
    "    \n",
    "    \n",
    "$$S^{abs}(P,N) :=\\frac{\\textrm{time taken by the best serial algorithm}}{\\textrm{time taken by the parallel algorithm on } P \\textrm{ processors}} = \\frac{T_S}{T(P,N)} .$$\n",
    "    \n",
    "    \n",
    "    \n",
    "**Relative Speedup**: \n",
    "    \n",
    "Ratio of serial time to the parallel execution time on $P$ processors using the parallel algorithm\n",
    "$$S(P,N) := \\frac{T(1,N)}{T(P,N)} \\ge \\frac{T_S}{T(P,N)} = S^{abs}(P,N).$$\n",
    "    \n",
    "    \n",
    "    \n",
    "**Relative Efficiency**:\n",
    "    \n",
    "Ratio of the speedup to the number of processors used. Measures the fraction of the time for which each processor is usefully utilized\n",
    "$$E(P,N) := \\frac{S(P,N)}{P} .$$\n",
    "    \n",
    "    \n",
    "    \n",
    "**Scalability**:\n",
    "    \n",
    "It is a measure of a parallel algorithm's increase in speedup with an increasing number of processors\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two type of **scalabilities** (strong and weak) which you will often see in literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong scaling - Amdahl's law\n",
    "\n",
    "Strong scaling concerns the speed up achieved by increasing the number of processors ($P$) while the **problem size ($N$) remains fixed**.\n",
    "\n",
    "The speedup with respect to strong scaling is expressed by the so-called **Amdahl's law**. \n",
    "\n",
    "Let's say an algorithm contains a fraction $f$ of work which is serial (not parallelizable) (Therefore $1-f$ is the fraction of work that is parallelizable).\n",
    "\n",
    "According to Amdahl's law, the theoretical speedup obtained by running this algorithm on $P$ processors is given by:\n",
    "\n",
    "$$S(P,N) = \\frac{1}{f + \\frac{1-f}{P}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style=\"font-weight: bold;\"><i class=\"fa fa-info-circle\"></i>Derivation:</p>\n",
    "\n",
    "As stated above, the execution time of a parallel algorithm on a single processor is $T(1,N)$. Let's assume an algorithm, fraction $f$ of which is non parallelizable.\n",
    "We can express the time taken by the parallel code using $P$ processors is:\n",
    "    \n",
    "$$T(P,N)=f\\times T(1,N) + (1-f)\\times \\frac{T(1,N)}{P} = T(1,N) \\times \\left( f + \\frac{1-f}{P}\\right) .$$\n",
    "\n",
    "Then the theoretical speedup of this algorithm is:\n",
    "\n",
    "$$S(P,N) = \\frac{T(1,N)}{T(P,N)}=\\frac{T(1,N)}{T(1,N) \\times \\left( f + \\frac{1-f}{P}\\right)} = \\frac{1}{f + \\frac{1-f}{P}} \\le \\frac{1}{f} .$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the problem can be solved $P$ times faster on $P$ processors only when $f=0$ (i.e. there is no serial part). When $f$ is non-zero the maximum theoretical speedup which corresponds to an infinite number of processors is\n",
    "$$S_{max}=\\frac{1}{f}$$ because  $\\frac{1-f}{p} \\to 0$ and therefore it is given by the serial fraction.\n",
    "\n",
    "To demonstrate the importance of this fact, let's say that we are very lucky and only 10% of our code is non-parallelizable and the rest 90% is.  \n",
    "Let's plot the speedup as a function of number of processors we throw in to solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def amdahl_speedup(f, P):\n",
    "    speedup = 1 / (f + ((1 - f) / P))\n",
    "    return speedup\n",
    "\n",
    "\n",
    "def speedup_plot(speedup, f, P, title):\n",
    "    max_speedup = round(1 / f, 2)\n",
    "    plt.plot(P, speedup, label=\"f={}, S_max ={}\".format(f, max_speedup))\n",
    "    # Add title and axis names\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Number of processors\")\n",
    "    plt.ylabel(\"Speedup\")\n",
    "    # Limits for the Y axis\n",
    "    plt.xlim(0, num_processors + 10)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "num_processors = 200\n",
    "P = np.arange(1, num_processors, 1, dtype=int)\n",
    "f = [0.1]\n",
    "for fi in f:\n",
    "    speedup = amdahl_speedup(fi, P)\n",
    "    speedup_plot(speedup, fi, P, \"Amdahl's law\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the speedup as a function of the serial fraction of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processors = 200\n",
    "P = np.arange(1, num_processors, 1, dtype=int)\n",
    "f = [0.1, 0.25, 0.33, 0.5]\n",
    "for fi in f:\n",
    "    speedup = amdahl_speedup(fi, P)\n",
    "    speedup_plot(speedup, fi, P, \"Amdahl's law\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above we can see that the maximum speedup obtainable for an algorithm to solve a problem of fixed size is strictly limited by its serial fraction.\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<div style=\"font-size: 150%;\"></div>\n",
    "<i class=\\\"fa fa-info-circle\\\"></i>\n",
    "\n",
    "**Take home message:** we are limited in the speedup by the serial fraction and we cannot efficiently use more and more processors. As stated above, the maximum speedup obtainable is give by\n",
    "\n",
    "$$S_{max}=\\frac{1}{f}$$\n",
    "</div>\n",
    "\n",
    "Question to audience:\n",
    "**Why do we need big clusters and supercomputers?!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak scaling - Gustafson's law\n",
    "\n",
    "In this scaling analysis, the problem size ($N$) is increased proportional to the number of processors added ($P$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speedup with respect to weak scaling is expressed by the so-called **Gustafson's law**. \n",
    "\n",
    "Let's consider that the parallel work performed varies linearly with the increased number of processors such that the **execution time stays constant** and that in such a case the **serial component is constant** and is therefore is independent of the problem size.\n",
    "\n",
    "Let's say a parallel algorithm takes time $T_{serial}$ for the serial part and  $T_{parallel}$ for the parallel part on $P$ processors:\n",
    "$$T(P,N)= T_{serial} + T_{parallel} .$$ \n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"./images/gustafson.png\" width=600 alt=\"gustafson\"/>\n",
    "    <div style=\"text-align:center;\">Based on:  <a href=\"https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-09766-4_78\">Encyclopedia of Parallel Computing - Gustafson‚Äôs Law</a></div>\n",
    "</p>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Gustafson's law, the effective speedup when running ùëÉ processors is given by:\n",
    "\n",
    "$$S(P,N) = \\alpha + P \\times (1-\\alpha)$$\n",
    "\n",
    "Where,\n",
    "\n",
    "$$\\alpha =  \\dfrac{T_{serial}}{T_{serial} + T_{parallel}}$$ \n",
    "\n",
    "Therefore, by keeping the execution time constant we can really **scale linearly** with the number of processors. The main idea is to increase the problem size $N$ and at the same time increase the number of processors $P$ such that the execution time remains constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style=\"font-weight: bold;\"><i class=\"fa fa-info-circle\"></i>Derivation:</p>\n",
    "\n",
    "Let's say a parallel algorithm takes time $T_{serial}$ for the serial part and  $T_{parallel}$ for the parallel part on $P$ processors:\n",
    "$$T(P,N)= T_{serial} + T_{parallel} .$$ \n",
    "    \n",
    "On one processor it would take:\n",
    "\n",
    "$$T(1,N) = T_{serial} + P \\times T_{parallel}.$$\n",
    "\n",
    "So the speedup\n",
    "$$S(P,N) = \\frac{T(1,N)}{T(P,N)}= \\frac{ T_{serial} + P \\times T_{parallel}}{T_{serial} + T_{parallel}}$$ \n",
    "    \n",
    "    \n",
    "Let \n",
    "$$\\alpha =  \\dfrac{T_{serial}}{T_{serial} + T_{parallel}}$$ \n",
    "be the serial fraction of the execution time, which is a constant.\n",
    "    \n",
    "Therefore \n",
    "$$S(P,N) =\\frac{ T_{serial}}{T_{serial} + T_{parallel}} +P \\times  \\frac{ T_{serial} + T_{parallel}-T_{serial}}{T_{serial} + T_{parallel}} = \\alpha + P \\times (1-\\alpha) .$$\n",
    "\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def gustafson_speedup(alpha, P):\n",
    "    speedup = P - alpha * (P - 1)\n",
    "    return speedup\n",
    "\n",
    "\n",
    "def speedup_plot(speedup, alpha, P, title):\n",
    "    plt.plot(P, speedup, label=r\"$\\alpha$={}\".format(alpha))\n",
    "    # Add title and axis names\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Number of processors\")\n",
    "    plt.ylabel(\"Speedup\")\n",
    "    # Limits for the Y axis\n",
    "    plt.xlim(0, num_processors + 10)\n",
    "    plt.ylim(0, None)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "num_processors = 200\n",
    "P = np.arange(1, num_processors, 1, dtype=int)\n",
    "alpha = [0.1, 0.3, 0.5, 0.7]\n",
    "for ai in alpha:\n",
    "    speedup = gustafson_speedup(ai, P)\n",
    "    speedup_plot(speedup, ai, P, \"Gustafson's law\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good weak scaling means we can solve $N$ times bigger problem in similar time by using $N$ times the processors used for the smaller problem.\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<div style=\"font-size: 150%;\"></div>\n",
    "<i class=\\\"fa fa-info-circle\\\"></i>\n",
    "    \n",
    "**Take home message:** speedup can be obtained by solving larger problems on larger computing resources.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<div style=\"font-size: 150%;\"><i class=\"fa fa-info-circle\"></i>&nbsp;Important</div>\n",
    "\n",
    "* Not every algorithm lends itself to parallelization\n",
    "* Algorithm that lends itself to good parallelization may not be the best for single core performance\n",
    "* In some problems there are other overheads such as communication between the parallel parts    \n",
    "* Choosing the right algorithm is a critical decision\n",
    "\n",
    "So the message is that choose algorithms with the end goal in mind.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does a computer program work, a (very) brief summary and recap\n",
    "\n",
    "Before we delve into parallel processing using python we take a short look into how a computer program works.\n",
    "\n",
    "When we run a program we start a **process** which is an instance of that program's execution. \n",
    "\n",
    "For example, if you run a serial python program 10 times **from the command line** you will have created 10 processes, even though they are running the same program.\n",
    "\n",
    "**Note:** running cells in a notebook all happen in the same process.\n",
    "\n",
    "A process and its associated resources are managed by the operating system.\n",
    "\n",
    "Each process has a main **thread** of execution which on modern operating systems are allowed to spawn multiple threads which execute different parts of the program at the \"same\" time.\n",
    "\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"./images/processes_threads.jpg\" alt=\"Threads\"/>\n",
    "     <div style=\"text-align:center;\">Source: <a href=\"https://subscription.packtpub.com/book/networking_and_servers/9781784396008/8/ch08lvl1sec69/multithreading-and-multiprocessing\">Learning Python Network Programming - Multithreading and multiprocessing</a></div>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Thread \n",
    "A Thread or a Thread of Execution (also called lightweight process) is defined in computer science as the smallest unit that can be scheduled in an operating system. Threads are usually contained in processes. source: https://www.python-course.eu/threads.php\n",
    "\n",
    "Threads work in a shared memory paradigm and can cause issues in code that is not thread-safe. Due to this CPython implementation has a **Global Interpreter lock (GIL)** (see below).\n",
    "\n",
    "Let's address this **GIL** elephant in the room."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Interpretor lock (GIL)\n",
    "\n",
    "In short, in Python only one thread controls the Python interpreter at a time.\n",
    "\n",
    "**NOTE:** This is true for the most common implementation of Python, CPython. There exist other implementations which do not have a GIL.\n",
    "\n",
    "As mentioned in https://wiki.python.org/moin/GlobalInterpreterLock :\n",
    "> In CPython, the global interpreter lock, or GIL, is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecodes at once. The GIL prevents race conditions and ensures thread safety. In short, this mutex is necessary mainly because CPython's memory management is not thread-safe.\n",
    "\n",
    "where Mutex (lock) is a programming flag used to grab and release an object.\n",
    "\n",
    "This lock needs to be acquired and released in order for a thread to execute.\n",
    "\n",
    "Therefore - as we learned in Section 1 - it prevents running code in a single Python interpreter on different cores in parallel! This is definitely a bad news for parallel processing.\n",
    "\n",
    "However, the good news is that there are use cases where we can still leverage the multithreading and also there are libraries that can circumvent the GIL issue, e.g. [Numba](https://numba.readthedocs.io/en/stable/user/jit.html#nogil), [Pythran](https://pythran.readthedocs.io/en/latest/MANUAL.html#gil-interaction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IO bound vs CPU bound\n",
    "\n",
    "In Python, we can achieve parallel speedup by either using multiple threads within the same process or spawning multiple processes. However, to decide the approach needed in a specific case, we need to find out if our problem is IO (input-output) or CPU bound.\n",
    "\n",
    "**IO bound problem**: A program that spends most of the time waiting for fetching or writing data rather than computing.\n",
    "\n",
    "**CPU bound problem**: A program that spends most of its time on computing.\n",
    "\n",
    "IO speed is much slower than the speed at which a CPU can execute instructions (see below).\n",
    "\n",
    "Example: one the newest Intel CPUs\n",
    "Intel Core i9-10910: Max Frequency 5GHz -> 5 X $10^9$ instructions per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap: Latency Comparison Numbers (~2012)**\n",
    "\n",
    "```\n",
    "                                             Real time              On human scale\n",
    "L1 cache reference ......................... 0.5 ns                 0.5 s\n",
    "Execute typical instruction ................   1 ns                 1.0 s\n",
    "L2 cache reference ........................... 7 ns                 7.0 s\n",
    "Main memory reference ...................... 100 ns                ~1.3 minutes\n",
    "Send 2K bytes over 1 Gbps network ....... 20,000 ns  =  20 ¬µs       5.5 hours\n",
    "SSD random read ........................ 150,000 ns  = 150 ¬µs       1.7 days\n",
    "Read 1 MB sequentially from memory ..... 250,000 ns  = 250 ¬µs       2.9 days\n",
    "Read 1 MB sequentially from SSD  ..... 1,000,000 ns  =   1 ms      11   days\n",
    "Send packet CA->Netherlands->CA .... 150,000,000 ns  = 150 ms       4.8 years\n",
    "```\n",
    "Source: https://gist.github.com/jboner/2841832"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrency\n",
    "\n",
    "As see in the section above, IO/network bound problems can wait for significant time (compared to CPUs execution time) for data.\n",
    "In a multi threaded application the GIL can be passed to another thread while one thread is still waiting for data. This can lead to a significant speed up in IO bound problems.\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"./images/multithreading.png\" alt=\"multithreading\"/>\n",
    "    <div style=\"text-align:center;\">Source: <a href=\"https://cloudsek.com/how-do-you-achieve-concurrency-with-python-threads\">https://cloudsek.com/how-do-you-achieve-concurrency-with-python-threads</a></div>\n",
    "</p>\n",
    "\n",
    "Therefore, we can profit from the concurrency of our problem. However, concurrent execution in this case gives an impression that every thread is executing at the same time by cleverly switching between the threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to multithreading and multiprocessing\n",
    "\n",
    "## `concurrent.futures` module\n",
    "\n",
    "Python has a `concurrent.futures` module as a part of the Python Standard Library that provides a convenient way of doing multithreading (and multiprocessing which comes later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the usage of this library let's start by creating a simple `sleeping()` function that sleeps for 1 second by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "\n",
    "def sleeping(secs=1):\n",
    "    \"\"\"This function just sleeps for secs seconds\n",
    "    This simulates an IO bound problem\"\"\"\n",
    "    time.sleep(secs)\n",
    "    print(\"I slept for {} seconds\".format(secs))\n",
    "    return secs\n",
    "\n",
    "\n",
    "sleeping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we call sleeping twice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "\n",
    "def sleeping(secs=1):\n",
    "    \"\"\"This function just sleeps for secs seconds\n",
    "    This simulates an IO bound problem\"\"\"\n",
    "    time.sleep(secs)\n",
    "    print(\"I slept for {} seconds\".format(secs))\n",
    "    return secs\n",
    "\n",
    "\n",
    "sleeping()\n",
    "sleeping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this program what happens is the following:\n",
    "1. The first sleep is executed\n",
    "2. It blocks any further execution until the specified time is passed\n",
    "3. Then the program proceeds with the second sleeping statement\n",
    "\n",
    "As you can see it took 2 seconds of \"wall time\" to run this program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make the above program faster by employing multithreading? \n",
    "\n",
    "Answer: Yes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ThreadPoolExecutor`\n",
    "\n",
    "Here is how we could do that using the `ThreadPoolExecutor` from `concurrent.futures`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def sleeping(secs=1):\n",
    "    \"\"\"This function just sleeps for secs seconds\n",
    "    This simulates an IO bound problem\"\"\"\n",
    "    time.sleep(secs)\n",
    "    print(\"I slept for {} seconds\".format(secs))\n",
    "    return secs\n",
    "\n",
    "\n",
    "num_threads = 2\n",
    "num_sleeps = 2\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    for _ in range(num_sleeps):\n",
    "        executor.submit(sleeping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that the above program executes the `sleeping()` function two times, each on an independent thread and it took 1 second to run the program.  \n",
    "\n",
    "The **context manager** (see line 17: `with`....) makes sure that the tasks are completed and the threads are cleaned up properly when we are done.\n",
    "\n",
    "`executor.submit()` is responsible for scheduling the tasks.\n",
    "\n",
    "### Exercise (10 Mins)\n",
    "Play with the `num_threads` and `num_sleep` and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future object\n",
    "We can easily pass positional and keyword arguments to the `executor` using the `submit()` method:\n",
    "- for a function `fn(*args, **kwargs)` we simply use `executor.submit(fn, *args, **kwargs)`.\n",
    "\n",
    "\n",
    "`submit` returns a **futures object** which does not keep track of the task . We have to track it explicitly using the available methods such as:\n",
    "- `running()` -  Returns True if the call is currently being executed and cannot be cancelled.\n",
    "\n",
    "- `done()` - Returns True if the call was successfully cancelled or finished running.\n",
    "\n",
    "- `result()` - Returns the value returned by the call. If the task is not completed this will wait.\n",
    "\n",
    "- `exception()` - Returns the exception raised by the call.\n",
    "\n",
    "For a complete list of the methods, please have a look at:\n",
    "https://docs.python.org/3/library/concurrent.futures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a list of future objects (or simply futures) it is practical to get them as they are completed. We can achieve this using `as_completed()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "def sleeping(secs):\n",
    "    \"\"\"This function just sleeps for secs seconds\n",
    "    This simulates an IO bound problem\"\"\"\n",
    "    time.sleep(secs)\n",
    "    print(\"I slept for {} seconds\".format(secs))\n",
    "    return secs\n",
    "\n",
    "\n",
    "num_threads = 4\n",
    "secs = range(8, 0, -1)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    results = [executor.submit(sleeping, i) for i in secs]\n",
    "    for tmp in as_completed(results):\n",
    "        print(tmp.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output was printed as it was completed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit statement returns a futures object\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By iterating over the original futures we can get them in the order they appear in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tmp in results:\n",
    "    print(tmp.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `executor.map()`\n",
    "`concurrent.futures` also provides a `executor.map()` method that mimics Python `map()` and hence it can be applied on a list (or other iterator):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "def sleeping(secs):\n",
    "    \"\"\"This function just sleeps for secs seconds\n",
    "    This simulates an IO bound problem\"\"\"\n",
    "    time.sleep(secs)\n",
    "    print(\"I slept for {} seconds\".format(secs))\n",
    "    return secs\n",
    "\n",
    "\n",
    "num_threads = 4\n",
    "secs = range(8, 0, -1)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    results = executor.map(sleeping, secs)\n",
    "\n",
    "for tmp in results:\n",
    "    print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** When using **executor.map()**, we do not get a future object but instead we directly get the data of the results returned by the function, in the order same as the inputs. Therefore, we do not have to use `as_completed()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error propagation\n",
    "\n",
    "It is very important to keep in mind that error within one of the executions does not crash the entire program!\n",
    "\n",
    "\n",
    "Error is raised when we try to access or do something with the result of the function.\n",
    "\n",
    "\n",
    "The **futures object** however contains the error information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def inverse(num):\n",
    "    return 1 / num\n",
    "\n",
    "\n",
    "num = range(8)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    results = [executor.submit(inverse, i) for i in num]\n",
    "\n",
    "    for tmp in results:\n",
    "        print(tmp.exception())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the actual error is thrown once we try to access the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    results = [executor.submit(inverse, i) for i in num]\n",
    "\n",
    "    for tmp in results:\n",
    "        print(tmp.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Downloading files [15 min]\n",
    "\n",
    "A typical use case of multithreading is to download a bunch of files from the internet.\n",
    "\n",
    "In this use case, threads generally have to wait for the network and IO and multithreading can offer substantial speedup.\n",
    "\n",
    "In the following exercise we will download a bunch of images from the internet.\n",
    "\n",
    "* Look at the code below and try to understand it from a high level perspective\n",
    "* Complete the code by creating a threadpoolexecutor which takes num of threads as the arguments and download files independently on each thread using the function `downloader`\n",
    "* Play with the number of threads and compare the execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import tempfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def downloader(args):\n",
    "    \"\"\"This function accepts a tuple (url, target_folder) and downloads the given file\n",
    "    target folder.\"\"\"\n",
    "\n",
    "    url, target_folder = args\n",
    "    # last string after last \"/\" is the filename, e.g.\n",
    "    # url is\n",
    "    # https://cdn.pixabay.com/photo/2015/07/13/21/54/gray-cat-843916__480.jpg\n",
    "    # then the file name is gray-cat-843916__480.jpg:\n",
    "    file_name = url.rsplit(\"/\", 1)[1]\n",
    "\n",
    "    target = os.path.join(target_folder, file_name)\n",
    "    try:\n",
    "        with requests.get(url, stream=True) as response:\n",
    "            response.raise_for_status()  # raise exception for invalid URL\n",
    "            with open(target, \"wb\") as fh:\n",
    "                fh.write(response.raw.read())\n",
    "        return file_name\n",
    "    except requests.HTTPError as exception:\n",
    "        print(exception)\n",
    "        return None\n",
    "\n",
    "\n",
    "# Reading in the urls of the images\n",
    "# (source, cats.txt: https://github.com/amitupreti/image_downloader_multiprocessing_python)\n",
    "\n",
    "with open(\"./data/cats.txt\", \"r\") as fh:\n",
    "    urls = [line.strip() for line in fh]\n",
    "\n",
    "# restrict to 20 urls for testing:\n",
    "urls = urls[:200]\n",
    "\n",
    "target_folder = tempfile.mkdtemp()\n",
    "print(\"download pictures to\", target_folder)\n",
    "\n",
    "args = [(url, target_folder) for url in urls]\n",
    "\n",
    "names = list(map(downloader, args))\n",
    "\n",
    "files_downloaded = list(filter(None, names))\n",
    "print(len(files_downloaded))\n",
    "\n",
    "# START TO COMPLETE HERE\n",
    "\n",
    "# 1. Create a list of tuples (url, target_folder)\n",
    "# 2. Use ThreadPoolExecutor to download the files\n",
    "# 3. Print the number of downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution",
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import tempfile\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def downloader(args):\n",
    "    \"\"\"This function accepts a tuple (url, target_folder) and downloads the given file\n",
    "    target folder.\"\"\"\n",
    "\n",
    "    url, target_folder = args\n",
    "    # last string after last \"/\" is the filename, e.g.\n",
    "    # url is\n",
    "    # https://cdn.pixabay.com/photo/2015/07/13/21/54/gray-cat-843916__480.jpg\n",
    "    # then the file name is gray-cat-843916__480.jpg:\n",
    "    file_name = url.rsplit(\"/\", 1)[1]\n",
    "\n",
    "    target = os.path.join(target_folder, file_name)\n",
    "    try:\n",
    "        with requests.get(url, stream=True) as response:\n",
    "            response.raise_for_status()  # raise exception for invalid URL\n",
    "            with open(target, \"wb\") as fh:\n",
    "                fh.write(response.raw.read())\n",
    "        return file_name\n",
    "    except requests.HTTPError as exception:\n",
    "        print(exception)\n",
    "        return None\n",
    "\n",
    "\n",
    "# Reading in the urls of the images\n",
    "# (source, cats.txt: https://github.com/amitupreti/image_downloader_multiprocessing_python)\n",
    "\n",
    "with open(\"./data/cats.txt\", \"r\") as fh:\n",
    "    urls = [line.strip() for line in fh]\n",
    "\n",
    "# restrict to n urls for testing:\n",
    "urls = urls[:20]\n",
    "\n",
    "target_folder = tempfile.mkdtemp()\n",
    "print(\"download pictures to\", target_folder)\n",
    "\n",
    "num_threads = 2\n",
    "\n",
    "args = [(url, target_folder) for url in urls]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    # list to trigger actual iteration over iterator object:\n",
    "    names = list(executor.map(downloader, args))\n",
    "\n",
    "files_downloaded = list(filter(None, names))\n",
    "\n",
    "print(\"files where downloaded to\", target_folder)\n",
    "print(\"number of downloaded files:\", len(files_downloaded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### When does multithreading not help?\n",
    "As already mentioned earlier multithreading will not be able to give us a speedup for CPU bound tasks. \n",
    "\n",
    "Let's demonstrate this with a very simple example that is compute intensive:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powern(list_numbers):\n",
    "    \"\"\"This function takes in a list of numbers,\n",
    "    computes the power of 9999 of each and sums the results\"\"\"\n",
    "    return sum([i ** 9999 for i in list_numbers])\n",
    "\n",
    "\n",
    "list_numbers = [564982 + 100 * i for i in range(400)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we call `powern` a few times in a serial fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = [powern(list_numbers) for _ in range(6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we try to improve speed using multiple threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "num_workers = 3\n",
    "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    results = [executor.submit(powern, list_numbers) for _ in range(6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the `GIL` avoided speed-up.\n",
    "\n",
    "In this case we have to run multiple Python interpreters which execute code in parallel. This is called multiprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing with `ProcessPoolExecutor`\n",
    "\n",
    "`concurrent.futures` provides a `ProcessPoolExecutor` that works same as the `ThreadPoolExecutor`.  \n",
    "As the name suggests it is distributing work to multiple processes and not to multiple threads. This circumvents the limitations due to the `GIL`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<div style=\"font-size: 150%;\"><i class=\"fa fa-info-circle\"></i>&nbsp;Important: Mac and Windows users</div>\n",
    "\n",
    "**Safe importing of main module:**\n",
    "    \n",
    "Make sure that the main module can be safely imported by a new Python interpreter without causing unintended side effects (such a starting a new process). \n",
    "\n",
    "For example, when running on Windows or macOS(), fence your code which triggers multi core execution as follows:\n",
    "    \n",
    "<div style=\"background: #ededed\">\n",
    "<code>if __name__ == \"__main__\":\n",
    "    from concurrent.futures import ProcessPoolExecutor\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "         ...\n",
    "</code>\n",
    "    </div>\n",
    "<br> \n",
    "    \n",
    "**Using multiprocessing in Jupyter notebooks:** \n",
    "    \n",
    "Using multiprocessing in Jupyter notebooks on Mac and Window has some limitations and causes errors and crashes not seen on Linux. \n",
    "    \n",
    "To prevent those we need to write the functions we want to run in parallel to a separate file, and then import the function, see also [here](https://stackoverflow.com/questions/47313732/jupyter-notebook-never-finishes-processing-using-multiprocessing-python-3/47374811#47374811).\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file powern.py\n",
    "\n",
    "## Write to file and import later due to multiprocessing problems in Jupyter notebooks\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def powern(list_numbers):\n",
    "    \"\"\"This function takes in a list of numbers, computes the power of 9999 of each and sums the results\"\"\"\n",
    "    return sum([i ** 9999 for i in list_numbers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from powern import powern\n",
    "\n",
    "list_numbers = [564982 + 100 * i for i in range(400)]\n",
    "\n",
    "num_workers = 3\n",
    "with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "    results = [executor.submit(powern, list_numbers) for _ in range(6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difference to the `ThreadPoolExecutor` library is that the arguments that we pass to the functions that we `executor.submit` need to be **serializable/picklable**.\n",
    "\n",
    "\n",
    "\n",
    "For more details please see [What can be pickled and unpickled?](https://docs.python.org/3/library/pickle.html#what-can-be-pickled-and-unpickled).\n",
    "\n",
    "Other programming guidelines for using `multiprocessing` library correctly can be found here:\n",
    "https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<div style=\"font-size: 150%;\"></div>\n",
    "<i class=\\\"fa fa-info-circle\\\"></i>\n",
    "    \n",
    "<strong>Take home messages:</strong>\n",
    "    \n",
    "- Use Multithreading when the tasks need to wait for IO and network.\n",
    "- Use Multiprocessing when the tasks are CPU bound.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Some libraries with C code, such as NumPy, Scipy and Numba can release GIL and can be used with multithreading. \n",
    "    \n",
    "Advantage of this is that threads are more \"light-weight\" than processes. \n",
    "    \n",
    "Also, the communication between the processes has a higher overhead (objects need to be serialized etc).\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style=\"font-weight: bold;\"><i class=\"fa fa-info-circle\"></i>Side note, good to know, ...</p>\n",
    "\n",
    "How does multiprocessing spawn the processes?\n",
    "\n",
    "This depends on the operating system (So it is <strong>NOT</strong> platform independent)\n",
    "\n",
    "source: https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods\n",
    "\n",
    "* **spawn**\n",
    "The parent process starts a fresh python interpreter process. The child process will only inherit those resources necessary to run the process object‚Äôs run() method. In particular, unnecessary file descriptors and handles from the parent process will not be inherited. Starting a process using this method is rather slow compared to using fork or forkserver.\n",
    "\n",
    "    **Available on Unix and Windows. The default on Windows and macOS (As of Python 3.8).**\n",
    "\n",
    "* **fork**\n",
    "The parent process uses os.fork() to fork the Python interpreter. The child process, when it begins, is effectively identical to the parent process. All resources of the parent are inherited by the child process. Note that safely forking a multithreaded process is problematic.\n",
    "\n",
    "    **Available on Unix only. The default on Unix.**\n",
    "\n",
    "* **forkserver**\n",
    "When the program starts and selects the forkserver start method, a server process is started. From then on, whenever a new process is needed, the parent process connects to the server and requests that it fork a new process. The fork server process is single threaded so it is safe for it to use os.fork(). No unnecessary resources are inherited.\n",
    "\n",
    "    **Available on Unix platforms which support passing file descriptors over Unix pipes.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversubscription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demonstration:** Let's play with the `num_workers` and see what happens when you increase this number such that you use more workers than cores available, e.g. `num_workers` = 1,3,6,12 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_workers = 4\n",
    "with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "    results = [executor.submit(powern, list_numbers) for _ in range(50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a maximum number of processes that we can use to get a speed up.  \n",
    "Trying to use more resources than available is called **oversubscription**.  \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<div style=\"font-size: 150%;\"><i class=\"fa fa-info-circle\"></i>&nbsp;Important</div>\n",
    "\n",
    "Oversubscription can slow down your program significantly! \n",
    "    \n",
    "So if a program scales badly then make sure that you are not using more cores than available.\n",
    "    \n",
    "Oversubscription can also be hidden!!! For example when you are using a multicore-enabled `numpy` or `scipy` in your code <a href=https://stackoverflow.com/questions/6941459/is-it-possible-to-know-which-scipy-numpy-functions-run-on-multiple-cores>(read also here)</a>. \n",
    "    \n",
    "Example: A `numpy` routine uses 4 cores and you try to speed it up by using `multiprocessing` with a pool of 4 workers. This results in an attempt to use 16 cores in total. In case your machine has less than 16 cores you will run into oversubscription.\n",
    "    \n",
    "If you would like to control this behavior of numpy, have a look at https://stackoverflow.com/questions/17053671/how-do-you-stop-numpy-from-multithreading       \n",
    "    \n",
    "</div>\n",
    "</div>\n",
    "\n",
    "For multiprocessing, the maximum number of processes, that can be used to get a speed up is equal to the number of virtual cores, can that be found using `os.cpu_count()`. \n",
    "\n",
    "**Warning**: There are caveats to this on HPC clusters that we will mention in **Section 6**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "print(f\"{num_workers=}\")\n",
    "# For Linux only\n",
    "# print(len(os.sched_getaffinity(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call `ProcessPoolExecutor` without specifying `max_workers` and in this case the number of CPUs is used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<div style=\"font-size: 150%;\"><i class=\"fa fa-info-circle\"></i>&nbsp;Note for Windows users:</div>\n",
    "\n",
    "- Better use one worker less than available cores to avoid freezing of your machine.\n",
    "- Multiprocessing scales badly on Windows since the start-up time for the workers is quite high.\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional approch: `threading` and `multiprocessing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Standard Library provides traditional libraries for doing multithreading and multiprocessing, `threading` and `multiprocessing` libraries, respectively. For writing concurrent code there is yet another library called `asyncio`. They are beyond the scope of the course. \n",
    "\n",
    "`concurrent.futures` relies on `threading` and `multiprocessing` and provides a unified, convenient, and simpler higher level API. However, once more control is needed `threading` and `multiprocessing` are the tools to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joblib\n",
    "\n",
    ">Joblib is a set of tools to provide lightweight pipelining in Python. In particular:\n",
    "> * transparent disk-caching of functions and lazy re-evaluation (memoize pattern)\n",
    "> * easy simple parallel computing\n",
    "> * Joblib is optimized to be fast and robust on large data in particular and has specific optimizations for numpy arrays.\n",
    ">\n",
    "> source: https://joblib.readthedocs.io/en/latest/\n",
    "\n",
    "In this section, we will look at the parallel computing functionality of `joblib`.  \n",
    "Joblib is perfect for parallelizing embarrassing parallel for loops.   \n",
    "It is essentially a wrapper library which has different backends for parallel computing.  \n",
    "\n",
    "By default Joblib uses the [loky](https://loky.readthedocs.io/en/stable/index.html) backend.\n",
    "Loky boasts several advantages over `concurrent.futures` and `multiprocessing`:\n",
    "* It claims to be more robust and interact better with third-party libraries (`multiprocessing` library is known to badly interact with some of these libraries).\n",
    "* Along with pickle it also has an integration with the `cloudpickle` library which allows for serialization of several objects which are unpicklable. However, `cloudpickle` is slower.\n",
    "* Deadlock free implementation: More robust behaviour w.r.t. deadlocks caused by crashing of workers. Although it is important to know that `concurrent.futures.ProcessPoolExecutor` as of Python 3.7+ is as robust as the executor from loky. However, loky would work well also for the older Python versions.\n",
    "\n",
    "Further reading: https://loky.readthedocs.io/en/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`joblib.Parallel`** - Helper class for readable parallel mapping\n",
    "\n",
    "**`joblib.delayed`** - Decorator used to capture the arguments of a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def powern(list_numbers):\n",
    "    \"\"\"This function takes in a list of numbers, computes the power of 9999 of each and sums the results\"\"\"\n",
    "    return sum([i ** 9999 for i in list_numbers])\n",
    "\n",
    "\n",
    "def main():\n",
    "    num_workers = 3\n",
    "    with Parallel(n_jobs=num_workers) as parallel:\n",
    "        results = parallel(delayed(powern)(list_numbers) for _ in range(6))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backend can be changed by using `parallel_backend`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from joblib import parallel_backend\n",
    "\n",
    "with parallel_backend(\"threading\", n_jobs=num_workers):\n",
    "    with Parallel(n_jobs=3) as parallel:\n",
    "        results = parallel(delayed(powern)(list_numbers) for _ in range(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration: Unpicklable objects with Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file lambda_func.py\n",
    "\n",
    "square = lambda x: x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from lambda_func import square\n",
    "\n",
    "\n",
    "def main():\n",
    "    # with Pool(2) as executor:\n",
    "    #    results = pool.map(square, [i for i in range(10)])\n",
    "    with Parallel(n_jobs=2) as parallel:\n",
    "        results = parallel(delayed(square)(i) for i in range(10))\n",
    "    #with ProcessPoolExecutor(2) as executor:\n",
    "    #    results = executor.map(square, [i for i in range(10)])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code example above, you can observe that passing a `lambda function` (unpicklable object) works with `joblib`.\n",
    "\n",
    "Uncomment the lines using `ProcessPoolExecutor` and `Pool` (not discussed in this lecture) and see what happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see Python allows us to parallelize the code on a PC with ease.\n",
    "\n",
    "- If you start from scratch you should probably try `concurrent.futures` and `joblib`.  \n",
    "- In case these libraries do not provide the functionality that you are looking for, you should use `threading` or `multiprocessing`, modules not covered in this course, as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Compute Pi  [20 min]\n",
    "\n",
    "Parallelize the $\\pi$ approximation exercise using  `ThreadPoolExecutor` and `ProcessPoolExecutor` from `concurrent.futures`, and `joblib`.\n",
    "\n",
    "1. What would you use?\n",
    "2. Why?\n",
    "3. Did you gain any speedup?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycat ../examples/pi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "%%file approx_pi.py\n",
    "\n",
    "from random import uniform\n",
    "\n",
    "\n",
    "def approx_pi_mod(n_attempts):\n",
    "    n_hits = 0\n",
    "\n",
    "    for _ in range(n_attempts):\n",
    "        x = uniform(-1.0, 1.0)\n",
    "        y = uniform(-1.0, 1.0)\n",
    "        if x ** 2 + y ** 2 <= 1.0:\n",
    "            n_hits += 1\n",
    "    return n_hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Computing Pi\n",
    "# Copy this code to a file and run on the command line\n",
    "\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from approx_pi import approx_pi_mod\n",
    "\n",
    "\n",
    "def main():\n",
    "    num_points = 2_000_000\n",
    "    num_workers = 4\n",
    "\n",
    "    num_points_worker = [int(num_points / num_workers) for _ in range(num_workers)]\n",
    "\n",
    "    num_points = sum(num_points_worker)\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        results = executor.map(approx_pi_mod, num_points_worker)\n",
    "\n",
    "    # Solution using Joblib\n",
    "    # with Parallel(n_jobs=num_workers) as parallel:\n",
    "    #    results = parallel(delayed(approx_pi_mod)(i) for i in num_points_worker)\n",
    "\n",
    "    print(\"The estimates value of Pi is: {}\".format(sum(results) * 4 / num_points))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced topics in multiprocessing and multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Profiling of multithreaded programs\n",
    "Profiling tools like **`cProfile`** lack support for multi-threaded programs.\n",
    "\n",
    "An out of the box solution is <a href=\"https://github.com/sumerc/yappi\"> **`yappi`** (Yet Another Python Profiler) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We skip a demonstration of yappi here as it's output is hard to read and understand, especially compared to the profiling tools we demonstrated in section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory profile of multiprocessing programs\n",
    "We can use `memory-profiler` at command line , i.e. `mprof`, together with `--multiprocess` and `--include-children` flags. The latter is especially needed if you use `subprocess` from the standard library and want to include the memory consumption of the spawned processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file memory_benchmark.py\n",
    "# Memory profiler for multiprocessing\n",
    "# Chapter 2 memory profiler: mprof\n",
    "\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def outer_product(n):\n",
    "    a = np.random.random(size=(n,))\n",
    "    b = compute_b(a)\n",
    "\n",
    "\n",
    "def compute_b(a):\n",
    "    AA = np.outer(a, a)\n",
    "    b = np.outer(a, a) @ a\n",
    "    time.sleep(1)\n",
    "    return b\n",
    "\n",
    "\n",
    "def main():\n",
    "    n = 10 * [100, 1_000, 10_000]\n",
    "\n",
    "    num_workers = 2\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        results = executor.map(outer_product, n)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Program took {} seconds\".format(end_time - start_time))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mprof run --multiprocess --include-children memory_benchmark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the plot\n",
    "!mprof plot -o mprof.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show plot in notebook\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(filename=\"./mprof.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Memory and Multithreading\n",
    "\n",
    "One has to be very careful of the so-called **race conditions** when using multithreading.\n",
    "\n",
    "An example of this is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "total = 0\n",
    "\n",
    "\n",
    "def increment(n):\n",
    "    \"\"\"This function updates the global variable total by 1\"\"\"\n",
    "    global total\n",
    "    for i in range(n):\n",
    "        total = total + 1\n",
    "\n",
    "\n",
    "n = 10_000\n",
    "m = 100\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(increment, [n] * m)\n",
    "\n",
    "print()\n",
    "print(f\"{total=}\\t expected={n*m}\\t ratio={round(total/(n*m),2)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the final value of `total` is not as expected.\n",
    "\n",
    "These generally occur when a variable is shared between the threads. The threads may read the same value of the variable and apply their respective operations. The threads will then write the updated value to the shared variable and the thread to do that last would overwrite the value written by the previous thread.\n",
    "\n",
    "\n",
    "\n",
    "This is what you might think what happens:\n",
    "\n",
    "| Thread 1| Thread 2|\n",
    "|---------|---------|\n",
    "| read total = 0 into register X      |         |\n",
    "| increment register X to 1       |         |\n",
    "| write register X, thus total = 1 | |\n",
    "|         | read total = 1 into register Y       |\n",
    "|         | increment register Y to 2        |\n",
    "| | write register Y, thus total = 2 |\n",
    "\n",
    "And this is what sometimes actually happens:\n",
    "\n",
    "\n",
    "| Thread 1| Thread 2|\n",
    "|---------|---------|\n",
    "| read total = 0 into register X      |         |\n",
    "| increment register X to 1       |         |\n",
    "|         | read total = 0 into register Y       |\n",
    "|         | increment register Y to 1        |\n",
    "| write register X, thus total = 1 | |\n",
    "| | write register Y, thus total = 1 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercise: \n",
    "\n",
    "Play with the `n` and `m` and see what happens! Try `n=10_000` and `m=100`; `n=1_000_000` and `m=1`;`n=1` and `m=10_000`?\n",
    "\n",
    "In case the `total` is different than the `expected` value you encounter the **race condition**: the global variable `total` is shared between threads they is no guarantee that the value is properly synchronized between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared memory and Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the previous example using multiprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<div style=\"font-size: 150%;\"><i class=\"fa fa-info-circle\"></i>&nbsp;Important:</div>\n",
    "\n",
    "In the examples in this section we use:\n",
    "\n",
    "<div style=\"background: #ededed\">\n",
    "<code> mp_context=multiprocessing.get_context(\"fork\")\n",
    "</code>\n",
    "    </div>\n",
    "<br>\n",
    "    \n",
    "in the `ProcessPoolExecutor` calls.\n",
    "    \n",
    "This is because the features provided by the multiprocessing library to share data between processes do not work on Windows and MacOS, as advertised, with their default `spawn` context.\n",
    "    \n",
    "</div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file increment.py\n",
    "\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "total = 0\n",
    "\n",
    "\n",
    "def increment(n):\n",
    "    \"\"\"This function updates the global variable total by 1\"\"\"\n",
    "    global total\n",
    "    for i in range(n):\n",
    "        total = total + 1\n",
    "\n",
    "\n",
    "def main():\n",
    "    n = 10_000\n",
    "    m = 100\n",
    "\n",
    "    with ProcessPoolExecutor(\n",
    "        max_workers=2, mp_context=mp.get_context(\"fork\")\n",
    "    ) as executor:\n",
    "        results = executor.map(increment, [n] * m)\n",
    "\n",
    "    print(f\"{total=}\\t expected={n*m}\\t ratio={round(total/(n*m),2)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python increment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`total` is not incremented. Why?\n",
    "\n",
    "As we just learned, when we use multiprocessing the variables are serialized and sent to child processes.\n",
    "Avoiding serialization can be useful in some cases, improving the time to solution.  \n",
    "We tried to achieve this using a global module variable, but things did not work as we expected.\n",
    "\n",
    "Let's investigate this further by first just reading the value of a global variable, and checking its location in memory using the `id` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file print_total.py\n",
    "\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "total = 6\n",
    "\n",
    "\n",
    "def print_total_and_return_id(location=\"main\"):\n",
    "    global total\n",
    "    print(total, location)\n",
    "    return id(total)\n",
    "\n",
    "\n",
    "def main():\n",
    "    m = 6\n",
    "    print_total_and_return_id()\n",
    "    with ProcessPoolExecutor(\n",
    "        max_workers=3, mp_context=mp.get_context(\"fork\")\n",
    "    ) as executor:\n",
    "        results = executor.map(print_total_and_return_id, [\"worker\"] * m)\n",
    "    print_total_and_return_id()\n",
    "\n",
    "    print(\"Number of different locations in memory for total =\", len(set(results)))\n",
    "    print(f\"{total=}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python print_total.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style=\"font-weight: bold;\"><i class=\"fa fa-info-circle\"></i>id() function:</p>\n",
    "\n",
    "Return the ‚Äúidentity‚Äù of an object. This is an integer which is guaranteed to be unique and constant for this object during its lifetime. Two objects with non-overlapping lifetimes may have the same id() value. CPython implementation detail: This is the address of the object in memory.\n",
    "    \n",
    "   source: https://docs.python.org/3/library/functions.html#id\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiprocessing, global module constants are seen by the child processes, and they really point to the same location in memory, so they are shared memory global constants.\n",
    "\n",
    "Next we will try to update the value of `total` and check its memory location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file total_id.py\n",
    "\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "total = 0\n",
    "\n",
    "\n",
    "def total_update_id(a):\n",
    "    global total\n",
    "    total = total + 1\n",
    "    return id(total)\n",
    "\n",
    "\n",
    "def main():\n",
    "    m = 6\n",
    "\n",
    "    with ProcessPoolExecutor(\n",
    "        max_workers=3, mp_context=mp.get_context(\"fork\")\n",
    "    ) as executor:\n",
    "        results = executor.map(total_update_id, [None] * m)\n",
    "\n",
    "    print(\"Number of different locations in memory for total =\", len(set(results)))\n",
    "    print(f\"{total=}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python total_id.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the global variables are not bounded to their location in memory once we try to update their value.\n",
    "\n",
    "Using global variables and updating their value is a source of surprises.\n",
    "\n",
    "In order to have shared variables we can use `multiprocessing.sharedctypes` or `multiprocessing.Manager` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiprocessing.sharedctypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to have a shared variable in a multiprocessing setting you can use\n",
    "\n",
    "- `multiprocessing.sharedctypes.Value()` or \n",
    "- `multiprocessing.sharedctypes.Array()`.\n",
    "\n",
    "The first corresponds to a shared `ctypes` object (that we have learned about in Section 4), while the second to an array.\n",
    "\n",
    "Let's use `multiprocessing.sharedctypes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%file total_id.py\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from ctypes import c_int\n",
    "from multiprocessing.sharedctypes import Value\n",
    "\n",
    "total = Value(c_int, 0)\n",
    "\n",
    "\n",
    "def total_update_id(a):\n",
    "    global total\n",
    "    total.value = total.value + 1\n",
    "    return id(total)\n",
    "\n",
    "\n",
    "def main():\n",
    "    m = 6\n",
    "\n",
    "    with ProcessPoolExecutor(\n",
    "        max_workers=3, mp_context=mp.get_context(\"fork\")\n",
    "    ) as executor:\n",
    "        results = executor.map(total_update_id, [None] * m)\n",
    "\n",
    "    print(\"Number of different locations in memory for total =\", len(set(results)))\n",
    "    print(f\"{total.value=}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python total_id.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So indeed, the same memory location is used for all processes.  \n",
    "We can access the value with the attribute `.value`.\n",
    "\n",
    "It is the time to check our initial example demonstrating race conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file increment.py\n",
    "\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from ctypes import c_int\n",
    "from multiprocessing.sharedctypes import Value\n",
    "\n",
    "total = Value(c_int, 0)\n",
    "\n",
    "\n",
    "def increment():\n",
    "    global total\n",
    "    total.value = total.value + 1\n",
    "\n",
    "\n",
    "def multi_increment(n):\n",
    "    for i in range(n):\n",
    "        increment()\n",
    "\n",
    "\n",
    "def main():\n",
    "    n = 10_000\n",
    "    m = 100\n",
    "\n",
    "    start = time.time()\n",
    "    with ProcessPoolExecutor(\n",
    "        max_workers=2, mp_context=mp.get_context(\"fork\")\n",
    "    ) as executor:\n",
    "        results = executor.map(multi_increment, [n] * m)\n",
    "\n",
    "    print(f\"Time take = {time.time()-start}\")\n",
    "    print(f\"{total.value=}\\t expected={n*m}\\t ratio={round(total.value/(n*m),2)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python increment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if `total` is shared, the total does not correspond to the expected value.\n",
    "This is because of the race condition that we had seen while applying multithreading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For operations that are not atomic (i.e., read/write) we have to lock the variable using `.get_lock()` method to avoid a race condition, but with a penalty on the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file increment.py\n",
    "\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from ctypes import c_int\n",
    "from multiprocessing.sharedctypes import Value\n",
    "\n",
    "total = Value(c_int, 0)\n",
    "\n",
    "\n",
    "def increment():\n",
    "    global total\n",
    "    with total.get_lock():\n",
    "        total.value = total.value + 1\n",
    "\n",
    "\n",
    "def multi_increment(n):\n",
    "    for i in range(n):\n",
    "        increment()\n",
    "\n",
    "\n",
    "def main():\n",
    "    n = 10_000\n",
    "    m = 100\n",
    "\n",
    "    start = time.time()\n",
    "    with ProcessPoolExecutor(\n",
    "        max_workers=2, mp_context=mp.get_context(\"fork\")\n",
    "    ) as executor:\n",
    "        results = executor.map(multi_increment, [n] * m)\n",
    "\n",
    "    print(f\"Time take = {time.time()-start}\")\n",
    "    print(f\"{total.value=}\\t expected={n*m}\\t ratio={round(total.value/(n*m),2)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python increment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `Value` are locked, but only for atomic operations. If you want to completely remove the locking, you can initialize the value with `total = Value(c_int, 0, lock=False)` (and of course also remove the `total.get_lock()` block). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiprocessing.Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you need to share `list` or `dict`, you can use `multiprocessing.Manager`. \n",
    "> A manager object returned by Manager() controls a server process which holds Python objects and allows other processes to manipulate them using proxies.\n",
    "\n",
    "This approach allows for higher flexibility, but it is slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file managers.py\n",
    "\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from multiprocessing import Manager\n",
    "\n",
    "\n",
    "def print_shared(task_id, shared_dict):\n",
    "    print(shared_dict[task_id])\n",
    "\n",
    "\n",
    "def main():\n",
    "    task_id = range(10)\n",
    "\n",
    "    manager = Manager()\n",
    "    shared_dict = manager.dict({el: el * 10 for el in task_id})\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=3) as executor:\n",
    "        results = executor.map(print_shared, task_id, [shared_dict] * len(task_id))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python managers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The manager itself has a lock `Lock()` that can be used to deal with operations that are not atomic.  \n",
    "In practice we recommend you to avoid locking operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style=\"font-weight: bold;\"><i class=\"fa fa-info-circle\"></i>Working with large data</p>\n",
    "    \n",
    "**Using Multiprocessing's shared memory functionality**\n",
    "\n",
    "When working with large data, serialization while using multiprocessing can cause high computational and memory costs.\n",
    "\n",
    "Multiprocessing introduced a module `multiprocessing.shared_memory` as of Python 3.8 which allows for shared memory across processors.\n",
    "\n",
    "A template procedure to use this is as follows:\n",
    "* Create a shared memory block of specified size\n",
    "* Create a numpy array passing in the created buffer\n",
    "* Copy data to this array\n",
    "* Pass the name of the shared memory to the function where the data processing takes place\n",
    "* Within the function recreate the numpy array based on the data in this shared memory without duplicating the data\n",
    "* Process the data\n",
    "\n",
    "see: https://mingze-gao.com/posts/python-shared-memory-in-multiprocessing/ for a working example\n",
    "    \n",
    "**Joblib's support of NumPy's memmap**\n",
    "    \n",
    "For working with large data, NumPy's `memmap` can be passed to joblib.Parallel.\n",
    "\n",
    "Have a look at this example from the official documentation.\n",
    "\n",
    "https://joblib.readthedocs.io/en/latest/auto_examples/parallel_memmap.html\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
