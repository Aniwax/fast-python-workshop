{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS CELL WHICH CUSTOMIZES LAYOUT AND STYLING OF THE NOTEBOOK !\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings = lambda *a, **kw: None\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(open(\"../documents/custom.html\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<span style=\"background:#f0f0e0;padding:1em\">Copyright (c) 2020-2021 ETH Zurich, Scientific IT Services. This work is licensed under <a href=\"https://creativecommons.org/licenses/by-nc/4.0/\">CC BY-NC 4.0</a></span><br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 2.5em; font-weight: bold;\">Section 3: Concepts</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous Section you've learned how to identify what is making your Python program run slow.\n",
    "\n",
    "Now it's turn to learn about common approaches to make your Python program run faster.\n",
    "\n",
    "We will start with an overview of various code optimization strategies, in order of application preference. Afterwards, we will dive into some of few basic code optimization concepts, leaving the more conceptually and technically advanced ones for the next Sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview: Strategies for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Avoid unnecessary computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not running computations saves runtime.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/captain_obvious.gif\" width=\"350px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://indepest.com/2021/03/14/captain-obvious/\">https://indepest.com/2021/03/14/captain-obvious/</a></sub></center></td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<p><i class=\"fa fa-info-circle\"></i>&nbsp You can avoid running unnecessary computations, for instance, by:</p>\n",
    "<ol>\n",
    "    <li><strong>input data pre-processing</strong> e.g. to read or to transform data, or to pre-compute useful values, using an extra data structures if needed,\n",
    "        <ul>\n",
    "        <li>falls into an art of using better algorithms and data structures</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>pre-allocating memory</strong>, to avoid copying in-memory data e.g. in a dynamic array growing in each loop iteration,</li>\n",
    "    <li><strong>saving (caching/&quot;memoizing&quot;) results of repeating computations</strong>, to avoid running multiple long-running function calls with the same input arguments.</li>\n",
    "</ol>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching and memoization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cache** is a hardware or software component that stores data so that future requests for that data can be served faster. The data stored in a cache might be the result of an earlier computation or a copy of data stored elsewhere. \n",
    "\n",
    "**Memoization** is an optimization technique that stores the results of resource-consuming function calls within a lookup table, allowing subsequent calls to reuse the stored results and avoid repeated computation. Memoization is a special case of caching.\n",
    "\n",
    "The basic memoization application to a function looks as follows:\n",
    "\n",
    "0. Set up a cache data structure for function results\n",
    "1. Every time the function is called with input arguments do one of the following:\n",
    "   * A) return the cached result, if any is available for given input arguments; OR\n",
    "   * B) compute the missing result, and update the cache before returning the result.\n",
    "\n",
    "    <table>\n",
    "        <tr><td><img src=\"imgs/memoization.png\" width=\"800px\"></td></tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you use as a cache?\n",
    "\n",
    "* Memory:\n",
    "    * variables\n",
    "    * arrays\n",
    "    * dictionaries\n",
    "* Data stores:\n",
    "   * disk files\n",
    "   * database\n",
    "   * cloud storage\n",
    "   \n",
    "You will find already implemented caches; in Python, to memoize function results use:\n",
    "\n",
    "* `functools.lru_cache` for memory-based caching, or\n",
    "* `joblib.Memory` for file-based caching.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use better algorithms and data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithmic improvements can have the biggest benefit in improving the runtime due to reducing time (or space) complexity;\n",
    "\n",
    "- Buying a faster expensive CPU/GPU may speedup your computations ca. $5$ to $10$ times;\n",
    "- Running in parallel on a high performance computer may speedup your computations ca. $50$ to $100$ times, using a very expensive hardware;\n",
    "\n",
    "**BUT**\n",
    "\n",
    "- Using a better algorithm and reducing number of operations, for instance, from $2 n^3$ to $10 n^2$ for input data of size $n = 10.000$, speeds up your code $\\frac{2\\cdot 10^{12}}{10^{9}} = 2000$ times, without any additional hardware costs.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/algorithms_and_data_structures.png\" width=\"800px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://dev.to/snj/how-to-learn-data-structures-and-algorithms-an-ultimate-guide-for-beginners-2h9c\">https://dev.to/snj/how-to-learn-data-structures-and-algorithms-an-ultimate-guide-for-beginners-2h9c</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<p><i class=\"fa fa-info-circle\"></i>&nbsp For the time-consuming parts of your scientific code you can:</p>\n",
    "\n",
    "<ol>\n",
    "    <li><strong>Pick an existing better algorithm</strong> - requires some knowledge in computer science (in particular, lingo for abstract problem formulation).</li>\n",
    "    <li><strong>Develop a new better algorithm</strong> - more challenging as it requires experience/practice specifically in algorithms development (many online training sites available).</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Complexity of an algorithm is closely related to the data structures used by it</strong>. Data structures allow to simplify or abstract-out parts of the problem being solved, like memoization of intermediate computations or search for elements within the data.</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making use of available fast memory speeds up the computations (recall [the latency table for different types of memory operations](https://gist.github.com/jboner/2841832)).\n",
    "\n",
    "**BUT**\n",
    "\n",
    "As already briefly discussed in the first Section, a **too high memory usage can negatively impact runtime** either by:\n",
    "\n",
    "* inefficient cache usage within the CPU, or by\n",
    "* memory swapping (paging) using a disk.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/memory_usage_vs_speed.jpg\" width=\"500px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://www.techpowerup.com/234514/firefox-54-released-multi-process-optimized-memory-footprint\">https://www.techpowerup.com/234514/firefox-54-released-multi-process-optimized-memory-footprint</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "**In case of CPU-intensive computations, you should avoid memory swapping at all cost.**\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<p><i class=\"fa fa-info-circle\"></i>&nbsp Memory usage can be reduced using various techniques, among others:</p>\n",
    "\n",
    "<ol>\n",
    "    <li><strong>using memory-efficient data structures</strong>, such as sparse arrays for numerical computations,</li>\n",
    "    <li><strong>using appropriate memory-efficient basic data types</strong>, such as fixed length strings, big-enough integer numbers, or lower precision floating point numbers,</li>\n",
    "    <li><strong>explicitly using external memory</strong> for data that does not fit into the main memory.</li>\n",
    "</ol>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory swapping problem*\n",
    "\n",
    "We introduced the concept of swapping in the first Section and it is important to understand that swapping can slow down program execution significantly.\n",
    "\n",
    "Swap space is a space on a disk which is a substitute of a fast main memory (RAM). Whenever an operating system runs short of main memory it swaps parts of the main memory (pages) with disk contents, i.e.:\n",
    "\n",
    "1. swap out: move currently unused main memory parts (pages) to a swap space on a disk, and make the free main memory available to the currently running program;\n",
    "1. swap in: read back from the swap space on a disk memory parts (pages) into the main memory, making it available to the currently running program.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/swapping.jpg\" width=\"400px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/9_VirtualMemory.html\">https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/9_VirtualMemory.html</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Swapping helps operating system to manage programs that use more memory than there actually is available (avoiding crashing applications instead).\n",
    "\n",
    "**BUT**\n",
    "\n",
    "Reading from a disk memory is slow. If swapping happens too often, especially in case **when one program uses more then available main memory, the program is mostly waiting for the operating system to swap the data** (is [\"swapping to death\"](https://en.wikipedia.org/wiki/Memory_paging#Swap_death)).\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use faster languages, like C/C++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that statically typed programming languages--ones where one declares types of variables, function arguments, or return values--can be much faster than dynamically typed languages, such as Python, at a cost of pre-required compilation to a machine code.\n",
    "\n",
    "Using fast languages for a project usually comes at cost of bigger program size and longer development-time.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/the_computer_language_benchmarks_game-time_vs_size.png\" width=\"1000px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://twitter.com/ChapelLanguage/status/921074120191655937/\">https://twitter.com/ChapelLanguage/status/921074120191655937/</a></sub></center></td></tr>\n",
    "    <tr><td><center><sub>Source<sup>2</sup>: <a href=\"https://benchmarksgame-team.pages.debian.net/benchmarksgame/\">https://benchmarksgame-team.pages.debian.net/benchmarksgame/</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "**BUT**\n",
    "\n",
    "You can write your project in Python and still delegate a lot of (crucial) computations to actually run as C/C++, without a significant program size or development time overhead.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<p><i class=\"fa fa-info-circle\"></i>&nbsp For your own Python programs you can employ C/C++ by:</p>\n",
    "\n",
    "<ol>\n",
    "    <li><strong>preferring vectorized C/C++-level routines</strong> using <a href=\"https://numpy.org/\">NumPy</a> arrays\n",
    "        (or <a href=\"https://pandas.pydata.org/\">Pandas</a> data frames) over slow Python <code>for</code> loops over\n",
    "        elements/rows/columns;<ul>\n",
    "            <li>Note: use libraries that build on top of NumPy or Pandas, like <a href=\"https://scipy.org/\">SciPy </a>\n",
    "                or <a href=\"https://scikit-learn.org\">Scikit-learn</a>.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>compiling your Python code to C/C++</strong> using tools such as <a\n",
    "            href=\"https://numba.pydata.org/\">Numba</a>, <a href=\"https://www.pypy.org/\">PyPy</a>, or <a\n",
    "            href=\"https://pythran.readthedocs.io\">Pythran</a>;</li>\n",
    "    <li><strong>&quot;<a href=\"https://en.wikipedia.org/wiki/Language_binding\">binding</a>&quot; to Python your own\n",
    "            C/C++ code</strong> using <a href=\"https://cython.org/\">Cython</a>\n",
    "        <ul>\n",
    "            <li>Note: while C/C++ is a convenient default for binding fast crucial routines and provides various options\n",
    "                for Python, you can call routines virtually from any other well-established fast languages, like, for\n",
    "                instance Fortran via <a href=\"https://numpy.org/doc/stable/f2py/\">F2PY</a>, Java via <a\n",
    "                    href=\"https://jpype.readthedocs.io/en/latest/\">JPype</a>, Rust via <a\n",
    "                    href=\"https://pyo3.rs\">PyO3</a> etc.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "The Section on code optimization will introduce in detail these approaches and tools.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run code in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principle is simple: split work among multiple workers to reduce the total runtime.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/serial_vs_parallel.png\" width=\"500px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://www.teldat.com/blog/parallel-computing-bit-instruction-task-level-parallelism-multicore-computers/\">https://www.teldat.com/blog/parallel-computing-bit-instruction-task-level-parallelism-multicore-computers/</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<p><i class=\"fa fa-info-circle\"></i>&nbsp Parallelization in computing has many different levels; from the lowest level:</p>\n",
    "\n",
    "<ol>\n",
    "    <li>\n",
    "        <p><a href=\"https://en.wikipedia.org/wiki/Bit-level_parallelism\">bit-level parallelism</a> - processor operating\n",
    "            on whole &quot;chunks&quot; of bits (32-bit, 64-bit);</p>\n",
    "    </li>\n",
    "    <li>\n",
    "        <p><a href=\"https://en.wikipedia.org/wiki/Instruction-level_parallelism\">instruction-level parallelism</a> -\n",
    "            simultaneous execution of multiple processor instructions, which are optimized first by hardware or\n",
    "            compilers for a maximizing average number of instructions run per CPU clock cycle (a 3.0GHz CPU performs 3\n",
    "            million clock cycles per second);</p>\n",
    "    </li>\n",
    "    <li><a href=\"https://en.wikipedia.org/wiki/Data_parallelism\"><strong>data parallelism</strong></a> - distributing\n",
    "        the data across multiple threads/processors, which operate on the data in parallel, e.g.,<ul>\n",
    "            <li><a\n",
    "                    href=\"https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm#Parallel_and_distributed_algorithms\">parallel\n",
    "                    and distributed large matrix multiplication</a></li>\n",
    "            <li>the <a href=\"https://pandas.pydata.org/docs/user_guide/groupby.html\">Split-Apply-Combine</a> strategy, or the <a href=\"https://en.wikipedia.org/wiki/MapReduce\">MapReduce programming model</a>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <td><img src=\"imgs/split-apply-combine.png\" width=\"500px\"></td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>\n",
    "                            <center><sub>Source: <a\n",
    "                                        href=\"https://github.com/anurag-code/Split-Apply-Combine-Data-Mining-in-Python\">https://github.com/anurag-code/Split-Apply-Combine-Data-Mining-in-Python</a></sub>\n",
    "                            </center>\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                </table>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><a href=\"https://en.wikipedia.org/wiki/Task_parallelism\"><strong>task parallelism</strong></a> - distributing\n",
    "        tasks across multiple threads/processors, which perform the tasks in parallel, e.g.<ul>\n",
    "            <li>parallel force updates in <a href=\"https://en.wikipedia.org/wiki/Molecular_dynamics\">molecular\n",
    "                    dynamics</a>, or parallel finite element updates in a <a\n",
    "                    href=\"https://en.wikipedia.org/wiki/Finite_element_method\">finite element method</a> mesh</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>and <a href=\"https://en.wikipedia.org/wiki/Loop-level_parallelism\">loop-level parallelism</a> - a special case\n",
    "        combining task and data parallelism, where tasks and a corresponding data chunks are instructions and data used\n",
    "        in <code>for</code> loop iterations, executed then in parallel.</li>\n",
    "</ol>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the three Sections on parallel computing on: 1) CPUs, 2) clusters, and 3) GPUs, you will learn how to use tools for the data and tasks parallelism.\n",
    "\n",
    "### Lower-level of parallelism in Python*\n",
    "\n",
    "The low bit- and instruction-levels of of parallelism can be employed only indirectly in Python; for instance:\n",
    "\n",
    "* by use of 32-bit NumPy floating-point numbers on a 64-bit CPU,\n",
    "* by use of CPU's vector instructions, called [single instruction, multiple data (SIMD)](https://en.wikipedia.org/wiki/SIMD) instructions, which can be employed by:\n",
    "    * using [Numba which leverages LLVM's ability to optimize some `for` loops to use SIMD instructions](https://numba.pydata.org/numba-doc/latest/user/faq.html#does-numba-vectorize-array-computations-simd);\n",
    "    * [Pythran compilation with a C++ xsimd library support](https://serge-sans-paille.github.io/pythran-stories/bye-bye-boostsimd-welcome-xsimd.html);\n",
    "    * binding to a plain C/C++ code which directly uses low-level SIMD intrinsics, like [Intel's Streaming SIMD Extensions (SSE)](https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions);\n",
    "\n",
    "  see also: [*Pushing Python toward C speeds with SIMD*](https://laurenar.net/posts/python-simd/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parallelization limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throwing more workers to work won't simply shorten the work time proportionally as there are always parts which cannot be done in parallel.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/ParallelLimits.png\" width=\"400px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://demotywatory.pl/913269/Polscy-robotnicy\">https://demotywatory.pl/913269/Polscy-robotnicy</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "**Amdahl's law** is a theoretical limit on achievable speedup of a task with a fixed problem size, when a known part of the task runtime must be spent in serial execution, and rest can be parallelized.\n",
    "\n",
    "For example:\n",
    "* program needs 20h to complete on 1 CPU,\n",
    "* 2h (10%) portion of the program cannot be parallelized, whereas the remaining 18h (90%) can,\n",
    "* ⇒ the minimum total execution time cannot be less than 2h\n",
    "* ⇒ speedup can't be more than 10x\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/AmdahlsLaw.png\" width=\"600px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-09766-4_77\">https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-09766-4_77</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "We will get back to this and other measures of parallel scaling in more detail in the Section on parallel computing.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use fast hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr><td><img src=\"imgs/Supercomputer.png\" width=\"350px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://dribbble.com/shots/1984685-Supercomputer-visual-pun\">https://dribbble.com/shots/1984685-Supercomputer-visual-pun</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<p><i class=\"fa fa-info-circle\"></i>&nbsp You can make your program run faster by using high-end hardware, like:</p>\n",
    "\n",
    "<ol>\n",
    "    <li><strong>Multi-core servers</strong>,</li>\n",
    "    <li><strong>High Performance Computing (HPC) clusters</strong> a.k.a. supercomputers,</li>\n",
    "    <li><strong>Grapic cards</strong> a.k.a. GPUs.</li>\n",
    "</ol>\n",
    "\n",
    "<p>Using any of the above means running your code in parallel, but <strong>some hardware is suitable only for some problems</strong>\n",
    "    (CPU-bound vs. I/O-bound problems, or problems fitting into &quot;single instruction, multiple data&quot; processing type).</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "Most often you need to adjust your program to target specific hardware.\n",
    "\n",
    "In the three Sections on parallel computing you will see how to adjust your Python programs for specific hardware, usually, without much overhead.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm complexity analysis: Big O notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen in the profiling example in the previous Section how one can empirically try to identify the relation between program's runtime and the input data size, e.g. to be roughly $n^2$, where $n$ is the input size. The program represents an algorithm that processes given input to solve a problem or perform a computation.\n",
    "\n",
    "**Algorithms can be theoretically classified according to how their runtime or space (memory) requirements grow as the input/problem size grows**.\n",
    "\n",
    "This is important because usually the scalability or practical applicability of an algorithm going from development tests input data to real-world large input data sets is limited by the order of growth of runtime or space requirements with respect to problem size (rather than by e.g. specific constants).\n",
    "\n",
    "<blockquote>\n",
    "    <strong>Example: algorithm complexity</strong>\n",
    "\n",
    "| Problem size |  Algorithm 1: $n^2 / 2$ seconds runtime | Algorithm 2: $100 n$ seconds runtime |\n",
    "| ------------ | --------------------------- | --------------------------- |\n",
    "| $n = 10$     | 50 sec                      | 16 min 40 sec               |  \n",
    "| $n = 100$    | ~1 hour 24 min              |  ~2 hours 47 min            |\n",
    "| $n = 1000$   | ~5 days 19 hours            | ~1 day 4 hours              |\n",
    "| $n = 10000$  | ~**1 year 30 weeks**        | ~1 week 5 days              |\n",
    "\n",
    "If we anticipate practical problems of size $n > 1000$, we should invest into developing Algorithm 2, which is considered to be theoretically faster as it has linear $n$ and not quadratic $n^2$ runtime. If all practical problems are limited by size $n \\leq 100$, it's completely fine to stick to Algorithm 1.\n",
    "\n",
    "[Galactic Algorithms](https://en.wikipedia.org/wiki/Galactic_algorithm) are algorithms that outperform other algorithms, but     will never be used on any of the merely terrestrial data sets on Earth. E.g. optimal integer multiplication, where the numbers need to be vastly bigger than the number of atoms in the observable universe.\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In algorithms complexity analysis **a unit is an abstract number**, respectively, **a performed basic operation (runtime unit) and a stored basic object (space unit)**.\n",
    "\n",
    "<blockquote>\n",
    "    <strong>Example: distance matrix</strong>\n",
    "\n",
    "Compute a matrix of squared distances between $n$ input points $x_1,x_2,\\ldots,x_n$:\n",
    "\n",
    "\n",
    "<math>\\begin{align}\\begin{bmatrix}\n",
    "0 & d_{12}^2 & \\dots & d_{1n}^2 \\\\\n",
    "d_{21}^2 & 0 & \\dots & d_{2n}^2 \\\\\n",
    "\\vdots&\\vdots & \\ddots&\\vdots&  \\\\\n",
    "d_{n1}^2 & d_{n2}^2 & \\dots & 0 \\\\\n",
    "\\end{bmatrix},\\end{align} </math>\n",
    "\n",
    "where $d_{ij} = \\text{dist}(x_i, x_j) = \\text{dist}(x_j, x_i) = d_{ji}$, e.g. a [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) between two points.\n",
    "\n",
    "Assuming:\n",
    "    \n",
    "1. as a runtime operation unit a single distance computation, and\n",
    "2. as a storage object unit a number representing a single distance,\n",
    "\n",
    "one needs to compute $\\frac{n\\cdot (n - 1)}{2} \\approx n^2\\,/\\,2$ distances, which are to be stored in a $n\\times n = n^2$ memory array.\n",
    "    \n",
    "We say that both **runtime complexity** and required **space complexity** of such distance matrix computation is quadratic, or, more formally, \"oh of $n$-squared\", which is denoted as:\n",
    "$$O(n^2)$$\n",
    "\n",
    ".\n",
    "</blockquote>\n",
    "\n",
    "$O(\\cdots)$ is a so called **Big O notation**, or which is a mathematical notation that describes the limiting behavior of a function, up to a constant, when the argument $n$ tends towards infinity (see e.g. [Big O notation @ Wikipedia](https://en.wikipedia.org/wiki/Big_O_notation) for a more formal mathematical description). The idea is to give an growth rate of required runtime or space, with respect to growing input/problem size. Thus, constant multipliers or lower order functions are ignored in the Big O notation, and we have, for example:\n",
    "\n",
    "<math>\\begin{align}\n",
    "\\frac{1}{2}(n^2 - 2n - 1) &= O(n^2)\\\\\n",
    "&\\text{ or}\\\\\n",
    "2n + \\log{n} &= O(n).\n",
    "\\end{align} </math>\n",
    "\n",
    "The incentive is to characterize functions according to their growth rates as this is the main factor for estimating a speed or memory use of an algorithm.\n",
    "\n",
    "The letter O is used because the growth rate of a function is also referred to as the **order of the function**.\n",
    "\n",
    "Beware: a description of a function in terms of Big O notation formally provides only an upper bound on the growth rate of the function (hence the capital letter), e.g. we also have $n = O(n^2)$, but no one really writes that their algorithms are slower or more memory-hungry than they actually are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orders of common functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<table>\n",
    "    <tr><td><img src=\"imgs/bigo.png\" width=\"600px\"></td></tr>\n",
    "        <tr><td><center><sub>Source: <a href=\"https://runestone.academy/runestone/books/published/pythonds/index.html\">https://runestone.academy/runestone/books/published/pythonds/index.html</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "In a table form, from lowest to highest \"order of growth\", with additional color coding for practical applicability, and with examples of problems with a corresponding complexity:\n",
    "\n",
    "<table >\n",
    "    <thead>\n",
    "        <tr style=\"border-bottom: 1px solid black;\" >\n",
    "            <th style=\"width:100px\">Notation</th>\n",
    "            <th style=\"width:100px\">Name</th>\n",
    "            <th style=\"width:600px\">Example</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr style=\"background: lightgreen; \">\n",
    "            <td>$O(1)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Constant_time\">Constant</a></td>\n",
    "            <td>Primitive operations<br/>Determining if a binary number is even or odd</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: lightgreen; border-bottom: 1px solid black; \">\n",
    "            <td>$O(\\log{}n)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Logarithmic_time\">Logarithmic</a></td>\n",
    "            <td>Binary search</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: yellow; \">\n",
    "            <td>$O(n)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Linear_time\">Linear</a></td>\n",
    "            <td>Single loop<br/> Find element in an unsorted list<br/> Scalar/dot product (multiplication of a vector and vector)<br/> Special sorting (Counting sort)</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: yellow; border-bottom: 1px solid black;\">\n",
    "            <td>$O(n\\log{}n)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Linearithmic_time\">Log Linear</a></td>\n",
    "            <td>Sorting (Quicksort, Heapsort and Merge sort)</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: orange; \">\n",
    "            <td>$O(n^2)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Table_of_common_time_complexities\">Quadratic</a>\n",
    "            </td>\n",
    "            <td><strong>Typical case where algorithms work well on test data or smaller data sets and start to fail for larger examples!</strong> <br/>One nested loop<br/> Find duplicate elements in a list (naive)<br/> Multiplication of a matrix and a vector<br/> Simple sorting algorithms (Bubble sort, Selection sort, Insertion sort)</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: orange;  \">\n",
    "            <td>$O(n^3)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Table_of_common_time_complexities\">Cubic</a></td>\n",
    "            <td>Two nested loops<br/> Matrix multiplication (naive)<br/> Solving a system of $n$ linear equations (via Gaussian elimination)</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: orange; border-bottom: 1px solid black;\">\n",
    "            <td>$O(n^c)$, $c>1$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time\">Polynomial</a></td>\n",
    "            <td>Finding maximum flow in a network (minimum cut in a graph)<br/> Workers to tasks Assignment Problem (maximum\n",
    "                weighted bipartite matching)</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: orangered;\">\n",
    "            <td>$O(2^n)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Exponential_time\">Exponential</a></td>\n",
    "            <td>Computing all subsets of a list<br/> Finding the (exact) solution to the Traveling Salesman Problem using dynamic programming</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: orangered;\">\n",
    "            <td>$O(n!)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Factorial\">Factorial</a></td>\n",
    "            <td>Computing all permutations of a list<br/> Solving the Traveling Salesman Problem via brute-force search</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz exercise [15 min]\n",
    "\n",
    "1. What is a runtime and space complexity of this function:\n",
    "    ```python\n",
    "    def compute(n: int):\n",
    "        a = -1\n",
    "        i = n\n",
    "        while i > 0:\n",
    "            a += 1\n",
    "            i //= 2\n",
    "        return a\n",
    "    ```\n",
    "\n",
    "\n",
    "1. What is a runtime complexity of an algorithm that performs $10^6 n$ initial operations, $2 n^3$ nested-loops operations and $10^9 \\log{n}$ post-processing operations?\n",
    "\n",
    "1. Does an $O(\\log{n})$ algorithm run faster than $O(n)$ algorithm?\n",
    "\n",
    "1. A program computes results\n",
    "   - for each $n$ in $2 n$ steps,\n",
    "   - but once every $n^2$ calls the program needs to run data maintenance that takes additionally $n^2 + 3$ steps. \n",
    "   \n",
    "What is the runtime complexity of the program?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "**Solutions**\n",
    "\n",
    "1. Runtime complexity is $O(\\log{n})$, as `i` decreases from `n`, `n/2`, `n/4`, ... to `1` ($\\log_2{n}$ is \"how many time to multiply by 2 to get over $n$ value\").\n",
    "\n",
    "1. $O\\left(10^6 n + 2 n^3 + 10^9 \\log{n}\\right) = O\\left(n + n^3 + \\log{n}\\right) = O\\left(n^3\\right)$\n",
    "\n",
    "1. No, not in general, only true for large enough input size $n$, but up to some $n$ values, depending on the actual constants and lower order terms of the runtime complexity, the $O(n)$ algorithm may run faster than the $O(\\log{n})$ algorithm.\n",
    "\n",
    "1. The Big-Oh runtime complexity is the worst-case scenario, so it's $O(2 n + n^2) = O(n^2)$, but in such cases it makes sense to talk about average runtime complexity, which would be in this case \n",
    "$\\frac{n^2  \\cdot 2 n + n^2 + 3}{n^2} = \\frac{2 n^3 + n^2 + 3}{n^2} = O(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding unnecessary computations: memoization example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<blockquote>\n",
    "    <strong>Example: Fibonacci numbers</strong>\n",
    "\n",
    "The [Fibonacci numbers](https://en.wikipedia.org/wiki/Fibonacci_number), commonly denoted $F_n$, form a sequence, called the Fibonacci sequence, such that each number is the sum of the two preceding ones, starting from $0$ and $1$. That is,\n",
    "\n",
    "<math>\\begin{align}\n",
    "F_0 &= 0,\\\\\n",
    "F_1 &= 1, \\\\\n",
    "F_{n}&=F_{n-1}+F_{n-2},\\text{ for } n > 1\n",
    "\\end{align}\n",
    "</math>\n",
    "\n",
    "The task is to simply compute $F_{n}$, given $n$ as an input.\n",
    "    \n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First implementation - directly from definition, a recursion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci_recursion(n):\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    if n == 1:\n",
    "        return 1\n",
    "\n",
    "    return fibonacci_recursion(n - 1) + fibonacci_recursion(n - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time fibonacci_recursion(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That wasn't very fast. Let's profile this implementation using `%lprun` from `line_profiler` (with some lower number to speed it up):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%lprun -f fibonacci_recursion fibonacci_recursion(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That does not look good - there are many more $F_0$ and $F_1$ checks then the actual recursion step summations $F_{n}=F_{n-1}+F_{n-2}$. Let's analyze the algorithm's complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's how the computation will look like:\n",
    "```\n",
    "            f(n)\n",
    "          /     \\\n",
    "     f(n-1)     f(n-2)\n",
    "    /     \\    /     \\\n",
    "f(n-2) f(n-3) f(n-3) f(n-4)\n",
    "  ...    ...    ...    ...\n",
    " /   \\\n",
    "f(1) f(0)\n",
    "```\n",
    "That's a $O\\left(2^n\\right)$ time and space complexity method (there are $2$ calls at each of $n$ recursion steps).\n",
    "\n",
    "And that's a lot of repetitive calls of our function (especially `f(1)` and `f(0)` calls), which we don't really need. Let's try to avoid that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we memoize the already computed results in a lookup table?\n",
    "\n",
    "In the recursive approach, on each function call check if the result was computed for given input, and if it was, then simply read it from memory in $O(1)$ time. So the computation looks then like:\n",
    "```\n",
    "                  f(n)\n",
    "                /     \\\n",
    "           f(n-1)   lookup f(n-2)\n",
    "          /     \\\n",
    "      f(n-2) lookup f(n-3)\n",
    "        ...\n",
    "     f(3)\n",
    "     /  \\\n",
    "  f(2) lookup f(1)\n",
    " /   \\\n",
    "f(1) f(0)\n",
    "```\n",
    "This would be now a $O(n)$ time (left recursion branch) and $O(n)$ space (lookup table) algorithm.\n",
    "\n",
    "Let's try available memory- and disk-based cache implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memoization in memory with `functools.lru_cache`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`@functools.lru_cache`** is a built-in Python decorator that wraps a function with a cache that **saves up to the `maxsize` most recent calls in memory**. It can save time when an expensive or I/O bound function is periodically called with repetitive arguments.\n",
    "\n",
    "An **LRU (least recently used)** cache works best when the most recent calls are the best predictors of upcoming calls (for example, the most popular articles on a news server tend to change each day). The cache’s size limit assures that the cache does not occupy too much space (for example, in long-running processes such as web servers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the Fibbonacci's number using the LRU cache.\n",
    "\n",
    "Second implementation - memoization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=128)\n",
    "def fibonacci_lru(n):\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    if n == 1:\n",
    "        return 1\n",
    "\n",
    "    return fibonacci_lru(n - 1) + fibonacci_lru(n - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time fibonacci_lru(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was much quicker.\n",
    "\n",
    "A `functools.lru_cache`-wrapped function has additional methods to display cache status or to clear (\"invalidate\") cache, respectively, `.cache_info()` and `.cache_clear()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fibonacci_lru.cache_info())\n",
    "# memoization-based solution used `hits` pre-computed results, and\n",
    "# on each of `misses` added cache entry contributing to `currsize`\n",
    "\n",
    "fibonacci_lru.cache_clear()\n",
    "print(fibonacci_lru.cache_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p>\n",
    "        <i class=\"fa fa-warn\"></i>&nbsp<strong>Beware</strong>:\n",
    "        benchmarking memoization-based solutions is tricky - it is only really fair to <strong>measure runtime with the cache setup</strong>.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "This is what we did with timing the single call above. Next runs will be much faster as it is really only getting pre-computed value from the cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time fibonacci_lru(35)\n",
    "%timeit -r3 fibonacci_lru(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%timeit` also tries to detect caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fibonacci_lru.cache_clear()\n",
    "%timeit -r3 -n 1 fibonacci_lru(35) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take-home notes:\n",
    "\n",
    "* caching using a decorator requires **only minimal code change**, and\n",
    "* **in-memory caching is fast**,\n",
    "\n",
    "BUT\n",
    "\n",
    "The LRU or caching **limitation** in general is that **cache arguments must be hashable** (in particular, **non-mutable**), so e.g. `functools.lru_cache` is not applicable to functions with arguments with type such as `list` (but, if possible, you can use `tuple` instead)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memoization in files with `joblib.Memory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joblib](https://joblib.readthedocs.io/) is an additional Python package that provides a set of tools for lightweight pipelining of computations. In particular it provides **disk-based caching of function calls** (memoization) via **`joblib.Memory`** class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fibonacci's number with disk cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import shutil\n",
    "\n",
    "\n",
    "# for a clean demonstration: using a new temp dir as a cache dir\n",
    "cachedir = \"./cache_dir\"\n",
    "\n",
    "shutil.rmtree(cachedir, ignore_errors=True)\n",
    "\n",
    "print(\"Caching to directory:\", cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory_disk = joblib.Memory(location=cachedir, verbose=0)\n",
    "\n",
    "\n",
    "@memory_disk.cache\n",
    "def fibonacci_disk(n):\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    if n == 1:\n",
    "        return 1\n",
    "\n",
    "    return fibonacci_disk(n - 1) + fibonacci_disk(n - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time fibonacci_disk(35)\n",
    "%timeit -r3 fibonacci_lru(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disk access (I/O) is slow**, so setting up or actual reads from the `joblib.Memory` disk cache are slower than in case of a memory cache, but:\n",
    "\n",
    "  * **cached data is available when code is run again** in a new interpreter session,\n",
    "  * it is useful as a **tool for creating \"checkpoints\"/\"snapshots\"** (e.g. during workflow development),\n",
    "  * `joblib.Memory` specifically works with non-hashable arguments, such as `list`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding exercise [15 min]\n",
    "\n",
    "Avoid unnecessary computations in the Euclidian matrix example by avoiding computing `0` values and by using a memory cache to avoid computing each non-zero value twice (hint: use a built-in `sorted` function to order points). How big cache do you need?\n",
    "\n",
    "The memoization strategy pays off only when a single distance computation takes some time. To that end, use the high dimensional points example (below).\n",
    "\n",
    "Compare runtimes (hint: benchmark first memory cache use separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../examples/euclidian_distance_d.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Euclidean distance example in d-dimensional space\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def setup_points(n, d=100_000):\n",
    "    # create n points in d-dim for testing\n",
    "    points = []\n",
    "    for i in range(0, d * n, d):\n",
    "        points.append(tuple(random.random() for j in range(d)))\n",
    "    return points\n",
    "\n",
    "\n",
    "def dist_squared(a, b):\n",
    "    s = 0\n",
    "    d = len(a)\n",
    "    for i in range(d):\n",
    "        s += (a[i] - b[i]) ** 2\n",
    "    return s\n",
    "\n",
    "\n",
    "def dist_matrix(points, dist_func=dist_squared):\n",
    "    # compute distance matrix using given `dist_func`\n",
    "    rows = []\n",
    "    for p in points:\n",
    "        row = []\n",
    "        for q in points:\n",
    "            row.append(dist_func(p, q))\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    M = dist_matrix(setup_points(10))\n",
    "    for row in M[:5]:\n",
    "        print(row[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking setup\n",
    "n_points = 10\n",
    "points = setup_points(n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modify and benchmark distance functions\n",
    "\n",
    "# %timeit -r 3 -n 1 dist_matrix(points, dist_func=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# The point is not that this the optimal way to do this, but that using `if` and\n",
    "# `lru_cache` is rather small and easy code modification that already improves\n",
    "\n",
    "print(f\"Problem size: n = {n_points}, d = {len(points[0])}\")\n",
    "print()\n",
    "\n",
    "print(\"# reference\")\n",
    "%timeit -r 3 -n 1 dist_matrix(points)\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"# no zero distance\")\n",
    "def dist_squared_no_zero(a, b):\n",
    "    if a == b:\n",
    "        return 0\n",
    "    return dist_squared(a, b)\n",
    "\n",
    "%timeit -r 3 -n 1 dist_matrix(points, dist_func=dist_squared_no_zero)\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"# no zero distance and cached ordered pair results\")\n",
    "import functools\n",
    "import time\n",
    "\n",
    "@functools.lru_cache(maxsize=(n_points - 1) * n_points // 2)\n",
    "def dist_squared_cached(a, b):\n",
    "    return dist_squared(a, b)\n",
    "\n",
    "def dist_squared_no_zero_ordered_cached(a, b):\n",
    "    if a == b:\n",
    "        return 0    \n",
    "    p, q = sorted((a, b))  # a < b\n",
    "    return dist_squared_cached(p, q)\n",
    "\n",
    "dist_squared_cached.cache_clear()\n",
    "\n",
    "# only call once because of caching:\n",
    "%timeit -r 1 -n 1 dist_matrix(points, dist_func=dist_squared_no_zero_ordered_cached)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python built-in data structures\n",
    "\n",
    "We will give a quick overview of important built-in Python data structures and complexity of typical operations. We've already seen that choosing the right data structure for your task can make a significant difference.\n",
    "\n",
    "To be able to choose well, or design well a data structure or an algorithm, you need to know the basic data structures first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer\n",
    "2 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float\n",
    "2 + 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding numbers and similar operations are $O(1)$, with one exception of very large integer numbers.\n",
    "\n",
    "Contrary to compiled languages such as `C`, Python integers are implemented internally as 32 or 64 bit numbers or as arrays of these to **avoid overflow**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine in C and in Python:\n",
    "print(2 ** 62)\n",
    "\n",
    "# overflows in C, but not in Python:\n",
    "print(2 ** 63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the 64 bit implementation is not suitable, computations such as addition or multiplication do not run directly on the CPU anymore but are implemented in software. This has two consequences:\n",
    "\n",
    "- operations become suddenly slower\n",
    "- complexity is not $O(1)$ any more but depends on the number of digits of the involved numbers.\n",
    "\n",
    "Luckily these differences are small and **non-constant operations complexity applies only to very large integer numbers** (with 19 and more digits).\n",
    "\n",
    "**float** numbers in Python and **all dtypes in numpy** overflow and, thus, **have constant operations complexity**.\n",
    "\n",
    "Note: if you want non-overflowing numbers in Python use the `decimal` module from the standard library. Here all operations are implemented in software and thus are always slower than working with native `float` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "\n",
    "print(decimal.Decimal(2.0) ** 1100)\n",
    "\n",
    "try:\n",
    "    print(2.0 ** 1100)\n",
    "except OverflowError as e:\n",
    "    print(\"OverflowError\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"This is a string!\"\n",
    "\n",
    "print(\n",
    "    len(a)\n",
    ")\n",
    "\n",
    "i = a.find(\"string\")\n",
    "print(i)\n",
    "\n",
    "print(\n",
    "    a[i:]\n",
    ")\n",
    "\n",
    "print(\n",
    "    a.split()\n",
    ")\n",
    "\n",
    "print(\n",
    "    a.lower().startswith(\"this\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**String is not a basic data type**. Since there is no single character type in Python string seems and feels like an atomic data type, but **string is an [array](https://en.wikipedia.org/wiki/Array_data_structure) of single characters**; as such, **operations on a string depend on its length**.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/String.png\" width=\"600px\"></td></tr>\n",
    "        <tr><td><center><sub>Source: <a href=\"https://en.wikipedia.org/wiki/String_(computer_science)\">https://en.wikipedia.org/wiki/String_(computer_science)</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "Arrays are most often stored as a one/contiguous memory segment of known size and element type, such that position of each element in the memory can be quickly computed from its index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Concatenation\n",
    "\n",
    "Concatenation of two strings `a` and `b` of length $n$ and $m$, , respectively, requires to: \n",
    "\n",
    "1. Allocate new string `c` to hold $n + m$ bytes. \n",
    "2. Copy $n$ bytes from `a` to `c`.\n",
    "3. Copy $m$ bytes from `b` to `c` (after the content from `a`).\n",
    "\n",
    "Step 1 is usually $O(1)$, step 2 is $O(n)$ and step 3 is $O(m)$. In total this is $O(n + m)$.\n",
    "\n",
    "**To concatenate a _hard-coded_ number of strings** use either `+`, or, even better, use [string formatting with f-strings or the `str.format` method](https://docs.python.org/3/tutorial/inputoutput.html#fancier-output-formatting), e.g. `f\"{a}_{b}\"` for concatenation of strings `a` and `b` with `_` as a separator.\n",
    "\n",
    "Naively concatenating $n$ strings of length $m$ one-by-one using `+` results in a $O(m + 2m + 3m + \\ldots + n\\cdot m) = O(m\\cdot (1+...+n)) = O(m\\cdot(n+1)\\cdot n/2) = O(n^2\\cdot m)$ runtime complexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "b = \"1\" * 2_000\n",
    "\n",
    "for n in 1_000, 2_000, 4_000:\n",
    "    c = \"\"\n",
    "    t = time.time()\n",
    "    for _ in range(n):\n",
    "        c = c + b + \"\"  # `+ \"\"` is an anti-optimization trick; more below\n",
    "    needed = time.time() - t\n",
    "    print(f\"concatenating {n:5d} strings of len {len(b)} took {needed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily Python has some **internal optimizations to get around this behavior** (mainly by reusing memory and avoiding / reducing copy operations) in many (**but not all**) situations\n",
    "\n",
    "In the example above we've used `+ \"\"` to trick Python into not using its internal optimization.\n",
    "\n",
    "Let's check how Python internal optimization works out-of-the box in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = \"1\" * 2_000\n",
    "\n",
    "for n in 1_000, 2_000, 4_000:\n",
    "    c = \"\"\n",
    "    t = time.time()\n",
    "    for _ in range(n):\n",
    "        c = c + b\n",
    "    needed = time.time() - t\n",
    "    print(f\"concatenating {n:5d}  strings of len {len(b)} took {needed:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In more complicated situations it is  **not easy to predict when internal optimizations are done**.\n",
    "\n",
    "Instead, **to concatenate a _variable_ number of strings use `join`**:\n",
    "1. collect the strings in a list `l` of strings (if you don't have this list already)\n",
    "2. use `\"\".join(l)` to concatenate strings.\n",
    "\n",
    "Comment: `s.join(l)` concatenates the strings from `l` with the separator string `s` between them.\n",
    "\n",
    "Preparing list for `join` takes $O(n)$ on average, and using `join` allows to pre-allocate final memory and do copy operations once for each of $n$ input string, giving a runtime complexity $O(n\\cdot m)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = \"1\" * 2_000\n",
    "\n",
    "for n in 1_000, 2_000, 4_000:\n",
    "    l = []\n",
    "    s = time.time()\n",
    "    for _ in range(n):\n",
    "        l.append(b)\n",
    "    c = \"\".join(l)\n",
    "    needed = time.time() - s\n",
    "    print(f\"concatenating {n:5d}  strings of len {len(b)} took {needed:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that `a` and `b` are Python strings of length $n$ and $m$, respectively; then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "| Operation | Comment | Complexity | |\n",
    "| - | - | - | - |\n",
    "| `a = \"...\"` | create string | $O(1)$ | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `len(a)`    | length of string  | $O(1)$ |  |\n",
    "| `a[i]` | read string character | $O(1)$ |  |\n",
    "| `a + b`| concatenate two strings| $O(n+m)$ |  |\n",
    "| `\"\".join([b_1, ..., b_n]) ` | concatenate $n$ strings | $O(n\\cdot m)$ |  |\n",
    "| `a.find(b)`/`a.index(b)` | find index of substring | $O(n\\cdot m)$ | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>3</sup></i> |\n",
    "| `b in a`    | check if substring is in a string | $O(n\\cdot m)$ | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>3</sup></i> |\n",
    "| `a.split(b)` | split string using substring as separator | $O(n+m)$ |  |\n",
    "| `a.lower()/a.upper()` | transform string characters to lower/upper case | $O(n)$ |  |\n",
    "| `a.startswith(b)/a.endswith(b)` | check if string starts/ends with substring | $O($$\\min{(n,m)}$$)$|  |\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> Creation of hard-coded stings (literals) `a = \"...\"` is $O(n)$ when the Python bytecode (`.pyc` file) is compiled first by a Python interpreter, but afterwards, the pointer access is $O(1)$.\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> `b in a` or `a.find(b)`/`a.index(b)`, are implemented such that they often run faster than $O(n\\cdot m)$ ([read here](https://web.archive.org/web/20100221040018/http://effbot.org/zone/stringlib.htm)).\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>3</sup></i> For a pattern-based string search use the regular expression methods from the [`re` standard library module](https://docs.python.org/3/library/re.html). They are heavily optimized but there is no theoretical run-time guarantee in $O$ notation.\n",
    "\n",
    "The non-didactic \"string as basic type\" design, combined with many built-in string utilities, is, arguably, one of the cornerstones of Python popularity. Be sure to check out other [string methods of Python's standard library](https://docs.python.org/3/library/stdtypes.html#string-methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(2*i for i in range(5))\n",
    "# equivalently:\n",
    "#   x = [0, 2, 4, 6, 8]\n",
    "# equivalently:\n",
    "#   x = [2*i for i in range(5)]\n",
    "print(x)\n",
    "\n",
    "x.append(x[-1] + 2)\n",
    "print(x)\n",
    "\n",
    "del x[0]\n",
    "print(x)\n",
    "\n",
    "print(\n",
    "    4 in x\n",
    ")\n",
    "print(\n",
    "    x.index(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Python lists are the general workhorse in Python for \"collecting data\".\n",
    "\n",
    "Python lists are implemented using [dynamic arrays](https://en.wikipedia.org/wiki/Dynamic_array) (and not using [linked lists](https://en.wikipedia.org/wiki/Linked_list)). Dynamic arrays have spare space for new elements and are copied into 1.5-2 times larger array when the spare space runs out.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/Dynamic_array.svg\" width=\"250px\"></td></tr>\n",
    "        <tr><td><center><sub>Source: <a href=\"https://en.wikipedia.org/wiki/Dynamic_array\">https://en.wikipedia.org/wiki/Dynamic_array</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "The benefit of using dynamic arrays are low runtime complexities for access to list elements and, on average, for adding list elements, but this comes at the cost of some memory overhead (in Python around 13%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that `x` is a Python list of length $n$, `i` an arbitrary integer, `y` an arbitrary Python object and `it` an arbitrary iterable. Then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Operation | Comment | Complexity | |\n",
    "| - | - | - | - |\n",
    "| `x = list(it)` | create list | $O(\\text{len}(\\text{it}))$ |  |\n",
    "| `len(x)`    | length of list  | $O(1)$ |  |\n",
    "| `x.append(y)`| append element| $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `x.extend(it)` | extend list | $O(\\text{len}(\\text{it}))$ on average |  |\n",
    "| `x[i] = y` | overwrite list element | $O(1)$ |   |\n",
    "| `y = x[i]` | read list element | $O(1)$ |  |\n",
    "| `del x[i]` | remove element | $O(n)$ ($O(1)$ for the last element)| |\n",
    "| `y = x.pop(i)` | read element and remove | $O(n)$ ($O(1)$ for the last element)| |\n",
    "| `x.insert(i, y)` | insert element | $O(n - i)$ | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> |\n",
    "| `x.index(y)` | find index of element | $O(n)$ |  |\n",
    "| `y in x`    | check membership | $O(n)$ |  |\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> Appending and element to a list takes constant time  $O(1)$ in most situations. On rare occasions the internal data needs to be reorganized which takes $O(n)$ time. The average runtime is $O(1)$ (cf. [amortized cost analysis for dynamic array](https://en.wikipedia.org/wiki/Amortized_analysis#Dynamic_array)).\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i>\n",
    "Inserting an element at the beginning of the list using `x.insert(0, y)` is $O(n)$. If reading, appending or deleting both ends of a list are common operations in your problem use a `deque` (*double ended queue*) data structure from the [collections module](https://docs.python.org/3/library/collections.html#collections.deque); they all have runtime $O(1)$. The caveat is that operations on `deque` inner elements like `x[i] = y` or `y = x[i]` are $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuples are immutable lists, so there are no element insertion or deletion operations, but otherwise behave just like lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tuple(2*i for i in range(5))\n",
    "# equivalently:\n",
    "#   x = (0, 2, 4, 6, 8)\n",
    "print(x)\n",
    "\n",
    "print(\n",
    "    4 in x\n",
    ")\n",
    "print(\n",
    "    x.index(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuples can be slightly faster than lists, but replacing tuples by lists for reasons of performance is most often an unnecessary micro-optimization.\n",
    "\n",
    "Tuples are \"hashable\" so can be used e.g. as dictionary keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dict((2*i, i) for i in range(5))\n",
    "# equivalently:\n",
    "#   x = {0: 0, 2: 1, 4: 2, 6: 3, 8: 4}\n",
    "# equivalently:\n",
    "#   x = {2*i: i for i in range(5)}\n",
    "print(x)\n",
    "\n",
    "x[2*5] = 5\n",
    "print(x)\n",
    "\n",
    "del x[0]\n",
    "print(x)\n",
    "\n",
    "print(\n",
    "    4 in x\n",
    ")\n",
    "print(\n",
    "    5 in x.values()\n",
    ")\n",
    "print(\n",
    "    x.get(5, None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries implement lookup tables and are heavily optimized in Python.\n",
    "\n",
    "- dictionary **values can be arbitrary** Python objects,\n",
    "- dictionary **keys must be immutable** Python objects (such as `int`, `str`, `tuple`s of immutable objects, **not**: `list`s, `set`s, `dict`s).\n",
    "- dictionary **keys are unique**.\n",
    "\n",
    "\n",
    "Dictionaries are also used inside the Python interpreter in many places.\n",
    "\n",
    "E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binascii\n",
    "\n",
    "binascii.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when you e.g. use `binascii.hexlify`, the Python interpreter actually accesses `binascii.__dict__[\"hexlify\"]`.\n",
    "\n",
    "Thus having a fast and efficient dictionary implementation is crucial for the overall speed of Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that `x` is a Python dictionary with $n$ entries, `k` an arbitrary object which can be used as key, `y` an arbitrary Python object and `it` an arbitrary iterable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Operation | Comment | Complexity | |\n",
    "| - | - | - | - |\n",
    "| `x = dict(it)` | create dictionary | $O(\\text{len}(\\text{it}))$ | |\n",
    "| `len(x)`   | size of dictionary | $O(1)$ | |\n",
    "| `x[k] = y` | insert/overwrite value at key | $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> | \n",
    "| `y = x[k]`/`y = x.get(k)` | lookup value at key | $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `del x[k]` | remove value and key | $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `k in x`   | key membership test | $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `x.keys()` | keys of dictionary | $O(1)$ | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> |\n",
    "| `x.values()` | values of dictionary | $O(1)$ | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> |\n",
    "| `x.items()` | key and value pairs of dictionary | $O(1)$ | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> |\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> The internal data structure used for dictionaries is a so-called [Hash table / Hash map](https://en.wikipedia.org/wiki/Hash_table) (for Python-specific). This data structure makes lookup, insertion and deletion operations $O(1)$ on average (using so called hashing of keys and open addressing strategy to resolve hash conflicts; [read more](http://thepythoncorner.com/dev/hash-tables-understanding-dictionaries/)).\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> A `for` loop over `.keys()`, `.values()` or `.items()` is still $O(n)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dict.setdefault` and `Counter`\n",
    "\n",
    "\n",
    "Python's standard library offers a class which adds some convenience on-top of dictionaries to make your live easier: the `collections.Counter` class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Counter`](https://pymotw.com/3/collections/counter.html) takes a collection of data / an iterable and computes a dictionary which maps each item from the data to the number of its occurrences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "items = list(range(5)) + list(range(4)) + list(range(2))\n",
    "print(items)\n",
    "print()\n",
    "\n",
    "counter = Counter(items)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We also introduced\n",
    " [`defaultdict`](https://pymotw.com/3/collections/defaultdict.html) during previous versions of the script but `defaultdict` can lead to hard to debug issues when not handled carefully, e.g. when passing a `defaultdict` to a function which expects a proper `dict`. Instead the `dict.setdefault` method can be used:\n",
    " \n",
    "This methods sets a default value for unknown keys and returns the updated dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {3 : 9}\n",
    "print(d.setdefault(2, 4))  # creates new entry and creates new value 4\n",
    "print(d.setdefault(3, 10))  # key exists already and thus returns 9\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set(i//2 for i in range(5))\n",
    "# equivalently:\n",
    "#   a = set([0, 0, 1, 1, 2])\n",
    "print(a)\n",
    "\n",
    "print(\n",
    "    1 in a\n",
    ")\n",
    "\n",
    "a.add(3)\n",
    "print(a)\n",
    "\n",
    "a.remove(0)\n",
    "print(a)\n",
    "\n",
    "b = a.union(set([3, 4])).difference(set([0, 1]))\n",
    "print(b)\n",
    "print(a.symmetric_difference(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python sets are a fast lookup-based collections. Recall, that we've already used sets in Section 2 to optimize our profiling example.\n",
    "\n",
    "A set in Python represents a mathematical set. Contrary to lists / tuples:\n",
    "\n",
    "1. There are **no duplicate elements** in a set.\n",
    "2. Set has **no order**; e.g., you can not ask for the first element of set.\n",
    "3. Set **elements must be immutable** (same as for dictionary keys)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets can be cleverly used e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_only_unique_elements(li):\n",
    "    return len(set(li)) == len(li)\n",
    "\n",
    "\n",
    "def count_duplicate_elements(li):\n",
    "    return len(li) - len(set(li))\n",
    "\n",
    "\n",
    "print(has_only_unique_elements([1, 2, 3]))\n",
    "print(has_only_unique_elements([1, 2, 3, 1]))\n",
    "\n",
    "print(count_duplicate_elements([1, 2, 3, 1, 2, 1, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that `a` and `b` are Python sets of size $n$ and $m$, respectively; then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Operation | Comment | <div style=\"width:200px\">Complexity</div> | |\n",
    "| - | - | - | - |\n",
    "| `a = set(it)` | create set | $O(\\text{len}(\\text{it}))$ | |\n",
    "| `len(a)`   | size of set | $O(1)$ | |\n",
    "| `a.add(x)` | add element | $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `x in a` | element membership test | $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `a.remove(x)` | remove element | $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `a \\| b`, `a.union(b)`  | set union  | $O(n + m)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `a - b`, `a.difference(b)`  | set difference  | $O(n)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `a ^ b`, `a.symmetric_difference(b)` | set symmetric difference | $O(n + m)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `a & b`, `a.intersection(b)`  | set intersection  | $O(\\min(n, m))$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> As with dictionaries, sets are implemented in Python using hash tables (with dummy values and some optimizations for that), making lookup (membership test), insertion and deletion operations $O(1)$ on average, as well as all other size-dependent operations on average, as they use the lookup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in `frozenset` is to `set` as `tuple` is to `list` - an immutable variant of the corresponding collection type, that does not allow for element addition or removal but, e.g., can be used as a dictionary keys or other sets elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = frozenset(i//2 for i in range(5))\n",
    "# equivalently:\n",
    "#   a = frozenset([0, 0, 1, 1, 2])\n",
    "print(a)\n",
    "\n",
    "print(\n",
    "    1 in a\n",
    ")\n",
    "\n",
    "# Since the union etc. operations create new sets, the result is also a frozenset\n",
    "b = a.union(set([3, 4])).difference(set([0, 1]))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Coding exercise [15 min]\n",
    "\n",
    "Given two strings `s1` and `s2` of words which are separated by spaces, find words which are unique in each string and only appear in one of the strings; for instance:\n",
    "```python\n",
    "s1 = \"you used to code in MATLAB but then you switched to Python\"\n",
    "s2 = \"we used to program in MATLAB but then we switched to Python\"\n",
    "\n",
    "unique_words_in_strings(s1, s2)\n",
    "```\n",
    "should return any of:\n",
    "```python\n",
    "['code', 'program']\n",
    "['program', 'code']\n",
    "```\n",
    "\n",
    "Hint: use lookup-based structures such as `set` or `Counter`.\n",
    "\n",
    "What is the time and space complexity of your solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def _unique_words(words, excluded_words):\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    unique_words = ...\n",
    "    ...\n",
    "    for word in unique_words:\n",
    "        if ...:\n",
    "            out.append(word)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def unique_words_in_strings(s1, s2):\n",
    "\n",
    "    s1_words = ...\n",
    "    s2_words = ...\n",
    "\n",
    "    return _unique_words(s1_words, s2_words) + _unique_words(s2_words, s1_words)\n",
    "\n",
    "\n",
    "# s1 = \"you used to code in MATLAB but then you switched to Python\"\n",
    "# s2 = \"we used to program in MATLAB but then we switched to Python\"\n",
    "\n",
    "# unique_words(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def _unique_words(words, excluded_words):\n",
    "    \n",
    "    unique_words = set(words) - set(excluded_words)  # O(n + m) time and mem\n",
    "    words_counter = Counter(words)  # O(n) time\n",
    "    \n",
    "    out = []\n",
    "    # O(n + m) space and time:\n",
    "    for word in unique_words:\n",
    "        if words_counter[word] == 1:\n",
    "            out.append(word)\n",
    "    return out\n",
    "\n",
    "def unique_words_in_strings(s1, s2):\n",
    "\n",
    "    # worst-case: one letter words only (string size equals half of the words)\n",
    "    s1_words = s1.split()  # O(n) time and mem\n",
    "    s2_words = s2.split()  # O(m) time and mem\n",
    "\n",
    "    # O(n + m) time and mem (output)\n",
    "    return _unique_words(s1_words, s2_words) + _unique_words(s2_words, s1_words)\n",
    "    # Total complexity: O(n+m) time and mem\n",
    "\n",
    "\n",
    "# Bonus: using once set symmetric difference and a single list comprehension,\n",
    "#        instead of twice set difference, list comprehension, and additional list\n",
    "#        concatenation\n",
    "#        => smaller hidden complexity constant\n",
    "def unique_words_in_strings_2(s1, s2):\n",
    "    \n",
    "    # worst-case: one letter words only (string size equals half of the words)\n",
    "    s1_words = s1.split()  # O(n) time and mem\n",
    "    s2_words = s2.split()  # O(m) time and mem\n",
    "\n",
    "    # worst-case: each word occurs exactly once\n",
    "    unique_words = set(s1_words) ^ set(s2_words)  # O(n + m) time and mem\n",
    "    \n",
    "    s1_counter = Counter(s1_words)  # O(n) time and mem\n",
    "    s2_counter = Counter(s2_words)  # O(m) time and mem\n",
    "    \n",
    "    return [\n",
    "        w for w in unique_words\n",
    "        if w in s1_counter and s1_counter[w] == 1 or s2_counter[w] == 1\n",
    "    ] # O(n+m) time and mem\n",
    "\n",
    "    # Total complexity: O(n+m) time and mem\n",
    "\n",
    "s1 = \"you used to code in MATLAB but then you switched to Python\"\n",
    "s2 = \"we used to program in MATLAB but then we switched to Python\"\n",
    "\n",
    "print(unique_words_in_strings(s1, s2))\n",
    "print(unique_words_in_strings_2(s1, s2))\n",
    "\n",
    "# the constant imporovement gets more visible with bigger input\n",
    "%timeit unique_words_in_strings(s1*100, s2)\n",
    "%timeit unique_words_in_strings_2(s1*100, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional coding exercise\n",
    "\n",
    "Can you now implement an optimized version of `count_common` from Script 2 such that duplicates in both collections are also counted correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "from collections import Counter\n",
    "\n",
    "def count_common(data_1, data_2):\n",
    "        \n",
    "    # o(n)\n",
    "    c = Counter(data_2)\n",
    "    counts = 0\n",
    "    \n",
    "    # o(n)\n",
    "    for item in data_1:\n",
    "        counts += c[item]\n",
    "    return counts\n",
    "        \n",
    "    \n",
    "print(count_common([1, 2, 1, 2, 3], [3, 4, 1, 1, 7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using better algorithms and data structures: examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, that in the Fibonacci's number example, we improved the time complexity of a brute-force recursive method from $O\\left(2^n\\right)$ to $O(n)$ by a memoization-based solution with $O(n)$ space complexity.\n",
    "\n",
    "But we actually can do even a bit better memoizing only the last two Fibonacci's numbers, and, thus, replacing the recursion with a straightforward `for` loop:\n",
    "```\n",
    "iteration 1:   f(2) = f(1) + f(0)\n",
    "iteration 2:   f(3) = f(2) + f(1)\n",
    "...\n",
    "iteration n-1: f(n) = f(n-1) + f(n-2)\n",
    "```\n",
    "\n",
    "Third implementation - sum up in a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fibonacci_loop(n):\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    if n == 1:\n",
    "        return 1\n",
    "\n",
    "    prev_prev_fib = 0\n",
    "    prev_fib = 1\n",
    "    for i in range(2, n + 1):\n",
    "        fib = prev_fib + prev_prev_fib\n",
    "        prev_prev_fib = prev_fib\n",
    "        prev_fib = fib\n",
    "    return fib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time fibonacci_loop(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now a $O(n)$ time (`for` loop) and $O(1)$ space (three variables) algorithm.\n",
    "\n",
    "The loop solution not only uses less memory than the memoization-based solution, but is actually also a bit faster in practice (as no time is needed for setting up the cache data structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some ideas for what algorithmic improvements may entail on example of two basic computing problems: sorting and searching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection sort\n",
    "\n",
    "A simple method to sort a list of numbers is the [selection sort algorithm](https://en.wikipedia.org/wiki/Selection_sort):\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "    <tr>\n",
    "        <td style=\"font-size:120%; vertical-align:top; horizontal-align:left; width: 450px\">\n",
    "            <ol>\n",
    "                <li>\n",
    "                    <p>find the smallest number starting at position 0 and swap this with the entry at position 0.</p>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <p>find the smallest number starting at position 1 and swap this with the entry at position 1.</p>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <p>find the smallest number starting at position 2 and swap this with the entry at position 2.</p>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <p>...</p>\n",
    "                </li>\n",
    "            </ol>\n",
    "        </td>\n",
    "        <td style=\"width: 300px;\">\n",
    "            <center>\n",
    "                <img src=\"imgs/selection_sort.jpg\" />\n",
    "                <sub>Source: <a href=\"https://stackoverflow.com/questions/36700830/selection-sort-algorithm\">https://stackoverflow.com/questions/36700830/selection-sort-algorithm</a></sub>\n",
    "            </center>\n",
    "        </td>\n",
    "    </tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def selection_sort(data):\n",
    "    # as described above\n",
    "    for starting_position in range(len(data) - 1):\n",
    "        index = find_position(data, starting_position)\n",
    "        swap(data, index, starting_position)\n",
    "\n",
    "\n",
    "def find_position(data, starting_position):\n",
    "    # we start with assuming that the value at best_idx is the smallest\n",
    "    # value\n",
    "    best_idx = starting_position\n",
    "    smallest_value = data[starting_position]\n",
    "\n",
    "    # ... and the we iterate over the rest of the list updating best_idx:\n",
    "    for idx in range(starting_position + 1, len(data)):\n",
    "        if data[idx] < smallest_value:\n",
    "            smallest_value = data[idx]\n",
    "            best_idx = idx\n",
    "    return best_idx\n",
    "\n",
    "\n",
    "def swap(data, index_1, index_2):\n",
    "    data[index_1], data[index_2] = data[index_2], data[index_1]\n",
    "\n",
    "\n",
    "random.seed(42)  # for reproducibility\n",
    "\n",
    "data = list(range(20))\n",
    "random.shuffle(data)\n",
    "print(data)\n",
    "\n",
    "selection_sort(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 120%\"><strong>Runtime analysis</strong><sup>*</sup></p>\n",
    "\n",
    "\n",
    "The runtime complexity analysis for this algorithm applied to a list of length $n$ is as follows:\n",
    "\n",
    "We count the operations in each step which depend on $n$ and name the remaining number of operations per step as $n_0$ (e.g. array access, increasing loop counter, swap).\n",
    "\n",
    "- The first step needs $n - 1$ comparisons and $n_0$ other operations\n",
    "- The second step needs $n - 2$ comparisons and $n_0$ other operations\n",
    "- The third step needs $n - 3$ comparisons and $n_0$ other operations\n",
    "- ...\n",
    "-  The $n-1$st step needs $1$ comparisons and $n_0$ other operations.\n",
    "\n",
    "This is in total\n",
    "\n",
    "$$\n",
    "   (n - 1 + n_0)  + (n - 2 + n_0) + \\ldots + (1 + n_0)\n",
    "$$\n",
    "\n",
    "The number of terms in parenthesis is $n-1$ and separating $n_0$ plus rearranging terms leads to\n",
    "\n",
    "$$\n",
    "(n - 1) + (n - 2) + \\ldots + 1 + \\,\\,\\, (n-1) n_0 \n",
    "$$\n",
    "\n",
    "Using some math for summing up the first $n - 1$ natural numbers this is the same as\n",
    "\n",
    "$$\n",
    "\\frac{n (n - 1)}{2} + (n-1) n_0 = \\frac{n^2}{2} + \\frac{n - 1}{2} \\left(1 + 2 n_0 \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "We can ignore the second summand $\\frac{n - 1}{2} \\left(1 + 2 n_0 \\right)$ which is linear in $n$ and grows slower than $n^2$,\n",
    "\n",
    "we further assume that each  operation (depending if it is comparison, swap, ...) has a runtime between $t_0$ and $t_1$ and thus the total runtime $T(n)$ is bounded by\n",
    "\n",
    "$$\n",
    "t_0 \\left\\{ \\frac{n^2}{2} + \\ldots \\right\\} \\le T(n) \\le t_1 \\left\\{ \\frac{n^2}{2} + \\ldots \\right\\}\n",
    "$$\n",
    "\n",
    "Since we omit constants and lower order terms in the $O$ notation we conclude that \n",
    "$$T(n) = O(n^2)$$ for the runtime of selection sort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes we can! Practically important sorting algorithms have a runtime complexity $O(n \\log  n)$ which grows **much slower** than $O(n^2)$. \n",
    "\n",
    "**Note**: $\\log n$ is proportional to the number of digits of $n$ and thus $O(n \\log n)$ is often said to be \"almost linear\".\n",
    "\n",
    "Known standard algorithms from this class are\n",
    "\n",
    "- [Merge sort](https://en.wikipedia.org/wiki/Merge_sort) is a [stable](https://en.wikipedia.org/wiki/Sorting_algorithm#Stability) sorting algorithm, with $O(n \\log n)$ runtime and $O(n)$ memory requirements.</br>\n",
    "  It can sort efficiently data which does fit into memory. Conceptually, you divide data into smaller sub-problems until you reach single elements, which are sorted, and then re-assemble sorted lists by zipping them together simultaneously.\n",
    "\n",
    "<table>\n",
    "<tr><td><img src=\"imgs/merge_sort.png\" width=\"600px\"></td></tr>\n",
    "<tr><td><center><sub>Source: <a href=\"https://en.wikipedia.org/wiki/Merge_sort\">https://en.wikipedia.org/wiki/Merge_sort</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "- [Quick sort](https://en.wikipedia.org/wiki/Quicksort) is not stable, with $O(n \\log n)$ runtime **on average** and $O(\\log n)$ memory requirements.</br>\n",
    "  In corner cases such as presorted data or reversed presorted data the runtime can degrade to $O(n^2)$. Nevertheless quick sort is used often in practice due to its lower memory requirement and lower (hidden) constants in the runtime complexity compared to merge sort. Efficient implementations of quicksort are among the fastest sorting algorithms in practice. \n",
    "- [Timsort](https://en.wikipedia.org/wiki/Timsort)  is stable, with $O(n \\log n)$ worst-case runtime and $O(n)$ worst-case memory requirement.</br>\n",
    "  It is a hybrid algorithm, derived from merge sort and insertion sort ($O(n^2)$ algorithm), designed to perform well in practice.\n",
    "  It has small hidden complexity constants and is fast on naturally occurring (partially) pre-sorted data.</br>\n",
    "  **This is the sorting algorithm used in Python** (since version 2.3 and became meanwhile also the sorting algorithm used in Java and some other programming languages).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "k = 500_000\n",
    "\n",
    "random.seed(42)  # for reproducibility\n",
    "\n",
    "for n in 1_000, 2_000, 4_000:\n",
    "    l = [random.randint(0, k) for i in range(n)]\n",
    "    \n",
    "    t = time.time()\n",
    "    selection_sort(l)\n",
    "    needed = time.time() - t\n",
    "    print(f\"selection sort of list of len {n} took {needed:.2f} seconds\")\n",
    "\n",
    "print()\n",
    "\n",
    "for n in 100_000, 200_000, 400_000:\n",
    "    l = [random.randint(0, k) for i in range(n)]\n",
    "    \n",
    "    t = time.time()\n",
    "    l = sorted(l)\n",
    "    needed = time.time() - t\n",
    "    print(f\"timsort of list of len {n} took {needed:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special sorting algorithms\n",
    "\n",
    "One can prove that a **sorting algorithm which is based on comparing items can not be faster than $O(n \\log n)$**.\n",
    "\n",
    "**But**: can you sort items without comparing them?\n",
    "\n",
    "Yes, under some extra assumptions, you actually can sort items without comparing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 120%\">Example: <a href=\"https://en.wikipedia.org/wiki/Counting_sort\"><strong>Counting sort</strong></a></p>\n",
    "\n",
    "Counting sort algorithm **assumes that the items to sort belong to a limited set $S$ of $N$ items with known order**; e.g. the numbers $0 \\ldots N -1$. If the maximum item is not given, you can always first find it in $O(n)$ time.\n",
    "\n",
    "We continue to show case how this sort works using numbers.\n",
    "\n",
    "The idea is as follows:\n",
    "\n",
    "1. Initialise array of counts for every item in $S$ with `0` values.\n",
    "1. Run once over the input numbers increasing count for every seen item.\n",
    "2. Iterate over the $S$ counts array, from smallest to largest number, and append each number to the result as many times as it was seen in the input.\n",
    "\n",
    "E.g the list `[3, 2, 1, 0, 1, 3, 5, 1, 2, 3]` would result in the following counts and output lists:\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/algorithms-counting_sort.png\" width=\"500px\"></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume domain S is integer numbers \n",
    "\n",
    "def counting_sort(numbers):\n",
    "    # find N = max(S)\n",
    "    # O(n) time, O(1) space\n",
    "    N = max(numbers) + 1\n",
    "\n",
    "    # count items\n",
    "    # O(n) time, O(N) space\n",
    "    counts = [0] * N\n",
    "    for number in numbers:\n",
    "        counts[number] += 1\n",
    "\n",
    "    # output items\n",
    "    # O(N) time, O(n) space\n",
    "    result = []\n",
    "    for i in range(N):\n",
    "        result.extend([i] * counts[i])\n",
    "\n",
    "    return result\n",
    "\n",
    "    # total: O(n + N) time, O(n + N) memory\n",
    "\n",
    "\n",
    "numbers = [3, 2, 1, 0, 1, 3, 5, 1, 2, 3]\n",
    "\n",
    "counting_sort(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runtime complexity of this algorithm is $O(n + N)$ with a \"hard\" $O(n + N)$ memory requirement. This is only practical with a relatively small $N$;\n",
    "\n",
    "Take-home notes:\n",
    "\n",
    "* We **leverage additional knowledge** of the sort domain to get the $O(n)$ performance, at a cost of a potentially much bigger space requirement.\n",
    "* For problems with an already \"fast\" algorithms available, quite often there is a **trade-off between space and runtime**, i.e. reducing runtime requires additional space, and vice-versa.\n",
    "\n",
    "Other notable sorting algorithms which do not rely on comparisons are [bucket sort](https://en.wikipedia.org/wiki/Bucket_sort) and [radix sort](https://en.wikipedia.org/wiki/Radix_sort).\n",
    "\n",
    "All these algorithms do not work on general data and can include a significant memory overhead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### How to sort in Python?\n",
    "\n",
    "The previous explanations served the purpose to introduce runtime analysis and some sorting basics. In practice you should use available sorting algorithms:\n",
    "\n",
    "1. `sorted()` or `list.sort()` for sorting arbitrary data in Python; see [Python Documentation HOWTO on sorting](https://docs.python.org/3/howto/sorting.html#sortinghowto).\n",
    "2. `numpy.sort()` for numerical data, which also offers $O(n)$ radix sort for integers (see [`numpy.sort` documentation](https://numpy.org/doc/stable/reference/generated/numpy.sort.html)).\n",
    "\n",
    "Implementing your own sorting algorithm can be fun and insightful but you can not expect that your result will outperform these implementations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching\n",
    "\n",
    "Searching algorithms can be divided into exact and approximate matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact searching\n",
    "\n",
    "Python offers several methods to find an element in a given data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List lookup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"ab\", \"def\", \"ghi\", \"xyz\"]\n",
    "\n",
    "print(\"ghi\" in words)\n",
    "print(words.index(\"ghi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both operations are $O(n)$ if $n$ is the size of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary lookup**\n",
    "\n",
    "In case the items in our collection are unique (no duplicates) we can search faster.\n",
    "\n",
    "Either we reduce lookup time by using a dictionary (assuming elements are immutable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_indices = {}\n",
    "for position, word in enumerate(words):\n",
    "    words_indices[word] = position\n",
    "\n",
    "print(\"ghi\" in words_indices.keys())\n",
    "print(words_indices[\"ghi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the preprocessing time is $O(n)$ (with $O(n)$ extra memory), but all subsequent lookups are $O(1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set lookup**\n",
    "\n",
    "In case you only want to check for membership, you can also use a set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_unique = set(words)\n",
    "print(\"ghi\" in words_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives also $O(1)$ lookup, but has lower memory requirements than a dictionary (by a constant factor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary search**\n",
    "\n",
    "In case your data set is sorted you can use binary search from the `bisect` module from the standard library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "\n",
    "\n",
    "words = sorted(words)\n",
    "\n",
    "\n",
    "def get_index_of(element, sorted_sequence):\n",
    "    i = bisect.bisect_left(sorted_sequence, element)\n",
    "    # all sequence elements in positions:\n",
    "    #     < i are smaller\n",
    "    #    >= i are greater or equal\n",
    "    if i >= len(sorted_sequence) or sorted_sequence[i] != element:\n",
    "        raise ValueError(f'\"{element}\" not found')\n",
    "    return i\n",
    "\n",
    "\n",
    "print(get_index_of(\"ghi\", words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary search algorithm implements the way we would naively search in a phone book or encyclopedia:\n",
    "\n",
    "1. look in the middle,\n",
    "2. if the entry at this position is the entry we are looking for we are done;\n",
    "3. otherwise, if the entry at this position is \"smaller\" than the entry we are looking for, we restrict search to the 2nd half and repeat from 1. with a \"reduced\" collection.\n",
    "4. else, we restrict search to the 1st half and repeat from 1. with this \"reduced\" collection.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/algorithms-binary_search.png\" width=\"800px\"></td></tr>\n",
    "</table>\n",
    "\n",
    "Notes:\n",
    "* if the item was not found you end up with a position where the number should have been, if it would be present in the input;\n",
    "* in implementation we do not actually \"reduce the data set\" - we only narrow down lower or upper index to indicate section of the data that is to be used in a recursive call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For run-time analysis we can establish the relation $T(n) = T(n / 2) +  c$ because we can reduce the data set \"virtually\" by a factor of 2 in every step, having some fixed cost $c$ in every iteration. Using the so called [master theorem](https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)) we can conclude that this algorithm has run-time complexity $O(\\log n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "Assuming list of $n$ element as input, this is an exact search summary table sorted by increasing memory requirements (which you can neglect in most situations):\n",
    "\n",
    "| method | runtime &nbsp; &nbsp; | works with duplicates |  requires | returns position |\n",
    "|--------|---------|-----------------------|-----------------|------------------|\n",
    "| list lookup | $O(n)$ | yes |  - | yes |\n",
    "| bisection | $O(\\log n)$ | yes |  data sorting $O(n \\log n)$ | yes |\n",
    "| set | $O(1)$ | no | set construction $O(n)$ | no |\n",
    "| dict| $O(1)$ | no | dict construction $O(n)$ | yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate search*\n",
    "\n",
    "Approximate search is used to find elements in a collection which are **close** to a given element. In most cases closeness is measured by an mathematical distance, such as $|a - b$| for numbers or $\\|a - b \\|_p$ ($p = 2$ is euclidean distance) for multi-dimensional data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching numbers\n",
    "\n",
    "**List lookup**\n",
    "\n",
    "Using a list we return numbers within the given proximity tolerance threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [(i % 5) / 10 for i in range(10)]\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_approx(numbers, number, tolerance):\n",
    "    \"\"\"find numbers which deviate from `number` by\n",
    "    max distance `tolerance`\"\"\"\n",
    "    positions, matches = [], []\n",
    "    for position, current_number in enumerate(numbers):\n",
    "        if abs(number - current_number) <= tolerance:\n",
    "            positions.append(position)\n",
    "            matches.append(current_number)\n",
    "\n",
    "    return positions, matches\n",
    "\n",
    "\n",
    "positions, matches = find_approx(numbers, 0.25, tolerance=0.1)\n",
    "print(\"found\", matches, \"at positions\", positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime of this approach is $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary search**\n",
    "\n",
    "Using binary search we:\n",
    "\n",
    "1. require the input numbers to be sorted,\n",
    "2. have to adjust the search for the tolerance threshold - we can look for index where minimum `number - tolerance` number would go and return numbers until we get over maximum `number + tolerance` (or vice versa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_numbers = sorted(numbers)\n",
    "print(sorted_numbers)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_approx_sorted(sorted_numbers, number, tolerance):\n",
    "    # results will be in the interval\n",
    "    # [number - tolerance, number + tolerance]\n",
    "\n",
    "    # find first element >= number - tolerance:\n",
    "    current_index = bisect.bisect_left(sorted_numbers, number - tolerance)\n",
    "    positions, matches = [], []\n",
    "\n",
    "    # go through subsequent sorted elements while tolerance is kept\n",
    "    while (\n",
    "        current_index < len(sorted_numbers)  # check if not end first!\n",
    "        and abs(sorted_numbers[current_index] - number) <= tolerance\n",
    "    ):\n",
    "        positions.append(current_index)\n",
    "        matches.append(sorted_numbers[current_index])\n",
    "        current_index += 1\n",
    "\n",
    "    return positions, matches\n",
    "\n",
    "\n",
    "positions, matches = find_approx_sorted(sorted_numbers, 0.25, tolerance=0.1)\n",
    "print(\"found\", matches, \"at positions\", positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime of this approach is $O(\\log n + N)$ for sorted data of size $n$ and $N$ matches.\n",
    "\n",
    "In case your data is not sorted from the beginning the complexity of a single search is $O(n \\log n)$ which looks worse than the simple list approach from before. \n",
    "\n",
    "But if you do **many look-ups** $k$ in the same data it can be much faster as data is sorted once; assuming tolerance low enough to make the number of matches in each lookup $N$ negligible, the complexity in multi-lookup case is $O(n \\log n + k \\log n)$, which is linear in $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching n-dimensional data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate search in $n$-dimensional euclidean spaces is a common task in some fields such as machine learning (e.g. the [kNN classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)), [geographic information systems (GIS)](https://en.wikipedia.org/wiki/Geographic_information_system), [computer graphics](https://en.wikipedia.org/wiki/Space_partitioning#In_computer_graphics) and [computer games](https://en.wikipedia.org/wiki/Collision_detection#Spatial_partitioning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard algorithms for n-dimensional point search boil down to building and using special data structures, analogously to dictionary-based exact search of an element's position.\n",
    "\n",
    "The main data structures from this domain are so called [k-d trees](https://en.wikipedia.org/wiki/K-d_tree).\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr><td><img  src=\"imgs/3dtree.png\" width=\"300\" ></td></tr>\n",
    "        <tr><td><center><sub>Source: <a href=\"https://en.wikipedia.org/wiki/K-d_tree\">https://en.wikipedia.org/wiki/K-d_tree</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For $n$ data points, which are randomly distributed $k$-d trees offer nearest point lookup in $O(\\log n)$ time and an (axis-parallel) range lookup returning $m$ points in $O\\left(n^{1−\\frac{1}{k}} + m\\right)$ time. Performance **degrades in higher dimensions** and for some particular data distributions. General runtime analysis is difficult.\n",
    "\n",
    "Such data structures also require time to build, e.g. from a Python list or from numpy arrays. Depending on the implementation setting up a $k$-d tree takes $O(n \\log n)$ or $O(n \\log^2 n)$ time: [see also here](https://en.wikipedia.org/wiki/K-d_tree#Complexity).\n",
    "\n",
    "Python implementations of $k$-d trees can be found\n",
    "- in `scipy`: [scipy.spatial.cKDTrees](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.cKDTree.html#scipy-spatial-ckdtree), this implementation also supports multiple core computations for many lookups and matching two data sets for finding pairs of close points.\n",
    "- in `scikit-learn`: [sklearn.neighbors.KDTree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# matrix of 100 points in 3d space\n",
    "np.random.seed(42)  # for reproducibility\n",
    "points = np.random.random(size=(100, 3))\n",
    "\n",
    "tree = cKDTree(points)\n",
    "\n",
    "# find points around center which max distance 0.25\n",
    "center = np.array([0.5, 0.5, 0.5])\n",
    "rows = tree.query_ball_point(center, r=0.25)\n",
    "\n",
    "print(\"matching points\")\n",
    "print(points[rows])\n",
    "\n",
    "print(\"actual distances\")\n",
    "print(np.sum((points[rows] - center) ** 2, axis=1) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate point search for a-priori known distance tolerance*\n",
    "\n",
    "In case one runs many queries to match points in $n$-dimensional space ($n$ small) and one uses a constant search tolerance, one can use the following technique. This can be handy to match data points from 2 collections, as e.g. when matching $(rt, m)$ peaks from an LCMS device or star catalogs in astronomy.\n",
    "\n",
    "We demonstrate the idea for two dimensional point collections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(points, tolerance):\n",
    "    \"\"\"we 'round' points to a n-d grid with grid length\n",
    "    tolerance\"\"\"\n",
    "    \n",
    "    lookup = {}\n",
    "    for point in points:\n",
    "        # must also be hashable:\n",
    "        grid_point = tuple((point / tolerance).astype(np.int64))\n",
    "        lookup.setdefault(grid_point, []).append(point)\n",
    "    return lookup\n",
    "\n",
    "\n",
    "def search(point, lookup, tolerance):\n",
    "    \"\"\"find all points in lookup which are close to `point` up\n",
    "    to given tolerance`\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    center_grid_point = (point / tolerance).astype(np.int64)\n",
    "    \n",
    "    # we have to iterate over neighbours also, might get\n",
    "    # false positives though:\n",
    "    for grid_point in _neighbours(center_grid_point):\n",
    "        candidates = lookup.get(grid_point, [])\n",
    "        \n",
    "        # filter out false positives:\n",
    "        for candidate in candidates:\n",
    "            if np.linalg.norm(point - candidate) < tolerance:\n",
    "                result.append(candidate)\n",
    "    return result\n",
    "    \n",
    "def _neighbours(grid_point):\n",
    "    \"\"\"iterate over all neihbouring cells of the point\"\"\"\n",
    "    # O(3^dim(point) !!!!! So this becomes infeasible in high dimensions:\n",
    "    for offset in itertools.product(*((-1, 0, 1),) * len(grid_point)):\n",
    "        yield tuple(grid_point + offset)\n",
    "            \n",
    "        \n",
    "np.random.seed(42)\n",
    "points_1 = np.round(np.random.random((10, 3)), 3)\n",
    "points_2 = points_1 + 0.01 * np.ones((10, 3))\n",
    "\n",
    "tolerance = 0.5\n",
    "lookup = preprocess(points_1, tolerance)\n",
    "for point in points_2:\n",
    "    matches = search(point, lookup, tolerance)\n",
    "    print(\"point\", point, \"has\", len(matches), \"matches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: in case the points are scattered over a space which is much larger  than the search tolerance, this technique is efficient since the `candidate` list will be small. This performance degrades to $O(n)$ in the worst-case situation that all points are rounded to the same grid point.\n",
    "\n",
    "**Comment**: Compared to the k-d trees, the setup of `lookup` is $O(n)$, thus very fast, but only works in case the search tolerance is `fixed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Coding exercise [15 min]\n",
    "\n",
    "Given a list of $n$ words and another input word, find the number of anagrams of the input word there are in the list of words, ignoring case of the letters.\n",
    "\n",
    "Anagram is a word formed by rearranging the letters of a different word, e.g., `\"Santa\"` is an anagram of `\"Satan\"`.\n",
    "\n",
    "Importantly, the list of words is static (always the same), so you can do any kind of preprocessing of it, to speed up the queries.\n",
    "\n",
    "For instance:\n",
    "```python\n",
    "words = [\n",
    " \"AA\",\n",
    " \"Santa\",\n",
    " \"Satan\",\n",
    " \"a\",\n",
    " \"an\",\n",
    " \"ant\",\n",
    " \"antas\",\n",
    " \"ants\",\n",
    " \"as\",\n",
    " \"at\",\n",
    " \"sat\",\n",
    " \"ta\",\n",
    " \"tan\",\n",
    " \"tans\",\n",
    "]\n",
    "\n",
    "query_words = preprocess_words(words)\n",
    "\n",
    "count_anagrams(\"aa\", query_words)\n",
    "count_anagrams(\"Ant\", query_words)\n",
    "count_anagrams(\"rant\", query_words)\n",
    "count_anagrams(\"santa\", query_words)\n",
    "```\n",
    "should return, respectively:\n",
    "```\n",
    "1\n",
    "2\n",
    "0\n",
    "3\n",
    "```\n",
    "\n",
    "What is the time and space complexity of your solution when doing $k$ queries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def _fingerprint(word):\n",
    "    \"\"\"\n",
    "    returns fingerprint which is the same for anagrams and differs else\n",
    "    return value is also immutable and can be used as a key in a \n",
    "    dictionary. O(m log m) time and O(m) space for m = len(word).\n",
    "    \"\"\"   \n",
    "    return \"\".join(sorted(word.lower()))\n",
    "    \n",
    "def preprocess_words(words):\n",
    "    # O(n) time, O(n) space\n",
    "    return Counter(_fingerprint(word) for word in words)\n",
    "\n",
    "\n",
    "def count_anagrams(word, query_words):\n",
    "    # O(1) time, O(1) space\n",
    "    return query_words[_fingerprint(word)]\n",
    "\n",
    "# k queries take, with pre-processing:\n",
    "#    O(n + k) time, and O(n) space\n",
    "\n",
    "\n",
    "words = [\n",
    " \"AA\",\n",
    " \"Santa\",\n",
    " \"Satan\",\n",
    " \"a\",\n",
    " \"an\",\n",
    " \"ant\",\n",
    " \"antas\",\n",
    " \"ants\",\n",
    " \"as\",\n",
    " \"at\",\n",
    " \"sat\",\n",
    " \"ta\",\n",
    " \"tan\",\n",
    " \"tans\",\n",
    "]\n",
    "\n",
    "query_words = preprocess_words(words)\n",
    "\n",
    "print(count_anagrams(\"aa\", query_words))\n",
    "print(count_anagrams(\"Ant\", query_words))\n",
    "print(count_anagrams(\"rant\", query_words))\n",
    "print(count_anagrams(\"santa\", query_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Coding exercise [homework]\n",
    "\n",
    "Compute intersection of two lists of integers. Each element of the intersection must appear as many times as it shows in both arrays and order of elements in the results does not matter. For instance:\n",
    "```python\n",
    "ints1 = [1, 2, 2, 1, 3]\n",
    "ints2 = [2, 3, 2]\n",
    "\n",
    "intersect_ints(ints1, ints2)\n",
    "```\n",
    "can return either of:\n",
    "```python\n",
    "[2, 2, 3]\n",
    "[2, 3, 2]\n",
    "[3, 2, 2]\n",
    "```\n",
    "\n",
    "What is the time and space complexity of your solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "# Approach 1: sorting input to reduce problem to a known one - zip-merging of two sorted lists\n",
    "#             (but outputing only when equal)\n",
    "def intersect_ints_sort(ints1, ints2):\n",
    "    out = []\n",
    "    \n",
    "    # sorting: O(n log n), O(n) space (alt. O(1) space when allowed in-place)\n",
    "    ints1_sorted = sorted(ints1)  # alt. in-place: `ints1.sort()`\n",
    "    ints2_sorted = sorted(ints2)  # alt. in-place: `ints2.sort()`\n",
    "    \n",
    "    # \"merge\" sorted lists, appending to output only when ints are equal:\n",
    "    #     O(n_max) time => worst-case: all element of longer list are smaller\n",
    "    #     O(n_min) space (output) => worst-case: smaller list is a subset of longer list\n",
    "    n1, n2 = len(ints1), len(ints2)\n",
    "    i1, i2 = 0, 0\n",
    "    while i1 < n1 and i2 < n2:\n",
    "        num1, num2 = ints1_sorted[i1], ints2_sorted[i2]\n",
    "        if num1 < num2:\n",
    "            i1 += 1\n",
    "        elif num2 < num1:\n",
    "            i2 += 1\n",
    "        else: # equal\n",
    "            out.append(num1)\n",
    "            i1 += 1\n",
    "            i2 += 1\n",
    "\n",
    "    return out\n",
    "\n",
    "    # total: O(n_max log(n_max)) time, O(n_min) space\n",
    "    #\n",
    "    # Note: O(n_max) time, O(n_min) space if input lists are sorted\n",
    "    #       => same as more general counter-based solution\n",
    "\n",
    "\n",
    "# Approach 2: better, using a counter w/ a list size discrepancy optimisation\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def intersect_ints(ints1, ints2):\n",
    "    out = []\n",
    "    \n",
    "    # Ensure: n1 <= n2, where n1 = len(ints1), n2 = len(ints2)\n",
    "    # O(1)\n",
    "    ints_short, ints_long = (\n",
    "        (ints1, ints2) if len(ints1) < len(ints2)\n",
    "        else (ints2, ints1)\n",
    "    )\n",
    "    \n",
    "    # Build counter\n",
    "    # O(n1) time, O(n1) space\n",
    "    seen = Counter(ints_short)\n",
    "    \n",
    "    # Check bigger list for elements, updating counter\n",
    "    # O(n2) time, O(n1) space (output)\n",
    "    for num in ints_long:\n",
    "        if seen.get(num, 0) > 0:  # O(1)\n",
    "            out.append(num)  # average O(1); we could also pre-alloc n1 mem\n",
    "            seen[num] -= 1  # O(1)\n",
    "\n",
    "    return out\n",
    "\n",
    "    # total: O(n_max) time, O(n_min) space\n",
    "\n",
    "\n",
    "ints1 = [1, 2, 2, 1, 3]\n",
    "ints2 = [2, 3, 2]\n",
    "\n",
    "\n",
    "print(intersect_ints_sort(ints1, ints2))\n",
    "print(intersect_ints(ints1, ints2))\n",
    "\n",
    "# Note: difference in the algorithms complexity order kicks-in only with a sufficiently large input\n",
    "%timeit intersect_ints_sort(ints1*500, ints2*50)\n",
    "%timeit intersect_ints(ints1*500, ints2*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other data structures\n",
    "\n",
    "We've already mentioned some either Python-specific or more abstract data structures, like Python dictionaries (hash tables) or $k$-d tree. There are many more - cf. [Wikipedia's list of data structures](https://en.wikipedia.org/wiki/List_of_data_structures). We won't cover them here, but it's highly recommended to get familiar with at least few additional basic and common ones data structures:\n",
    "* [Stacks](https://en.wikipedia.org/wiki/Stack_(abstract_data_type)) and [queues](https://en.wikipedia.org/wiki/Queue_(abstract_data_type)),\n",
    "  * cf. [`collections.deque`](https://docs.python.org/3/library/collections.html#collections.deque),\n",
    "* [Trees](https://en.wikipedia.org/wiki/Tree_(data_structure)),\n",
    "  * ([self-balancing](https://en.wikipedia.org/wiki/Self-balancing_binary_search_tree)) [binary search tree](https://en.wikipedia.org/wiki/Binary_search_tree),\n",
    "  * [B-tree](https://en.wikipedia.org/wiki/B-tree),\n",
    "  * [heap](https://en.wikipedia.org/wiki/Heap_(data_structure)),\n",
    "* [Graphs](https://en.wikipedia.org/wiki/Graph_(abstract_data_type)) (adjacency list, adjacency matrix)\n",
    "  * cf. [NetworkX package](https://networkx.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other algorithms\n",
    "\n",
    "We recommend getting familiar with some general **[algorithm design paradigms](https://en.wikipedia.org/wiki/Algorithmic_paradigm)**, that help developing own algorithms; some important and common ones are:\n",
    "* **[Divide-and-conquer algorithms](https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm)**, where you split recursively your problem into independent smaller  problems,\n",
    "    * like Merge sort;\n",
    "* **[Dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming)**, where you also split your problem into same but smaller problems; the difference to divide-and-conquer is that your sub-problems overlap so you need to build-up the final solution,\n",
    "    * like computing the Fibonacci's number;\n",
    "* **[Backtracking](https://en.wikipedia.org/wiki/Backtracking)**, where you \"track\" history of your algorithm steps and return to a previous \"branching\" step (\"backtrack\") as soon as you find the there is no solution in the current branch,\n",
    "    * like in solving Sudoku by picking one (e.g. lowest) of the many remaining alternative numbers, or\n",
    "    * like in a [depth-first search in a graph](https://en.wikipedia.org/wiki/Depth-first_search).\n",
    "* ...\n",
    "\n",
    "The field of algorithms is vast, [wikipedia offers this overview](https://en.wikipedia.org/wiki/List_of_algorithms).\n",
    "\n",
    "Some fields to mention are:\n",
    "\n",
    "- [Graph algorithms](https://en.wikipedia.org/wiki/Travelling_salesman_problem). E.g. [topological sort](https://en.wikipedia.org/wiki/Topological_sorting) is used in workflow engines (e.g. [snakemake](https://snakemake.github.io)) to figure out the order of steps to execute based on their dependencies. The [Traveling salesman problem](https://en.wikipedia.org/wiki/Travelling_salesman_problem) also requires a graph, and the algorithms named [A*](https://en.wikipedia.org/wiki/A*_search_algorithm) and [D*](https://en.wikipedia.org/wiki/D*) have been widely used, resp., in games and in mobile robots and autonomous vehicle navigation. [Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm) finds the shorted path between nodes in a graph, e.g. in navigation systems.\n",
    "- [String search algorithms](https://en.wikipedia.org/wiki/String-searching_algorithm) to find substrings in a (long) string, e.g. the [Boyer-Moore algorithm](https://en.wikipedia.org/wiki/Boyer–Moore_string-search_algorithm)\n",
    "- [Approximate string matching algorithms](https://en.wikipedia.org/wiki/Approximate_string_matching) find strings that match a pattern approximately (rather than exactly) e.g. to compensate for typos.\n",
    "- [Sequence alignment](https://en.wikipedia.org/wiki/Sequence_alignment) algorithms, most known are the [Needleman-Wunsch](https://en.wikipedia.org/wiki/Needleman–Wunsch_algorithm) and [Swith-Waternam](https://en.wikipedia.org/wiki/Smith–Waterman_algorithm#Explanation) algorithms.\n",
    "- [Numerical algorithms](https://en.wikipedia.org/wiki/Numerical_analysis#Areas_of_study) for numerical computations such as curve fitting, solving linear equations, ...\n",
    "- [Optimization algorithms](https://en.wikipedia.org/wiki/Mathematical_optimization) (may overlap with numerical algorithms), e.g. [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent), the [Simplex algorithm](https://en.wikipedia.org/wiki/Simplex_algorithm) or the [BFGS method](https://en.wikipedia.org/wiki/Broyden–Fletcher–Goldfarb–Shanno_algorithm).\n",
    "- [Combinatorical optimisation algorithms](https://en.wikipedia.org/wiki/Combinatorial_optimization) to solve the [Knapsack](https://en.wikipedia.org/wiki/Knapsack_problem) or [Traveling salesman](https://en.wikipedia.org/wiki/Travelling_salesman_problem) problems. Many of these problems require $O(2^n)$ (**WHICH IS VERY VERY BAD**) operations and can be solved pragmatically if approximate solutions are acceptable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dive into some common techniques of reducing memory usage in scientific computing.\n",
    "\n",
    "It's gonna be convenient to measure memory use of a single object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Measuring memory consumption of an object**\n",
    "\n",
    "In addition to `memory-profile` (FIXME: add link) introduced in Section 2 one can measure the size of a specific object.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p><i class=\"fa fa-warning\"></i>&nbsp;\n",
    "       <strong>Beware</strong>:\n",
    "        The standard library offers a function <code>sys.getsizeof</code>, which you should avoid, whenever possible; see: <a href=\"https://nedbatchelder.com/blog/202002/sysgetsizeof_is_not_what_you_want.html\"><em><code>sys.getsizeof</code> is not what you want</em></a>.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "Instead, we can use the external package `pympler` to measure (approximate) size, in bytes, of an object in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler.asizeof import asizeof\n",
    "\n",
    "print(asizeof([]))\n",
    "print(asizeof(1))\n",
    "print(asizeof([1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse arrays "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **[sparse matrix or sparse array](https://en.wikipedia.org/wiki/Sparse_matrix)** is a matrix in which most of the elements are zero. The opposite of this is called a **dense matrix / array**.\n",
    "\n",
    "So instead of storing all entries of a matrix it can be more efficient to **store only the positions of the nonzero entries and the corresponding nonzero values**. E.g.\n",
    "\n",
    "`[0, 0, 0, 0, 1.25, 0, 3.4, 0]` could be represented as `[(4, 1.25), (6, 3.4)]`. Such representation is known as coordinate format (COO).\n",
    "\n",
    "<table>\n",
    "<tr><td><img src=\"imgs/sparse_matrix-coo.gif\" width=\"600px\"></td></tr>\n",
    "<tr><td><center><sub>Source: <a href=\"https://matteding.github.io/2019/04/25/sparse-matrices/\">https://matteding.github.io/2019/04/25/sparse-matrices/</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "This technique not only reduces memory usage but can speed up mathematical operations like matrix-vector, matrix-matrix of vector-vector products significantly. E.g. adding up the entries of a sparse vector is $O(n_*)$ with $n_*$ is the number of nonzero entries, compared to $O(n)$ for a dense vector.\n",
    "\n",
    "\n",
    "For some special sparse matrices also [very fast solvers for linear systems are known](https://en.wikipedia.org/wiki/Conjugate_gradient_method).\n",
    "\n",
    "Sparse matrices play a crucial role in the [numerical solution of partial-differential equations](https://en.wikipedia.org/wiki/Finite_element_method), which are used in many simulations e.g. weather forecasts or virtual crash tests. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse vectors and matrices in `scipy`*\n",
    "\n",
    "`scipy` offers [different storage formats for sparse matrices](https://docs.scipy.org/doc/scipy/reference/sparse.html), e.g. the [CSR (Compressed Sparse Row)](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format))  matrix format, which, in contrast to CCO, gives fast row access and fast matrix-vector multiplications.\n",
    "\n",
    "<table>\n",
    "<tr><td><img src=\"imgs/sparse_matrix-csr.gif\" width=\"800px\"></td></tr>\n",
    "<tr><td><center><sub>Source: <a href=\"https://matteding.github.io/2019/04/25/sparse-matrices/\">https://matteding.github.io/2019/04/25/sparse-matrices/</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "These formats can be used for vectors and matrices, but not for multi dimensional arrays with dimensions $\\gt$ 2.\n",
    "\n",
    "[scipy.sparse.linalg](https://docs.scipy.org/doc/scipy/reference/sparse.linalg.html#module-scipy.sparse.linalg) also offers special implementations of many common numerical algorithms optimized for sparse matrices, such as computing eigenvectors or solving linear systems.\n",
    "\n",
    "Here we showcase how to use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pympler.asizeof import asizeof\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# create a matrix with only few random nonzero entries\n",
    "np.random.seed(43)\n",
    "array = np.zeros((20, 20))\n",
    "for i in range(13):\n",
    "    row, col = np.random.randint(0, 20, size=(2,))\n",
    "    array[row, col] = 100.0 + i\n",
    "\n",
    "print()\n",
    "print(\"       numpy array size:\", asizeof(array))\n",
    "\n",
    "# convert matrix to a compressed sparse row (csr) matrix\n",
    "scipy_sparse_array = csr_matrix(array)\n",
    "print(\"scipy sparse array size:\", asizeof(scipy_sparse_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the code above constructs sparse matrix from a dense matrix, which still takes \"a lot\" of memory. The proper way to setup a sparse matrix from scratch is a bit more cumbersome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, rows, cols = [], [], []\n",
    "\n",
    "# create a compressed sparse row (csr) matrix with only few random nonzero entries\n",
    "# prepare a \"sparse\" list of rows, columns and nonzero values\n",
    "np.random.seed(43)\n",
    "for i in range(13):\n",
    "    row, col = np.random.randint(0, 20, size=(2,))\n",
    "    rows.append(row)\n",
    "    cols.append(col)\n",
    "    data.append(100.0 + i)\n",
    "\n",
    "# create the csr matrix from the list of nonzero entries\n",
    "scipy_sparse_array_2 = csr_matrix(\n",
    "    (np.array(data), (np.array(rows), np.array(cols))), shape=(20, 20)\n",
    ")\n",
    "\n",
    "print(\"scipy sparse array size:\", asizeof(scipy_sparse_array_2))\n",
    "\n",
    "# sanity check for equality\n",
    "# Note: it's cheap to check inequality of sparse matrices\n",
    "#       => check number of nonzero elements (nnz) in inequality sparse matrix\n",
    "print(\"scipy sparse arrays equal:\", (scipy_sparse_array != scipy_sparse_array_2).nnz == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: The random seed for construction of the entries in the previous example was carefully chosen to avoid duplicate entries in `rows`, `cols`; otherwise, duplicate entries values are added up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place number 1, then number 2 at position (0, 0)\n",
    "data = [1, 2]\n",
    "rows = [0, 0]\n",
    "cols = [0, 0]\n",
    "print(\n",
    "    csr_matrix(\n",
    "        (np.array(data), (np.array(rows), np.array(cols))), shape=(2, 2)\n",
    "    ).todense()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scipy` offers conversion between different sparse formats and from and to dense arrays. Sparse arrays can be used in the same way as dense arrays. \n",
    "\n",
    "`scikit-learn` [also supports sparse matrices to represent features](https://scikit-learn.org/stable/modules/feature_extraction.html#sparsity). E.g. encoding texts using [vector space models](https://en.wikipedia.org/wiki/Vector_space_model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse vectors in `pandas`*\n",
    "\n",
    "**Warning**: `pandas` support [sparse vectors (1-dimensional `SparseArray`)](https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#sparsearray), but not sparse matrices.\n",
    "\n",
    "Contrary to `scipy.sparse` arrays, the \"common value\", also called a \"fill value\", is not required to be `0`, but can be any arbitrary value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pympler.asizeof import asizeof\n",
    "\n",
    "vector = np.zeros((1000,))\n",
    "nnz = 40  # number non-zero\n",
    "\n",
    "vector[np.random.randint(len(vector), size=nnz)] = 42\n",
    "print(vector[:100])\n",
    "print()\n",
    "print(\"numpy vector size:\", asizeof(vector))\n",
    "\n",
    "pandas_sparse_vector = pd.arrays.SparseArray(vector, fill_value=0)  # \n",
    "print(\"pandas sparse vector size:\", asizeof(pandas_sparse_vector))\n",
    "print()\n",
    "print(pandas_sparse_vector[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `pandas` only supports sparse vectors, the internal format is also more space efficient compared to `scipy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "scipy_sparse_vector = csr_matrix(vector)\n",
    "print(\"scipy sparse vector size:\", asizeof(scipy_sparse_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas.DataFrame` can store any column as `pandas.arrays.SparseArray`, in particular all columns if you work on matrices\n",
    "\n",
    "**BUT**\n",
    "\n",
    "there is an overhead for storing each sparse data structure, so prefer long columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "numpy_dense_vector = pandas_sparse_vector.to_dense()\n",
    "\n",
    "for n_col in (10, 100):\n",
    "\n",
    "    numpy_dense_matrix = numpy_dense_vector.reshape(numpy_dense_vector.size // n_col, n_col)\n",
    "    print(\"                matrix shape:\", numpy_dense_matrix.shape)\n",
    "    scipy_sparse_matrix = csc_matrix(numpy_dense_matrix)\n",
    "    print(\"    scipy sparse matrix size:\", asizeof(scipy_sparse_matrix))\n",
    "\n",
    "    pandas_df = pd.DataFrame(numpy_dense_matrix)\n",
    "    print(\" pandas dense dataframe size:\", asizeof(pandas_df))\n",
    "\n",
    "    pandas_sparse_df = pd.DataFrame.sparse.from_spmatrix(scipy_sparse_matrix)\n",
    "    print(\"pandas sparse dataframe size:\", asizeof(pandas_sparse_df))\n",
    "    print(pandas_sparse_df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other options*\n",
    "\n",
    "The Python package **[`sparse`](https://sparse.pydata.org/en/latest/)** supports sparse data containers which can be used like `numpy` arrays and supports **arbitrary dimensions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appropriate numerical data type in NumPy\n",
    "\n",
    "We will learn more about `numpy` in the next script, but we anticipate that `numpy` arrays consume much less memory than Python lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_el, n_min, n_max = 500, -1000, 1000\n",
    "\n",
    "import numpy as np\n",
    "from pympler.asizeof import asizeof\n",
    "\n",
    "\n",
    "print(f\"0...{n_max} sequence container size:\")\n",
    "\n",
    "python_list = list(np.random.randint(low=n_min, high=n_max, size=n_el))\n",
    "print(\"* list:\\t\\t\\t\", asizeof(python_list))\n",
    "\n",
    "np_array_int = np.array(python_list, dtype=int)\n",
    "print(f\"* numpy {np_array_int.dtype} array:\\t\", asizeof(np_array_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy supports a much greater variety of numerical types than Python does. You can reduce the amount of memory by using these types.\n",
    "\n",
    "In our previous example the default `int` type takes 8 bytes (64 bit), whereas absolute values of our numbers are $\\leq$ 1000 and thus can be represented using 11 bytes ($2^{10} = 1024$, plus a byte for a sign). The smallest suitable [`numpy` data type](https://numpy.org/doc/stable/user/basics.types.html) is `np.short`, which is an alias for `np.int16`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array_short = np.array(python_list, dtype=np.short)\n",
    "print(f\"* numpy {np_array_short.dtype} array:\\t\", asizeof(np_array_short))\n",
    "\n",
    "# Sanity check: we did not \"cut\" any number value\n",
    "print(all(np_array_int == np_array_short))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reduced allocated memory already by a factor or ~4.\n",
    "\n",
    "Note: when dealing with positive numbers, you can use an unsigned variant of the numeric type (e.g. `np.ushort`/`np.uint16`); unsigned types do not use less bytes, but they do increase the maximal value that can be stored (twice, as the \"leading\" sign byte is used as a binary number digit instead).\n",
    "\n",
    "Use `np.iinfo` function to check min/max value of a NumPy integer type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.iinfo(np.int64))\n",
    "print(np.iinfo(np.int16))\n",
    "print(np.iinfo(np.uint16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default floating-point type in `numpy` is `numpy.double` (`numpy.float64`), thus such a number consumes 64 bit or 8 bytes.\n",
    "\n",
    "Choosing `numpy.single` (`numpy.float32`) instead leads to 50% memory reduction **at the cost of reduced accuracy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few words about floating-point accuracy\n",
    "\n",
    "To store real numbers on a computer with finite memory (infinite memory not invented as of 2021), real numbers need to be approximated using so called [floating-point numbers](https://docs.python.org/3/tutorial/floatingpoint.html). See also [this article from Python documentation](https://docs.python.org/3/tutorial/floatingpoint.html) and [this Wikipedia article](https://en.wikipedia.org/wiki/Floating-point_arithmetic).\n",
    "\n",
    "You can see that e.g. $\\pi$ is approximated, we know that $\\sin(\\pi) = 0$, but:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.pi)\n",
    "# print with 32 digits after decimal point:\n",
    "print(\"%.32f\" % np.sin(np.pi))\n",
    "# print first significant digit in the scientific notation\n",
    "print(\"%.1g\" % np.sin(np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number $0.0000000000000001...$ differs from $0$ only at 16 position after the decimal place, but it is not exactly $0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sin(np.pi) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p>\n",
    "        <i class=\"fa fa-warning\"></i>&nbsp<strong>Beware</strong>:\n",
    "        Real numbers computations on computers are <strong>approximate computations</strong>.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "Approximate computations means propagating errors, which, in turn, may lead to unexpected behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0.0\n",
    "while x != 1:\n",
    "    x += 1/7\n",
    "    print(x)\n",
    "    if x > 1:  # too far\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([A real-life consequences of floating-point arithmetic errors might be quite serious](https://en.wikipedia.org/wiki/Floating-point_arithmetic#Incidents).)\n",
    "\n",
    "Let's call $rd(x)$ the representation of a real number $x$ in a given floating-point format. We define a [**machine epsilon**](https://en.wikipedia.org/wiki/Machine_epsilon) $\\epsilon$ as the **maximum relative error of a real number representation**, i.e.:\n",
    "<br/><br/>\n",
    "$$\n",
    "\\epsilon = \\max_x \\frac{|\\mathrm{rd}(x) - x|}{|x|}\n",
    "$$\n",
    "\n",
    "The more bytes a floating-point type uses the better the representation precision.\n",
    "\n",
    "With a **a floating-point type precision** or **trusted digits** we refer to the number of digits that we can trust to be correctly represented (starting with the first non-zero/significant digit); this is $\\left\\lfloor -\\log_{10}{\\epsilon} \\right\\rfloor$.\n",
    "\n",
    "Using the `np.finfo` function to check info on a NumPy floating point type, including machine epsilon, precision, or min/max value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.finfo(np.double))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table summarizes the most common NumPy floating-point types and their main properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "idouble = np.finfo(np.double)\n",
    "isingle = np.finfo(np.single)\n",
    "ihalf = np.finfo(np.half)\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "| name   | `np.ndtype`         | trusted digits      | $\\epsilon$        | max / -min        |\n",
    "| --     | --                  | --                  | --                | --                |\n",
    "| double | `{idouble.dtype!s}` | {idouble.precision} | {idouble.eps:.2e} | {idouble.max:.2e} |\n",
    "| single | `{isingle.dtype!s}` | {isingle.precision} | {isingle.eps:.2e} | {isingle.max:.2e} |\n",
    "| half   | `{ihalf.dtype!s}`   | {ihalf.precision}   | {ihalf.eps:.2e}   | {ihalf.max:.2e}   |\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in practice, you may need a bit more digits to encounter directly the representation issue, depending on the specific number values, e.g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1234567890123456.1\n",
    "b = 1234567890123456  # difference on 17-th digit\n",
    "print(f\"{a}=={b}:\", a == b)\n",
    "print()\n",
    "print(f\"np.float64({a}) == np.float64({b}):\", np.float64(a) == np.float64(b))  # same as built-in `float`\n",
    "print()\n",
    "\n",
    "a = 12345678.1\n",
    "b = 12345678  # difference on 9-th digit\n",
    "print(f\"np.float32({a}) == np.float32({b}):\", np.float32(a) == np.float32(b))  \n",
    "print()\n",
    "\n",
    "a = 1234.1\n",
    "b = 1234  # difference on 5-th digit\n",
    "print(f\"np.float16({a}) == np.float16({b}):\", np.float16(a) == np.float16(b)) \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save space by using an appropriate data type, but **be aware of the resulting limitations of numerical range and precision**.\n",
    "\n",
    "So what to do in cases, where you consciously accept the chosen floating-point type limitations, but still want to compare for numbers equality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p>\n",
    "        <i class=\"fa fa-warning\"></i>&nbsp<strong style=\"font-size:120%;\">Important</strong>\n",
    "    </p>\n",
    "    <p>\n",
    "        <strong>You must NOT compare floating-point numbers exactly</strong>, i.e. using equality operator <code>==</code>.\n",
    "    </p>\n",
    "    <p>\n",
    "        Instead, check for floating-point numbers closeness in terms of their relative or absolute difference using functions such as <a href=\"https://docs.python.org/dev/library/math.html#math.isclose\">math.isclose</a>, <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.isclose.html#numpy.isclose\">numpy.isclose</a>, or <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.allclose.html#numpy.allclose\">numpy.allclose</a>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sin(np.pi) == 0.0)\n",
    "print(np.isclose(np.sin(np.pi), 0.0, atol=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = 0.0\n",
    "while not np.isclose(x, 1, atol=0):\n",
    "    x += 1/7\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External memory data: example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**External memory data processing** or **out of memory processing** is done when data are too large to fit into a computer's main memory; most often, data is kept on a disk and read in blocks fitting main memory.\n",
    "\n",
    "E.g. given a huge text file with a fixed number of numbers per line, we can determine the largest number without loading all values into memory.\n",
    "\n",
    "Let's fabricate a dummy input file first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "N = 50_000  # nr of lines\n",
    "M = 50  # nr of columns\n",
    "\n",
    "with open(\"numbers.csv\", \"w\") as fh:\n",
    "    for i in range(N):\n",
    "        np.savetxt(fh, np.random.rand(1, M) * M, delimiter = \",\", fmt=\"%18.16f\")\n",
    "\n",
    "# file size\n",
    "!du -h numbers.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the  largest number it is sufficient to inspect line per line:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum = None\n",
    "\n",
    "with open(\"numbers.csv\") as fh:\n",
    "    for line in fh:\n",
    "        number = max(float(n) for n in line.split(\",\"))\n",
    "        if maximum is None or number > maximum:\n",
    "            maximum = number\n",
    "\n",
    "print(\"largest number is\", maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` supports chunk-wise reading CSV or similar data files. Chunks are `pandas.DataFrame` objects and a chunk size determines numbers of lines to read at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for chunk in pd.read_csv(\"numbers.csv\", header=None, chunksize=5):\n",
    "    print(chunk.iloc[:,:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used to determine the largest number by reading chunks of `10_000` lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum = None\n",
    "\n",
    "for chunk in pd.read_csv(\"numbers.csv\", header=None, chunksize=10_000):\n",
    "    max_in_chunk = float(chunk.max().max())\n",
    "    if maximum is None or max_in_chunk > maximum:\n",
    "        maximum = max_in_chunk\n",
    "\n",
    "print(\"largest number is\", maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember**: external memory data processing is suitable for data that does not fit memory; it's faster to process data in-memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading file multiple times: example*\n",
    "\n",
    "Sometimes you might have to run multiple times over the same file to process external memory data.\n",
    "\n",
    "E.g. to scale a file of numbers to the range 0..1 you \n",
    "   1. First iterate over the data to determine the minimal $x_\\min$ and maximal value $x_\\max$.\n",
    "   2. During a second iteration you replace every $x$ by $\\frac{x - x_\\min}{x_\\max - x_\\min}$  and write this number to an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "minimum = maximum = None\n",
    "\n",
    "with open(\"numbers.csv\") as fh:\n",
    "    for line in fh:\n",
    "        for n in line.split(\",\"):\n",
    "            number = float(n)\n",
    "            if maximum is None or number > maximum:\n",
    "                maximum = number\n",
    "            if minimum is None or number < minimum:\n",
    "                minimum = number\n",
    "\n",
    "print(\"minimum\", minimum)\n",
    "print(\"maximum\", maximum)\n",
    "\n",
    "with open(\"numbers.csv\") as fh_in:\n",
    "    with open(\"numbers_scaled.csv\", \"w\") as fh_out:\n",
    "        for line in fh_in:\n",
    "            line_array = np.array([float(n) for n in line.split(\",\")]).reshape(1, M)\n",
    "            scaled = (line_array - minimum) / (maximum - minimum)\n",
    "            np.savetxt(fh_out, scaled, delimiter = \",\", fmt=\"%18.16f\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"rescaling done in {elapsed_time} sec\")\n",
    "\n",
    "print(next(pd.read_csv(\"numbers_scaled.csv\", header=None, chunksize=5)).iloc[:,:5])\n",
    "!du numbers_scaled.csv\n",
    "!rm numbers_scaled.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `pandas` chunk-based variant uses a `.to_csv()` method for writing, with a keyword arguments:\n",
    "* `mode` (Python write mode), which when set to `\"a\"` appends results to a given file name (or a file handle);\n",
    "* `index` which when set to `None` prevents printing row names (which by default are row numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "n_chunks = 5  # try different nr of chunks\n",
    "chunksize = N / n_chunks\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "minimum = maximum = None\n",
    "\n",
    "for chunk in pd.read_csv(\"numbers.csv\", header=None, chunksize=chunksize):\n",
    "    max_in_chunk = float(chunk.max().max())\n",
    "    min_in_chunk = float(chunk.min().min())\n",
    "    if maximum is None or max_in_chunk > maximum:\n",
    "        maximum = max_in_chunk\n",
    "    if minimum is None or min_in_chunk < minimum:\n",
    "        minimum = min_in_chunk\n",
    "\n",
    "print(\"minimum\", minimum)\n",
    "print(\"maximum\", maximum)\n",
    "\n",
    "for chunk in pd.read_csv(\"numbers.csv\", header=None, chunksize=chunksize):\n",
    "    scaled_chunk = (chunk - minimum) / (maximum - minimum)\n",
    "    scaled_chunk.to_csv(\"numbers_scaled_pandas.csv\", header=None, index=None, mode=\"a\", float_format=\"%18.16f\")\n",
    "\n",
    "    # Note: you could keep the file handle open for all writes,\n",
    "    #       but time to get a file handle is negligible as compared to I/O time\n",
    "    # with open(\"numbers_scaled_pandas.csv\", \"a\") as fh_out:\n",
    "    #     chunk.to_csv(fh_out, ...)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"rescaling done in {elapsed_time} sec\")\n",
    "\n",
    "print(next(pd.read_csv(\"numbers_scaled_pandas.csv\", header=None, chunksize=5)).iloc[:,:5])\n",
    "!du numbers_scaled_pandas.csv\n",
    "!rm numbers_scaled_pandas.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* You should aim at using as much as memory as possible (but leaving some operational margin).\n",
    "* Python's `open()` does already buffer I/O reads and writes to minimize number of I/O operations.\n",
    "* The `pandas` chunks functionality is very convenient, but it does not really offer a speed-up over the plain I/O interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Relation to online algorithms*\n",
    "\n",
    "**Online algorithm** is one that can process its input piece-by-piece in a serial fashion; the entire input is not available from the start, and algorithm takes action as soon as new data pieces arrive (in contrast to so called *streaming algorithms*).\n",
    "\n",
    "**Offline algorithm** is given the whole problem data from the start (and is required to output an answer which solves the whole problem).\n",
    "\n",
    "Online algorithms are considered for real-time/continuously provided data, for instance, for data that are sequentially read from a disk. On the other hand, external memory data processing, in general, considers use of external memory also in parallel, in some structured way, if needed (e.g. queries to databases).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory mapped files\n",
    "\n",
    "A convenient technique to keep data on disk, but still working with the data **as if it is stored in memory** are [memory mapped files](https://en.wikipedia.org/wiki/Memory-mapped_file).\n",
    "\n",
    "`numpy` uses [Python `mmap` standard library module](https://docs.python.org/3/library/mmap.html) to support memory mapped files to support work with an array data stored on a disk.\n",
    "\n",
    "Let's create a large matrix array on a disk in the [NumPy array's binary format `.npy`](https://numpy.org/doc/stable/reference/generated/numpy.lib.format.html#module-numpy.lib.format):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.format import open_memmap\n",
    "\n",
    "out_of_mem_array = open_memmap(\n",
    "    \"large_file.npy\",\n",
    "    mode=\"w+\",\n",
    "    dtype=np.float64,\n",
    "    shape=(10_000, 10_000),\n",
    ")\n",
    "\n",
    "print(type(out_of_mem_array))\n",
    "\n",
    "!ls -lh large_file.npy\n",
    "\n",
    "# alternatively, ask the underlying mmap object for the memory-mapped file size:\n",
    "print(type(out_of_mem_array._mmap))\n",
    "print(f\"{out_of_mem_array._mmap.size() / (1024 * 1024):.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `10_000 x 10_000` matrix occupies `763M` on hard drive but consumes almost no RAM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"     virtual matrix size: {out_of_mem_array.nbytes / (1024 * 1024):.2f}M\")\n",
    "# pympler.asizeof does not work correctly w/ numpy.memmap objects => using __sizeof__ built-in\n",
    "print(f\"numpy.memmap object size: {out_of_mem_array.__sizeof__()} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: measuring size of `numpy.memmap` objects is not useful - there is no direct way to monitor how much of the underlying file is actually mapped into the memory.\n",
    "\n",
    "We did not initialize any matrix values. By default, it contains only zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beware: just for a demonstration purpose - np.all triggers mapping of\n",
    "#          the whole array to the memory, which misses the whole point\n",
    "np.all(out_of_mem_array == 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this like a regular matrix, but operations will be slowed down due to I/O overhead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_mem_array[9, 9] = 1.234\n",
    "\n",
    "# stay on the safe side: flush changes to disk after done with writing\n",
    "out_of_mem_array.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us open the file in a read-only mode now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_mem_array = open_memmap(\n",
    "    filename=\"large_file.npy\",\n",
    "    mode=\"r\",\n",
    ")\n",
    "print(type(out_of_mem_array))\n",
    "print(out_of_mem_array.dtype)\n",
    "print(out_of_mem_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* we can equivalently use `np.load(..., mmap_mode=...)`;\n",
    "* using `np.memmap` will save only binary-serialized array, without array metadata header (`shape`, `dtype`) - both constitute the `.npy` binary format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can map chunks of array file into RAM and create regular NumPy array out of it, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_mem_array = np.array(out_of_mem_array[:10, :10])\n",
    "print(type(in_mem_array))\n",
    "print(in_mem_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Side note: in general, for an efficient random access to any type of files, instead of reading file chunks into a buffer, you can use the `mmap` standard library module, getting addressing/indexing-based access to file contents (the `mmap` module is also very efficient when reading whole files into the memory; cf. [`mmap` tutorial](https://realpython.com/python-mmap/)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other file formats\n",
    "\n",
    "- **[HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format)** is a general data format designed to store huge amounts of numerical data as a dictionary of `numpy` arrays.\n",
    "    * The [`h5py` package](https://docs.h5py.org/en/stable/index.html) can be used to work with HDF5 files in Python; [cf. with the `pytables` package for some more advanced HDF5 features](http://www.pytables.org/FAQ.html#how-does-pytables-compare-with-the-h5py-project).\n",
    "- **[Apache Parquet](https://en.wikipedia.org/wiki/Apache_Parquet)** is another efficient column/data frame/table-oriented file format\n",
    "    * Use [Parquet files in Python via the `pyarrow` package](https://arrow.apache.org/docs/python/parquet.html), which integrates with both `numpy` and `pandas`.\n",
    "- **[SQLite](https://www.sqlite.org/index.html)** offers a SQL database in a single file. SQL database enables efficient storage, extraction and transformation of subsets of data from many, possibly related, large tables/datasets.\n",
    "    * The [`sqlite3` standard library module](https://docs.python.org/3/library/sqlite3.html) allows to work with SQLite in Python.\n",
    "    * A [built-in `pandas` support for SQL](https://towardsdatascience.com/python-pandas-and-sqlite-a0e2c052456f) can be used to work with SQL tables as data frames; read *[Fast subsets of large datasets with Pandas and SQLite](https://pythonspeed.com/articles/indexing-pandas-sqlite/)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python generator expressions](https://docs.python.org/3/howto/functional.html#generator-expressions-and-list-comprehensions) can produce a stream of data items, fetching items on demand.\n",
    "\n",
    "Generator expressions look similar to list comprehensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct full list of first 1000 square numbers\n",
    "direct = [i * i for i in range(1000)]\n",
    "\n",
    "# \"lazy list\" of first 1000 square numbers\n",
    "lazy = (i * i for i in range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both behave similar in many situations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(direct))\n",
    "print(sum(lazy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BUT**\n",
    "\n",
    "Generator expressions create generators and generators **don't store objects in memory**. Instead, generators **compute values on demand**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler.asizeof import asizeof\n",
    "\n",
    "direct = [i * i for i in range(1000)]\n",
    "lazy = (i * i for i in range(1000))\n",
    "\n",
    "print(f\"direct: {asizeof(direct)} bytes, {type(direct)}\")\n",
    "print(f\"  lazy: {asizeof(lazy)} bytes, {type(lazy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all iterators, generators **can be exhausted**, but you can **iterate through a generator only once**: \n",
    "\n",
    "Once all generator items have been computed, iteration will stop and a loop over elements will exit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(lazy))\n",
    "print(sum(lazy))\n",
    "print(list(lazy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: use `next()` to get an explicit `StopIteration` exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "next(lazy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In turn, **generators don't support operations such as `len` or indexing/slicing using `[...]`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement more flexible generators, use functions with the `yield` keyword. This is beyond this course - read [Python documnetation HOWTO on generators](https://docs.python.org/3/howto/functional.html#generators) to learn more about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reads\n",
    "\n",
    "- *[Reducing Pandas memory usage #1: lossless compression](https://pythonspeed.com/articles/pandas-load-less-data/)*\n",
    "- *[Reducing Pandas memory usage #2: lossy compression](https://pythonspeed.com/articles/pandas-reduce-memory-lossy/)*\n",
    "- ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
