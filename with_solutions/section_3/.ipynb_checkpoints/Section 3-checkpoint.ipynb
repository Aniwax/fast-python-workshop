{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 2.5em; font-weight: bold;\">Section 3: Concepts</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous Section you've learned how to identify what is making your Python program run slow.\n",
    "\n",
    "Now it's turn to learn about common approaches to make your Python program run faster.\n",
    "\n",
    "We will start with an overview of various code optimization strategies, in order of application preference. Afterwards, we will dive into some of few basic code optimization concepts, leaving the more conceptually and technically advanced ones for the next Sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Avoid unnecessary computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not running computations saves runtime.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/captain_obvious.gif\" width=\"350px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://indepest.com/2021/03/14/captain-obvious/\">https://indepest.com/2021/03/14/captain-obvious/</a></sub></center></td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<p><i class=\"fa fa-info-circle\"></i>&nbsp You can avoid running unnecessary computations, for instance, by:</p>\n",
    "<ol>\n",
    "    <li><strong>input data pre-processing</strong> e.g. to read or to transform data, or to pre-compute useful values, using an extra data structures if needed,\n",
    "        <ul>\n",
    "        <li>falls into an art of using better algorithms and data structures</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>pre-allocating memory</strong>, to avoid copying in-memory data e.g. in a dynamic array growing in each loop iteration,</li>\n",
    "    <li><strong>saving (caching/&quot;memoizing&quot;) results of repeating computations</strong>, to avoid running multiple long-running function calls with the same input arguments.</li>\n",
    "</ol>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching and memoization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cache** is a hardware or software component that stores data so that future requests for that data can be served faster. The data stored in a cache might be the result of an earlier computation or a copy of data stored elsewhere. \n",
    "\n",
    "**Memoization** is an optimization technique that stores the results of resource-consuming function calls within a lookup table, allowing subsequent calls to reuse the stored results and avoid repeated computation. Memoization is a special case of caching.\n",
    "\n",
    "The basic memoization application to a function looks as follows:\n",
    "\n",
    "0. Set up a cache data structure for function results\n",
    "1. Every time the function is called with input arguments do one of the following:\n",
    "   * A) return the cached result, if any is available for given input arguments; OR\n",
    "   * B) compute the missing result, and update the cache before returning the result.\n",
    "\n",
    "    <table>\n",
    "        <tr><td><img src=\"imgs/memoization.png\" width=\"800px\"></td></tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you use as a cache?\n",
    "\n",
    "* Memory:\n",
    "    * variables\n",
    "    * arrays\n",
    "    * dictionaries\n",
    "* Data stores:\n",
    "   * disk files\n",
    "   * database\n",
    "   * cloud storage\n",
    "   \n",
    "You will find already implemented caches; in Python, to memoize function results use:\n",
    "\n",
    "* `functools.lru_cache` for memory-based caching, or\n",
    "* `joblib.Memory` for file-based caching.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use better algorithms and data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithmic improvements can have the biggest benefit in improving the runtime due to reducing time (or space) complexity;\n",
    "\n",
    "- Buying a faster expensive CPU/GPU may speedup your computations ca. $5$ to $10$ times;\n",
    "- Running in parallel on a high performance computer may speedup your computations ca. $50$ to $100$ times, using a very expensive hardware;\n",
    "\n",
    "**BUT**\n",
    "\n",
    "- Using a better algorithm and reducing number of operations, for instance, from $2 n^3$ to $10 n^2$ for input data of size $n = 10.000$, speeds up your code $\\frac{2\\cdot 10^{12}}{10^{9}} = 1000$ times, without any additional hardware costs.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/algorithms_and_data_structures.png\" width=\"800px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://dev.to/snj/how-to-learn-data-structures-and-algorithms-an-ultimate-guide-for-beginners-2h9c\">https://dev.to/snj/how-to-learn-data-structures-and-algorithms-an-ultimate-guide-for-beginners-2h9c</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<p><i class=\"fa fa-info-circle\"></i>&nbsp For the time-consuming parts of your scientific code you can:</p>\n",
    "\n",
    "<ol>\n",
    "    <li><strong>Pick an existing better algorithm</strong> - requires some knowledge in computer science (in particular, lingo for abstract problem formulation).</li>\n",
    "    <li><strong>Develop a new better algorithm</strong> - more challenging as it requires experience/practice specifically in algorithms development (many online training sites available).</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Complexity of an algorithm is closely related to the data structures used by it</strong>. Data structures allow to simplify or abstract-out parts of the problem being solved, like memoization of intermediate computations or search for elements within the data.</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making use of available fast memory speeds up the computations.\n",
    "\n",
    "**BUT**\n",
    "\n",
    "As already briefly discussed in the first Section, a **too high memory usage can negatively impact runtime** either by:\n",
    "\n",
    "* inefficient cache usage within the CPU, or by\n",
    "* memory swapping (paging) using a disk.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/memory_usage_vs_speed.jpg\" width=\"500px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://www.techpowerup.com/234514/firefox-54-released-multi-process-optimized-memory-footprint\">https://www.techpowerup.com/234514/firefox-54-released-multi-process-optimized-memory-footprint</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "**In case of CPU-intensive computations, you should avoid memory swapping at all cost.**\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<p><i class=\"fa fa-info-circle\"></i>&nbsp Memory usage can be reduced using various techniques, among others:</p>\n",
    "\n",
    "<ol>\n",
    "    <li><strong>using memory-efficient data structures</strong>, such as sparse arrays for numerical computations,</li>\n",
    "    <li><strong>using appropriate memory-efficient basic data types</strong>, such as fixed length strings, big-enough integer numbers, or lower precision floating point numbers,</li>\n",
    "    <li><strong>explicitly using disk memory*</strong> for data that does not fit into the main memory.</li>\n",
    "</ol>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory swapping problem*\n",
    "\n",
    "We introduced the concept of swapping in the first Section and it is important to understand that swapping can slow down program execution significantly.\n",
    "\n",
    "Swap space is a space on a disk which is a substitute of a fast main memory (RAM). Whenever an operating system runs short of main memory it swaps parts of the main memory (pages) with disk contents, i.e.:\n",
    "\n",
    "1. swap out: move currently unused main memory parts (pages) to a swap space on a disk, and make the free main memory available to the currently running program;\n",
    "1. swap in: read back from the swap space on a disk memory parts (pages) into the main memory, making it available to the currently running program.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/swapping.jpg\" width=\"400px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/9_VirtualMemory.html\">https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/9_VirtualMemory.html</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Swapping helps operating system to manage programs that use more memory than there actually is available (avoiding crashing applications instead).\n",
    "\n",
    "**BUT**\n",
    "\n",
    "Reading from a disk memory is slow. If swapping happens too often, especially in case **when one program uses more then available main memory, the program is mostly waiting for the operating system to swap the data** (is [\"swapping to death\"](https://en.wikipedia.org/wiki/Memory_paging#Swap_death)).\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use faster languages, like C/C++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that statically typed programming languages--ones where one declares types of variables, function arguments, or return values--can be much faster than dynamically typed languages, such as Python, at a cost of pre-required compilation to a machine code.\n",
    "\n",
    "Using fast languages for a project usually comes at cost of bigger program size and longer development-time.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/the_computer_language_benchmarks_game-time_vs_size.jpg\" width=\"500px\"><img src=\"imgs/the_computer_language_benchmarks_game-time_vs_size-zoom.jpg\" width=\"500px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://twitter.com/ChapelLanguage/status/921074120191655937/\">https://twitter.com/ChapelLanguage/status/921074120191655937/</a></sub></center></td></tr>\n",
    "    <tr><td><center><sub>Source<sup>2</sup>: <a href=\"https://benchmarksgame-team.pages.debian.net/benchmarksgame/\">https://benchmarksgame-team.pages.debian.net/benchmarksgame/</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "**BUT**\n",
    "\n",
    "You can write your project in Python and still delegate a lot of (crucial) computations to actually run as C/C++, without a significant program size or development time overhead.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<p><i class=\"fa fa-info-circle\"></i>&nbsp For your own Python programs you can employ C/C++ by:</p>\n",
    "\n",
    "<ol>\n",
    "    <li><strong>preferring vectorized C/C++-level routines</strong> using <a href=\"https://numpy.org/\">NumPy</a> arrays\n",
    "        (or <a href=\"https://pandas.pydata.org/\">Pandas</a> data frames) over slow Python <code>for</code> loops over\n",
    "        elements/rows/columns;<ul>\n",
    "            <li>Note: use libraries that build on top of NumPy or Pandas, like <a href=\"https://scipy.org/\">SciPy </a>\n",
    "                or <a href=\"https://scikit-learn.org\">Scikit-learn</a>.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>compiling your Python code to C/C++</strong> using tools such as <a\n",
    "            href=\"https://numba.pydata.org/\">Numba</a>, <a href=\"https://www.pypy.org/\">PyPy</a>, or <a\n",
    "            href=\"https://pythran.readthedocs.io\">Pythran</a>;</li>\n",
    "    <li><strong>&quot;<a href=\"https://en.wikipedia.org/wiki/Language_binding\">binding</a>&quot; to Python your own\n",
    "            C/C++ code</strong> using <a href=\"https://cython.org/\">Cython</a>\n",
    "        <ul>\n",
    "            <li>Note: while C/C++ is a convenient default for binding fast crucial routines and provides various options\n",
    "                for Python, you can call routines virtually from any other well-established fast languages, like, for\n",
    "                instance Fortran via <a href=\"https://numpy.org/doc/stable/f2py/\">F2PY</a>, Java via <a\n",
    "                    href=\"https://jpype.readthedocs.io/en/latest/\">JPype</a>, Rust via <a\n",
    "                    href=\"https://pyo3.rs\">PyO3</a> etc.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "The Section on code optimization will introduce in detail these approaches and tools.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run code in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principle is simple: split work among multiple workers to reduce the total runtime.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/serial_vs_parallel.png\" width=\"500px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://www.teldat.com/blog/parallel-computing-bit-instruction-task-level-parallelism-multicore-computers/\">https://www.teldat.com/blog/parallel-computing-bit-instruction-task-level-parallelism-multicore-computers/</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<p><i class=\"fa fa-info-circle\"></i>&nbsp Parallelization in computing has many different levels; from the lowest level:</p>\n",
    "\n",
    "<ol>\n",
    "    <li>\n",
    "        <p><a href=\"https://en.wikipedia.org/wiki/Bit-level_parallelism\">bit-level parallelism</a> - processor operating\n",
    "            on whole &quot;chunks&quot; of bits (32-bit, 64-bit);</p>\n",
    "    </li>\n",
    "    <li>\n",
    "        <p><a href=\"https://en.wikipedia.org/wiki/Instruction-level_parallelism\">instruction-level parallelism</a> -\n",
    "            simultaneous execution of multiple processor instructions, which are optimized first by hardware or\n",
    "            compilers for a maximizing average number of instructions run per CPU clock cycle (a 3.0GHz CPU performs 3\n",
    "            million clock cycles per second);</p>\n",
    "    </li>\n",
    "    <li><a href=\"https://en.wikipedia.org/wiki/Data_parallelism\"><strong>data parallelism</strong></a> - distributing\n",
    "        the data across multiple threads/processors, which operate on the data in parallel, e.g.,<ul>\n",
    "            <li><a\n",
    "                    href=\"https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm#Parallel_and_distributed_algorithms\">parallel\n",
    "                    and distributed large matrix multiplication</a></li>\n",
    "            <li><a href=\"https://en.wikipedia.org/wiki/MapReduce\">MapReduce programming model</a>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <td><img src=\"imgs/mapreduce.png\" width=\"900px\"></td>\n",
    "                    </tr>\n",
    "                    <tr>\n",
    "                        <td>\n",
    "                            <center><sub>Source: <a\n",
    "                                        href=\"https://www.todaysoftmag.com/article/1358/hadoop-mapreduce-deep-diving-and-tuning\">https://www.todaysoftmag.com/article/1358/hadoop-mapreduce-deep-diving-and-tuning</a></sub>\n",
    "                            </center>\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                </table>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><a href=\"https://en.wikipedia.org/wiki/Task_parallelism\"><strong>task parallelism</strong></a> - distributing\n",
    "        tasks across multiple threads/processors, which perform the tasks in parallel, e.g.<ul>\n",
    "            <li>parallel force updates in <a href=\"https://en.wikipedia.org/wiki/Molecular_dynamics\">molecular\n",
    "                    dynamics</a>, or parallel finite element updates in a <a\n",
    "                    href=\"https://en.wikipedia.org/wiki/Finite_element_method\">finite element method</a> mesh</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>and <a href=\"https://en.wikipedia.org/wiki/Loop-level_parallelism\">loop-level parallelism</a> - a special case\n",
    "        combining task and data parallelism, where tasks and a corresponding data chunks are instructions and data used\n",
    "        in <code>for</code> loop iterations, executed then in parallel.</li>\n",
    "</ol>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the three Sections on parallel computing on: 1) CPUs, 2) clusters, and 3) GPUs, you will learn how to use tools for the data and tasks parallelism.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parallelization limits*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throwing more workers to work won't simply shorten the work time proportionally as there are always parts which cannot be done in parallel.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/ParallelLimits.png\" width=\"400px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://demotywatory.pl/913269/Polscy-robotnicy\">https://demotywatory.pl/913269/Polscy-robotnicy</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "Amdahl's law is a simple rule-of-thumb theoretical limit on achievable speedup of a task with a fixed problem size, when a known part of the task runtime must be spent in serial execution, and rest can be parallelized.\n",
    "\n",
    "For example:\n",
    "* program needs 20h to complete on 1 CPU,\n",
    "* 2h (10%) portion of the program cannot be parallelized, whereas the remaining 18h (90%) can,\n",
    "* ⇒ the minimum total execution time cannot be less than 2h\n",
    "* ⇒ speedup can't be more than 10x\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/AmdahlsLaw.png\" width=\"600px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-09766-4_77\">https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-09766-4_77</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "We will get back to this and other measures of parallel scaling in more detail in the Section on parallel computing.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use fast hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr><td><img src=\"imgs/Supercomputer.png\" width=\"350px\"></td></tr>\n",
    "    <tr><td><center><sub>Source: <a href=\"https://dribbble.com/shots/1984685-Supercomputer-visual-pun\">https://dribbble.com/shots/1984685-Supercomputer-visual-pun</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<p><i class=\"fa fa-info-circle\"></i>&nbsp You can make your program run faster by using high-end hardware, like:</p>\n",
    "\n",
    "<ol>\n",
    "    <li><strong>Multi-core servers</strong>,</li>\n",
    "    <li><strong>High Performance Computing (HPC) clusters</strong> a.k.a. supercomputers,</li>\n",
    "    <li><strong>Grapic cards</strong> a.k.a. GPUs.</li>\n",
    "</ol>\n",
    "\n",
    "<p>Using any of the above means running your code in parallel, but <strong>some hardware is suitable only for some problems</strong>\n",
    "    (CPU-bound vs. I/O-bound problems, or problems fitting into &quot;single instruction, multiple data&quot; processing type).</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "Most often you need to adjust your program to target specific hardware.\n",
    "\n",
    "In the three Sections on parallel computing you will see how to adjust your Python programs for specific hardware, usually, without much overhead.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm complexity analysis: Big O notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms can be theoretically classified according to how their runtime or space (memory) requirements grow as the input/problem size grows.\n",
    "\n",
    "This is important because usually the scalability or practical applicability of an algorithm going from development tests input data to real-world large input data sets is limited by the order of growth of runtime or space requirements with respect to problem size (rather than by e.g. specific constants).\n",
    "\n",
    "<blockquote>\n",
    "    <strong>Example: algorithm complexity</strong>\n",
    "\n",
    "| Problem size |  Algorithm 1: $n^2 / 2$ seconds runtime | Algorithm 2: $100 n$ seconds runtime |\n",
    "| ------------ | --------------------------- | --------------------------- |\n",
    "| $n = 10$     | 50 sec                      | 16 min 40 sec               |  \n",
    "| $n = 100$    | ~1 hour 24 min              |  ~2 hours 47 min            |\n",
    "| $n = 1000$   | ~5 days 19 hours            | ~1 day 4 hours              |\n",
    "| $n = 10000$  | ~**1 year 30 weeks**        | ~1 week 5 days              |\n",
    "\n",
    "If we anticipate practical problems of size $n > 1000$, we should invest into developing Algorithm 2, which is considered to be theoretically faster as it has linear $n$ and not quadratic $n^2$ runtime. If all practical problems are limited by size $n \\leq 100$, it's completely fine to stick to Algorithm 1 (the Algorithm 2 would be considered to be a [galactic algorithm](https://en.wikipedia.org/wiki/Galactic_algorithm), i.e. a theoretically faster one, but only for impractically large problems).\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In algorithms complexity analysis **a unit is an abstract number**, respectively, **a performed basic operation (runtime unit) and a stored basic object (space unit)**.\n",
    "\n",
    "<blockquote>\n",
    "    <strong>Example: distance matrix</strong>\n",
    "\n",
    "Compute a matrix of distances between $n$ points:\n",
    "\n",
    "<math>\\begin{align}\\begin{bmatrix}\n",
    "0 & d_{12}^2 & \\dots & d_{1n}^2 \\\\\n",
    "d_{21}^2 & 0 & \\dots & d_{2n}^2 \\\\\n",
    "\\vdots&\\vdots & \\ddots&\\vdots&  \\\\\n",
    "d_{n1}^2 & d_{n2}^2 & \\dots & 0 \\\\\n",
    "\\end{bmatrix},\\end{align} </math>\n",
    "\n",
    "Assuming:\n",
    "    \n",
    "1. as a runtime operation unit a single distance computation, and\n",
    "2. as a storage object unit a number representing a single distance,\n",
    "\n",
    "one needs to compute $(n-1)\\cdot (n-1)~/~2 \\approx n^2~/~2$ distances, which are to be stored in a $n\\times n = n^2$ memory array.\n",
    "    \n",
    "We say that both **runtime complexity** and required **space complexity** of such distance matrix computation is quadratic, or, more formally, \"oh of $n$-squared\", which is denoted as:\n",
    "$$O(n^2)$$\n",
    "\n",
    ".\n",
    "</blockquote>\n",
    "\n",
    "$O(\\cdots)$ is a so called **Big O notation**, or which is a mathematical notation that describes the limiting behavior of a function, up to a constant, when the argument $n$ tends towards infinity (see e.g. [Big O notation @ Wikipedia](https://en.wikipedia.org/wiki/Big_O_notation) for a more formal mathematical description). The idea is to give an growth rate of required runtime or space, with respect to growing input/problem size. Thus, constant multipliers or lower order functions are ignored in the Big O notation, and we have, for example:\n",
    "\n",
    "<math>\\begin{align}\n",
    "\\frac{1}{2}(n^2 - 2n - 1) &= O(n^2)\\\\\n",
    "&\\text{ or}\\\\\n",
    "2n + \\log{n} &= O(n).\n",
    "\\end{align} </math>\n",
    "\n",
    "The incentive is to characterize functions according to their growth rates as this is the main factor for estimating a speed or memory use of an algorithm.\n",
    "\n",
    "The letter O is used because the growth rate of a function is also referred to as the **order of the function**.\n",
    "\n",
    "Beware: a description of a function in terms of Big O notation formally provides only an upper bound on the growth rate of the function (hence the capital letter), e.g. we also have $n = O(n^2)$, but no one really writes that their algorithms are slower or more memory-hungry than they actually are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orders of common functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<table>\n",
    "    <tr><td><img src=\"imgs/bigo.png\" width=\"600px\"></td></tr>\n",
    "        <tr><td><center><sub>Source: <a href=\"https://runestone.academy/runestone/books/published/pythonds/index.html\">https://runestone.academy/runestone/books/published/pythonds/index.html</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "In a table form, from lowest to highest \"order of growth\", with additional color coding for practical applicability, and with examples of problems with a corresponding complexity:\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th style=\"width:100px\">Notation</th>\n",
    "            <th style=\"width:100px\">Name</th>\n",
    "            <th style=\"width:600px\">Example</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr style=\"background: lightgreen;\">\n",
    "            <td>$O(1)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Constant_time\">Constant</a></td>\n",
    "            <td>Primitive operations<br/>Determining if a binary number is even or odd</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: lightgreen;\">\n",
    "            <td>$O(\\log{}n)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Logarithmic_time\">Logarithmic</a></td>\n",
    "            <td>Binary search</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: yellow;\">\n",
    "            <td>$O(n)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Linear_time\">Linear</a></td>\n",
    "            <td>Single loop<br/> Find element in an unsorted list<br/> Scalar/dot product (multiplication of a vector and vector)<br/> Special sorting (Counting sort)</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: yellow;\">\n",
    "            <td>$O(n\\log{}n)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Linearithmic_time\">Log Linear</a></td>\n",
    "            <td>Sorting (Quicksort, Heapsort and Merge sort)</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: orange;\">\n",
    "            <td>$O(n^2)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Table_of_common_time_complexities\">Quadratic</a>\n",
    "            </td>\n",
    "            <td>One nested loop<br/> Find duplicate elements in a list (naive)<br/> Multiplication of a matrix and a vector<br/> Simple sorting algorithms (Bubble sort, Selection sort, Insertion sort)</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: orange;\">\n",
    "            <td>$O(n^3)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Table_of_common_time_complexities\">Cubic</a></td>\n",
    "            <td>Two nested loops<br/> Matrix multiplication (naive)<br/> Solving a system of $n$ linear equations (via Gaussian elimination)</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: orange;\">\n",
    "            <td>$O(n^c)$, $c>1$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time\">Polynomial</a></td>\n",
    "            <td>Finding maximum flow in a network (minimum cut in a graph)<br/> Workers to tasks Assignment Problem (maximum\n",
    "                weighted bipartite matching)</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: orangered;\">\n",
    "            <td>$O(2^n)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Time_complexity#Exponential_time\">Exponential</a></td>\n",
    "            <td>Iterating over all subsets of a list<br/> Finding the (exact) solution to the Traveling Salesman Problem using dynamic programming</td>\n",
    "        </tr>\n",
    "        <tr style=\"background: orangered;\">\n",
    "            <td>$O(n!)$</td>\n",
    "            <td><a href=\"https://en.wikipedia.org/wiki/Factorial\">Factorial</a></td>\n",
    "            <td>Iterating over all permutations of a list<br/> Solving the Traveling Salesman Problem via brute-force search</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz exercise [15 min]\n",
    "\n",
    "1. What is a runtime and space complexity of this function:\n",
    "    ```python\n",
    "    def compute(n: int):\n",
    "        a = -1\n",
    "        i = n\n",
    "        while (i > 0):\n",
    "            a += 1\n",
    "            i //= 2\n",
    "        return a\n",
    "    ```\n",
    "   ? What are the operation and storage units for the complexity analysis.\n",
    "\n",
    "1. What is a runtime complexity of an algorithm that performs $10^6 n$ initial operations, $2 n^3$ nested-loops operations and $10^9 \\log{n}$ post-processing operations?\n",
    "\n",
    "1. Does an $O(\\log{n})$ algorithm run faster than $O(n)$ algorithm?\n",
    "\n",
    "1. A program computes results in $2 n$ steps, but once every $n^2 + 1$ calls the program needs to run data maintenance that takes additionally $n^2$ steps. What is the runtime complexity of the program?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "**Solutions**\n",
    "\n",
    "1. Runtime complexity is $O(\\log{n})$, as `i` decreases from `n`, `n/2`, `n/4`, ... to `1`, where primitive (unit) operation is both `+` and `//` (integer division). Space complexity is $O(1)$ as we only use two variables, with primitive storage unit being `int`.\n",
    "\n",
    "1. $O\\left(10^6 n + 2 n^3 + 10^9 \\log{n}\\right) = O\\left(n + n^3 + \\log{n}\\right) = O\\left(n^3\\right)$\n",
    "\n",
    "1. No, not in general, only true for large enough input size $n$, but up to some $n$ values, depending on the actual constants and lower order terms of the runtime complexity, the $O(n)$ algorithm may run faster than the $O(\\log{n})$ algorithm.\n",
    "\n",
    "1. The Big-Oh runtime complexity is the worst-case scenario, so it's $O(2 n + n^2) = O(n^2)$, but in such cases it makes sense to talk about average runtime complexity, which would be in this case $\\left(n^2 \\cdot 2 n + 1 \\cdot (2 n + n^2)\\right) / n^2 = \\left(2 n^3 + n^2 + 2 n\\right) / n^2 = O(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding unnecessary computations: memoization example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<blockquote>\n",
    "    <strong>Example: Fibonacci numbers</strong>\n",
    "\n",
    "The [Fibonacci numbers](https://en.wikipedia.org/wiki/Fibonacci_number), commonly denoted $F_n$, form a sequence, called the Fibonacci sequence, such that each number is the sum of the two preceding ones, starting from $0$ and $1$. That is,\n",
    "\n",
    "<math>\\begin{align}\n",
    "F_0 &= 0,\\\\\n",
    "F_1 &= 1, \\\\\n",
    "F_{n}&=F_{n-1}+F_{n-2},\\text{ for } n > 1\n",
    "\\end{align}\n",
    "</math>\n",
    "\n",
    "The task is to simply compute $F_{n}$, given $n$ as an input.\n",
    "    \n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First implementation - directly from definition, a recursion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci_recursion(n):\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    if n == 1:\n",
    "        return 1\n",
    "\n",
    "    return fibonacci_recursion(n - 1) + fibonacci_recursion(n - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.29 s, sys: 31.3 ms, total: 4.33 s\n",
      "Wall time: 4.42 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9227465"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time fibonacci_recursion(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That wasn't very fast. Let's profile this implementation using `%lprun` from `line_profiler` (with some lower number to speed it up):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.002135 s\n",
       "File: <ipython-input-21-53f307cd2a44>\n",
       "Function: fibonacci_recursion at line 1\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     1                                           def fibonacci_recursion(n):\n",
       "     2      1973        638.0      0.3     29.9      if n == 0:\n",
       "     3       377        117.0      0.3      5.5          return 0\n",
       "     4      1596        815.0      0.5     38.2      if n == 1:\n",
       "     5       610        161.0      0.3      7.5          return 1\n",
       "     6                                           \n",
       "     7       986        404.0      0.4     18.9      return fibonacci_recursion(n - 1) + fibonacci_recursion(n - 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f fibonacci_recursion fibonacci_recursion(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That does not look good - there are many more $F_0$ and $F_1$ checks then the actual recursion step summations $F_{n}=F_{n-1}+F_{n-2}$. Let's analyze the algorithm's complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's how the computation will look like:\n",
    "```\n",
    "            f(n)\n",
    "          /     \\\n",
    "     f(n-1)     f(n-2)\n",
    "    /     \\    /     \\\n",
    "f(n-2) f(n-3) f(n-3) f(n-4)\n",
    "  ...    ...    ...    ...\n",
    " /   \\\n",
    "f(1) f(0)\n",
    "```\n",
    "That's a $O\\left(2^n\\right)$ time and space complexity method (there are $2$ calls at each of $n$ recursion steps).\n",
    "\n",
    "And that's a lot of repetitive calls of our function (especially `f(1)` and `f(0)` calls), which we don't really need. Let's try to avoid that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we memoize the already computed results in a lookup table?\n",
    "\n",
    "In the recursive approach, on each function call check if the result was computed for given input, and if it was, then simply read it from memory in $O(1)$ time. So the computation looks then like:\n",
    "```\n",
    "                  f(n)\n",
    "                /     \\\n",
    "           f(n-1)   lookup f(n-2)\n",
    "          /     \\\n",
    "      f(n-2) lookup f(n-3)\n",
    "        ...\n",
    "     f(3)\n",
    "     /  \\\n",
    "  f(2) lookup f(1)\n",
    " /   \\\n",
    "f(1) f(0)\n",
    "```\n",
    "This would be now a $O(n)$ time (left recursion branch) and $O(n)$ space (lookup table) algorithm.\n",
    "\n",
    "Let's try available memory- and disk-based cache implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memoization in memory with `functools.lru_cache`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`@functools.lru_cache`** is a built-in Python decorator that wraps a function with a cache that **saves up to the `maxsize` most recent calls in memory**. It can save time when an expensive or I/O bound function is periodically called with repetitive arguments.\n",
    "\n",
    "An **LRU (least recently used)** cache works best when the most recent calls are the best predictors of upcoming calls (for example, the most popular articles on a news server tend to change each day). The cache’s size limit assures that the cache does not occupy too much space (for example, in long-running processes such as web servers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the Fibbonacci's number using the LRU cache.\n",
    "\n",
    "Second implementation - memoization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=128)\n",
    "def fibonacci_lru(n):\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    if n == 1:\n",
    "        return 1\n",
    "\n",
    "    return fibonacci_lru(n - 1) + fibonacci_lru(n - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43 µs, sys: 7 µs, total: 50 µs\n",
      "Wall time: 67 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9227465"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time fibonacci_lru(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was much quicker.\n",
    "\n",
    "A `functools.lru_cache`-wrapped function has additional methods to display cache status or to clear (\"invalidate\") cache, respectively, `.cache_info()` and `.cache_clear()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CacheInfo(hits=33, misses=36, maxsize=128, currsize=36)\n",
      "CacheInfo(hits=0, misses=0, maxsize=128, currsize=0)\n"
     ]
    }
   ],
   "source": [
    "print(fibonacci_lru.cache_info())\n",
    "# memoization-based solution used `hits` pre-computed results, and\n",
    "# on each of `misses` added cache entry contributing to `currsize`\n",
    "\n",
    "fibonacci_lru.cache_clear()\n",
    "print(fibonacci_lru.cache_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p>\n",
    "        <i class=\"fa fa-warn\"></i>&nbsp<strong>Beware</strong>:\n",
    "        benchmarking memoization-based solutions is tricky - it is only really fair to <strong>measure runtime with the cache setup</strong>.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "This is what we did with timing the single call above. Next runs will be much faster as it is really only getting pre-computed value from the cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 µs, sys: 1e+03 ns, total: 37 µs\n",
      "Wall time: 40.1 µs\n",
      "83.5 ns ± 3.76 ns per loop (mean ± std. dev. of 3 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%time fibonacci_lru(35)\n",
    "%timeit -r3 fibonacci_lru(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take-home notes:\n",
    "\n",
    "* caching using a decorator requires **only minimal code change**, and\n",
    "* **in-memory caching is fast**,\n",
    "\n",
    "BUT\n",
    "\n",
    "The LRU or caching **limitation** in general is that **cache arguments must be hashable** (in particular, **non-mutable**), so e.g. `functools.lru_cache` is not applicable to functions with arguments with type such as `list` (but, if possible, you can use `tuple` instead)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memoization in files with `joblib.Memory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joblib](https://joblib.readthedocs.io/) is an additional Python package that provides a set of tools for lightweight pipelining of computations. In particular it provides **disk-based caching of function calls** (memoization) via **`joblib.Memory`** class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fibonacci's number with disk cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching to directory: /var/folders/k8/zfp7dvcs1m326gz1brql1tv80000gn/T/tmplo2m46qi\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "import joblib\n",
    "\n",
    "# for a clean demonstration: using a new temp dir as a cache dir\n",
    "cachedir = tempfile.mkdtemp()\n",
    "print(\"Caching to directory:\", cachedir)\n",
    "\n",
    "\n",
    "memory_disk = joblib.Memory(location=cachedir, verbose=0)\n",
    "\n",
    "\n",
    "@memory_disk.cache\n",
    "def fibonacci_disk(n):\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    if n == 1:\n",
    "        return 1\n",
    "\n",
    "    return fibonacci_disk(n - 1) + fibonacci_disk(n - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71.1 ms, sys: 159 ms, total: 230 ms\n",
      "Wall time: 369 ms\n",
      "104 ns ± 9.96 ns per loop (mean ± std. dev. of 3 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%time fibonacci_disk(35)\n",
    "%timeit -r3 fibonacci_lru(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joblib/__main__--Users-uweschmitt-Projects-fast-python-section_3-<ipython-input-b05f6a331c02>/fibonacci_disk/func_code.py\n",
      "joblib/__main__--Users-uweschmitt-Projects-fast-python-section_3-<ipython-input-b05f6a331c02>/fibonacci_disk/7d041c38383d2394b2f8017b44f964d7/metadata.json\n",
      "joblib/__main__--Users-uweschmitt-Projects-fast-python-section_3-<ipython-input-b05f6a331c02>/fibonacci_disk/7d041c38383d2394b2f8017b44f964d7/output.pkl\n",
      "joblib/__main__--Users-uweschmitt-Projects-fast-python-section_3-<ipython-input-b05f6a331c02>/fibonacci_disk/0232fc500b711a639db501f605ec5fef/metadata.json\n",
      "joblib/__main__--Users-uweschmitt-Projects-fast-python-section_3-<ipython-input-b05f6a331c02>/fibonacci_disk/0232fc500b711a639db501f605ec5fef/output.pkl\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# Ignore: Utility function to recursively get file names in a folder.\n",
    "def get_fns_rec(path):\n",
    "    for f in glob.glob(f\"{path}/**/*.*\", recursive=True):\n",
    "        yield os.path.relpath(f, path)\n",
    "\n",
    "\n",
    "# Print first 5 files\n",
    "for fn, _ in zip(get_fns_rec(cachedir), range(5)):\n",
    "    print(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disk access (I/O) is slow**, so setting up or actual reads from the `joblib.Memory` disk cache are slower than in case of a memory cache, but:\n",
    "\n",
    "  * **cached data is available when code is run again** in a new interpreter session,\n",
    "  * it is useful as a **tool for creating \"checkpoints\"/\"snapshots\"** (e.g. during workflow development),\n",
    "  * `joblib.Memory` specifically works with non-hashable arguments, such as `list`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding exercise [15 min]\n",
    "\n",
    "Avoid unnecessary computations in the Euclidian matrix example by avoiding computing `0` values and by using a memory cache to avoid computing each non-zero value twice (hint: use a built-in `sorted` function to order points). How big cache do you need?\n",
    "\n",
    "The memoization strategy pays off only when a single distance computation takes some time. To that end, use the high dimensional points example (below).\n",
    "\n",
    "Compare runtimes (hint: benchmark first memory cache use separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 16448314920.0, 65795050349.0, 148040605163.0, 263184044289.0]\n",
      "[16448314920.0, 0.0, 16449229561.0, 65797149527.0, 148043155561.0]\n",
      "[65795050349.0, 16449229561.0, 0.0, 16449360898.0, 65796993752.0]\n",
      "[148040605163.0, 65797149527.0, 16449360898.0, 0.0, 16449159728.0]\n",
      "[263184044289.0, 148043155561.0, 65796993752.0, 16449159728.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# %load ../examples/euclidian_distance_d.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Euclidean distance example in d-dimensional space\"\"\"\n",
    "\n",
    "\n",
    "def setup_points(n, d=100_000):\n",
    "    # create n points in d-dim for testing\n",
    "    points = []\n",
    "    for i in range(0, d * n, d):\n",
    "        points.append(tuple(1 + float(i // (j + 1)) for j in range(d)))\n",
    "    return points\n",
    "\n",
    "\n",
    "def dist_squared(a, b):\n",
    "    s = 0\n",
    "    d = len(a)\n",
    "    for i in range(d):\n",
    "        s += (a[i] - b[i]) ** 2\n",
    "    return s\n",
    "\n",
    "\n",
    "def dist_matrix(points, dist_func=dist_squared):\n",
    "    # compute distance matrix using given `dist_func`\n",
    "    rows = []\n",
    "    for p in points:\n",
    "        row = []\n",
    "        for q in points:\n",
    "            row.append(dist_func(p, q))\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    M = dist_matrix(setup_points(10))\n",
    "    for row in M[:5]:\n",
    "        print(row[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking setup\n",
    "n_points = 10\n",
    "points = setup_points(n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modify and benchmark distance functions\n",
    "\n",
    "# %timeit -r 3 -n 1 dist_matrix(points, dist_func=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem size: n = 10, d = 100000\n",
      "\n",
      "# reference\n",
      "2.5 s ± 270 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "\n",
      "# no zero distance\n",
      "2.26 s ± 38.2 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "\n",
      "# no zero distance and cached ordered pair results\n",
      "CPU times: user 1.36 s, sys: 11.7 ms, total: 1.37 s\n",
      "Wall time: 1.44 s\n",
      "1.09 s ± 49.9 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "print(f\"Problem size: n = {n_points}, d = {len(points[0])}\")\n",
    "print()\n",
    "\n",
    "print(\"# reference\")\n",
    "%timeit -r 3 -n 1 dist_matrix(points)\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"# no zero distance\")\n",
    "\n",
    "\n",
    "def dist_squared_no_zero(a, b):\n",
    "    if a == b:\n",
    "        return 0\n",
    "    return dist_squared(a, b)\n",
    "\n",
    "\n",
    "%timeit -r 3 -n 1 dist_matrix(points, dist_func=dist_squared_no_zero)\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"# no zero distance and cached ordered pair results\")\n",
    "import functools\n",
    "import time\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=(n_points - 1) ** 2 // 2)\n",
    "def dist_squared_cached(a, b):\n",
    "    return dist_squared(a, b)\n",
    "\n",
    "\n",
    "def dist_squared_no_zero_ordered_cached(a, b):\n",
    "    if a == b:\n",
    "        return 0\n",
    "    p, q = sorted((a, b))  # a < b\n",
    "    return dist_squared_cached(p, q)\n",
    "\n",
    "\n",
    "%time dist_matrix(points, dist_func=dist_squared_no_zero_ordered_cached)\n",
    "%timeit -r 3 -n 1 dist_matrix(points, dist_func=dist_squared_no_zero_ordered_cached)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python built-in data structures\n",
    "\n",
    "We will give a quick overview of important built-in Python data structures and complexity of typical operations. We've already seen that choosing the right data structure for your task can make a significant difference.\n",
    "\n",
    "To be able to choose well, or design well a data structure or an algorithm, you need to know the basic data structures first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: finish quick feedback and commit together w/ mmap (update issue for addressed feedback points)\n",
    "# TODO: always start data type sections w/ literal code example\n",
    "1 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding numbers and similar operations are $O(1)$, with one exception of very large integer numbers.\n",
    "\n",
    "Contrary to compiled languages such as `C`, Python integers are implemented internally as 32 or 64 bit numbers or as arrays of these to **avoid overflow**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4611686018427387904\n",
      "9223372036854775808\n"
     ]
    }
   ],
   "source": [
    "# fine in C:\n",
    "print(2 ** 62)\n",
    "\n",
    "# overflows in C, but not in Python:\n",
    "print(2 ** 63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the 64bit implementation is not suitable, computations such as addition or multiplication do not run directly on the CPU anymore but are implemented in software. This has two consequences:\n",
    "\n",
    "- operations become suddenly slower\n",
    "- complexity is not $O(1)$ any more but depends on the number of digits of the involved numbers.\n",
    "\n",
    "Luckily these differences are small and **non-constant operations complexity applies only to very large integer numbers** (like 1000 digits or more).\n",
    "\n",
    "**float** numbers in Python and **all dtypes in numpy** overflow and, thus, **have constant operations complexity**.\n",
    "\n",
    "Note: if you want non-overflowing numbers in Python use the `decimal` module from the standard library. Here all operations are implemented in software and thus are always slower than working with native `float` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.358298529049385849277351428E+331\n",
      "OverflowError (34, 'Result too large')\n"
     ]
    }
   ],
   "source": [
    "import decimal\n",
    "\n",
    "print(decimal.Decimal(2.0) ** 1100)\n",
    "\n",
    "try:\n",
    "    print(2.0 ** 1100)\n",
    "except OverflowError as e:\n",
    "    print(\"OverflowError\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings\n",
    "\n",
    "**String is not a basic data type**. Since there is no single character type in Python string seems and feels like an atomic data type, but **string is an [array](https://en.wikipedia.org/wiki/Array_data_structure) of single characters**; as such, **operations on a string depend on its length**.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/String.png\" width=\"600px\"></td></tr>\n",
    "        <tr><td><center><sub>Source: <a href=\"https://en.wikipedia.org/wiki/String_(computer_science)\">https://en.wikipedia.org/wiki/String_(computer_science)</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "Arrays are most often stored as a one/contiguous memory segment of known size and element type, such that position of each element in the memory can be quickly computed from its index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that `a` and `b` are Python strings of length $n$ and $m$, respectively; then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Operation | Comment | Complexity | |\n",
    "| - | - | - | - |\n",
    "| `a = \"...\"` | create string | $O(n)$ |  |\n",
    "| `len(a)`    | length of string  | $O(1)$ |  |\n",
    "| `a[i]` | read string character | $O(1)$ |  |\n",
    "| `a + b`| concatenate two strings| $O(n+m)$ |  |\n",
    "| `\"\".join([b_1, ..., b_n]) ` | concatenate $n$ strings | $O(n\\cdot m)$ |  |\n",
    "| `a.find(b)`/`a.index(b)` | find index of substring | $O(n\\cdot m)$ | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> |\n",
    "| `b in a`    | check if substring is in a string | $O(n\\cdot m)$ | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> |\n",
    "| `a.split(b)` | split string using substring as separator | $O(n+m)$ |  |\n",
    "| `a.lower()/a.upper()` | transform string characters to lower/upper case | $O(n)$ |  |\n",
    "| `a.startswith(b)/a.endswith(b)` | check if string starts/ends with substring | $O(\\min(n,m))$ |  |\n",
    "| $\\ldots$ | | | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>3</sup></i> |\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> `b in a` or `a.find(b)`/`a.index(b)`, are implemented such that they often run faster than $O(n\\cdot m)$ ([read here](https://web.archive.org/web/20100221040018/http://effbot.org/zone/stringlib.htm)).\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> For pattern-based string search use the regular expression methods from the [`re` standard library module](https://docs.python.org/3/library/re.html). They are heavily optimized but there is no theoretical run-time guarantee in $O$ notation.\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>3</sup></i> Do checkout [many convenient string methods of Python's standard library](https://docs.python.org/3/library/stdtypes.html#string-methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation\n",
    "\n",
    "Concatenation of two strings `a` and `b` of length $n$ and $m$, , respectively, requires to: \n",
    "\n",
    "1. Allocate new string `c` to hold $n + m$ bytes. \n",
    "2. Copy $n$ bytes from `a` to `c`.\n",
    "3. Copy $m$ bytes from `b` to `c` (after the content from `x`).\n",
    "\n",
    "Step 1 is usually $O(1)$, step 2 is $O(n)$ and step 3 is $O(m)$. In total this is $O(n + m)$.\n",
    "\n",
    "**To concatenate a _hard-coded_ number of strings** use either `+`, or, even better, use [string formatting with f-strings or the `str.format` method](https://docs.python.org/3/tutorial/inputoutput.html#fancier-output-formatting), e.g. `f\"{a}_{b}\"` for concatenation of strings `a` and `b` with `_` as a separator.\n",
    "\n",
    "Naively concatenating $n$ strings of length $m$ one-by-one using `+` results in a $O(m + 2m + 3m + \\ldots + n\\cdot m) = O(m\\cdot (1+...+n)) = O(m\\cdot(n+1)\\cdot n/2) = O(n^2\\cdot m)$ runtime complexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating  1000 strings of len 2000 took 0.79 seconds\n",
      "concatenating  2000 strings of len 2000 took 3.30 seconds\n",
      "concatenating  4000 strings of len 2000 took 14.72 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "b = \"1\" * 2_000\n",
    "\n",
    "for n in 1_000, 2_000, 4_000:\n",
    "    c = \"\"\n",
    "    t = time.time()\n",
    "    for _ in range(n):\n",
    "        c = c + b + \"\"  # `+ \"\"` is an anti-optimization trick; more below\n",
    "    needed = time.time() - t\n",
    "    print(f\"concatenating {n:5d} strings of len {len(b)} took {needed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily Python has some **internal optimizations to get around this behavior** (mainly by reusing memory and avoiding / reducing copy operations) in many (**but not all**) situations\n",
    "\n",
    "In the example above we've used `+ \"\"` to trick Python into not using its internal optimization.\n",
    "\n",
    "Let's check how Python internal optimization works out-of-the box in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating  1000  strings of len 2000 took 0.0024 seconds\n",
      "concatenating  2000  strings of len 2000 took 0.0044 seconds\n",
      "concatenating  4000  strings of len 2000 took 0.0150 seconds\n"
     ]
    }
   ],
   "source": [
    "b = \"1\" * 2_000\n",
    "\n",
    "for n in 1_000, 2_000, 4_000:\n",
    "    c = \"\"\n",
    "    t = time.time()\n",
    "    for _ in range(n):\n",
    "        c = c + b\n",
    "    needed = time.time() - t\n",
    "    print(f\"concatenating {n:5d}  strings of len {len(b)} took {needed:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In more complicated situations it is  **not easy to predict when internal optimizations are done**.\n",
    "\n",
    "Instead, **to concatenate a _variable_ number of strings use `join`**:\n",
    "1. collect the strings in a list `l` of strings (if you don't have this list already)\n",
    "2. use `\"\".join(l)` to concatenate strings.\n",
    "\n",
    "Comment: `s.join(l)` concatenates the strings from `l` with the separator string `s` between them.\n",
    "\n",
    "Preparing list for `join` takes $O(n)$ on average, and using `join` allows to pre-allocate final memory and do copy operations once for each of $n$ input string, giving a runtime complexity $O(n\\cdot m)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating  1000  strings of len 2000 took 0.0012 seconds\n",
      "concatenating  2000  strings of len 2000 took 0.0019 seconds\n",
      "concatenating  4000  strings of len 2000 took 0.0024 seconds\n"
     ]
    }
   ],
   "source": [
    "b = \"1\" * 2_000\n",
    "\n",
    "for n in 1_000, 2_000, 4_000:\n",
    "    l = []\n",
    "    s = time.time()\n",
    "    for _ in range(n):\n",
    "        l.append(b)\n",
    "    c = \"\".join(l)\n",
    "    needed = time.time() - s\n",
    "    print(f\"concatenating {n:5d}  strings of len {len(b)} took {needed:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists\n",
    "\n",
    "Python lists are the general workhorse in Python for \"collecting data\".\n",
    "\n",
    "Python lists are implemented using [dynamic arrays](https://en.wikipedia.org/wiki/Dynamic_array) (and not using [linked lists](https://en.wikipedia.org/wiki/Linked_list)). Dynamic arrays have spare space for new elements and are copied into 1.5-2 times larger array when the spare space runs out.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/Dynamic_array.svg\" width=\"250px\"></td></tr>\n",
    "        <tr><td><center><sub>Source: <a href=\"https://en.wikipedia.org/wiki/Dynamic_array\">https://en.wikipedia.org/wiki/Dynamic_array</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "The benefit of using dynamic arrays are low runtime complexities for access to list elements and, on average, for adding list elements, but this comes at the cost of some memory overhead (in Python around 13%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that `x` is a Python list of length $n$, `i` an arbitrary integer, `y` an arbitrary Python object and `it` an arbitrary iterable. Then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Operation | Comment | Complexity | |\n",
    "| - | - | - | - |\n",
    "| `x = list(it)` | create list | $O(\\text{len}(\\text{it}))$ |  |\n",
    "| `len(x)`    | length of list  | $O(1)$ |  |\n",
    "| `x.append(y)`| append element| $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `x.extend(it)` | extend list | $O(\\text{len}(\\text{it}))$ on average |  |\n",
    "| `x[i] = y` | overwrite list element | $O(1)$ |   |\n",
    "| `y = x[i]` | read list element | $O(1)$ |  |\n",
    "| `del x[i]` | remove element | $O(n)$ ($O(1)$ for the last element)| |\n",
    "| `y = x.pop(i)` | read element and remove | $O(n)$ ($O(1)$ for the last element)| |\n",
    "| `x.insert(i, y)` | insert element | $O(n - i)$ | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> |\n",
    "| `x.index(y)` | find index of element | $O(n)$ |  |\n",
    "| `y in x`    | check membership | $O(n)$ |  |\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> Appending and element to a list takes constant time  $O(1)$ in most situations. On rare occasions the internal data needs to be reorganized which takes $O(n)$ time. The average runtime is $O(1)$ (cf. [amortized cost analysis for dynamic array](https://en.wikipedia.org/wiki/Amortized_analysis#Dynamic_array)).\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i>\n",
    "Inserting an element at the beginning of the list using `x.insert(0, y)` is $O(n)$. If reading, appending or deleting both ends of a list are common operations in your problem use a `deque` (*double ended queue*) data structure from the [collections module](https://docs.python.org/3/library/collections.html#collections.deque); they all have runtime $O(1)$. The caveat is that operations on `deque` inner elements like `x[i] = y` or `y = x[i]` are $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuples\n",
    "\n",
    "Tuples are immutable lists, so the only interesting operations are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Operation | Comment | Complexity |  |\n",
    "| - | - | - | - |\n",
    "| `len(x)`    | length of tuple | $O(1)$ |  |\n",
    "| `x = tuple(it)` | create tuple | $O(\\text{len}(\\text{it}))$ |  |\n",
    "| `y = x[i]` | read tuple element | $O(1)$ |  |\n",
    "| `x.index(y)` | find index of element | $O(n)$ |  |\n",
    "| `y in x`    | check membership | $O(n)$ |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuples can be slightly faster than lists, but replacing tuples by lists for reasons of performance is most often an unnecessary micro-optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "\n",
    "Dictionaries implement lookup tables and are heavily optimized in Python.\n",
    "\n",
    "- dictionary **values can be arbitrary** Python objects,\n",
    "- dictionary **keys must be immutable** Python objects (such as `int`, `str`, `tuple`s of immutable objects, **not**: `list`s, `set`s, `dict`s).\n",
    "- dictionary **keys are unique**.\n",
    "\n",
    "\n",
    "Dictionaries are also used inside the Python interpreter in many places.\n",
    "\n",
    "E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': 'binascii',\n",
       " '__doc__': 'Conversion between binary data and ASCII',\n",
       " '__package__': '',\n",
       " '__loader__': <_frozen_importlib_external.ExtensionFileLoader at 0x10d8b8f70>,\n",
       " '__spec__': ModuleSpec(name='binascii', loader=<_frozen_importlib_external.ExtensionFileLoader object at 0x10d8b8f70>, origin='/Users/uweschmitt/.pyenv/versions/3.9.5/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/binascii.cpython-39-darwin.so'),\n",
       " 'a2b_uu': <function binascii.a2b_uu(data, /)>,\n",
       " 'b2a_uu': <function binascii.b2a_uu(data, /, *, backtick=False)>,\n",
       " 'a2b_base64': <function binascii.a2b_base64(data, /)>,\n",
       " 'b2a_base64': <function binascii.b2a_base64(data, /, *, newline=True)>,\n",
       " 'a2b_hqx': <function binascii.a2b_hqx(data, /)>,\n",
       " 'b2a_hqx': <function binascii.b2a_hqx(data, /)>,\n",
       " 'a2b_hex': <function binascii.a2b_hex(hexstr, /)>,\n",
       " 'b2a_hex': <function binascii.b2a_hex>,\n",
       " 'hexlify': <function binascii.hexlify>,\n",
       " 'unhexlify': <function binascii.unhexlify(hexstr, /)>,\n",
       " 'rlecode_hqx': <function binascii.rlecode_hqx(data, /)>,\n",
       " 'rledecode_hqx': <function binascii.rledecode_hqx(data, /)>,\n",
       " 'crc_hqx': <function binascii.crc_hqx(data, crc, /)>,\n",
       " 'crc32': <function binascii.crc32(data, crc=0, /)>,\n",
       " 'a2b_qp': <function binascii.a2b_qp(data, header=False)>,\n",
       " 'b2a_qp': <function binascii.b2a_qp(data, quotetabs=False, istext=True, header=False)>,\n",
       " '__file__': '/Users/uweschmitt/.pyenv/versions/3.9.5/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload/binascii.cpython-39-darwin.so',\n",
       " 'Error': binascii.Error,\n",
       " 'Incomplete': binascii.Incomplete}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import binascii\n",
    "\n",
    "binascii.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when you e.g. use `binascii.hexlify`, the Python interpreter actually accesses `binascii.__dict__[\"hexlify\"]`.\n",
    "\n",
    "Thus having a fast and efficient dictionary implementation is crucial for the overall speed of Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that `x` is a Python dictionary with $n$ entries, `k` an arbitrary object which can be used as key, `y` an arbitrary Python object and `it` an arbitrary iterable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Operation | Comment | Complexity | |\n",
    "| - | - | - | - |\n",
    "| `x = dict(it)` | create dictionary | $O(\\text{len}(\\text{it}))$ | |\n",
    "| `len(x)`   | size of dictionary | $O(1)$ | |\n",
    "| `x[k] = y` | insert/overwrite value at key | $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> | \n",
    "| `y = x[k]`/`y = x.get(k)` | lookup value at key | $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `del x[k]` | remove value and key | $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `k in x`   | key membership test | $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `x.keys()` | keys of dictionary | $O(1)$ | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> |\n",
    "| `x.values()` | values of dictionary | $O(1)$ | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> |\n",
    "| `x.items()` | key and value pairs of dictionary | $O(1)$ | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> |\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> The internal data structure used for dictionaries is a so-called [Hash table / Hash map](https://en.wikipedia.org/wiki/Hash_table) (for Python-specific). This data structure makes lookup, insertion and deletion operations $O(1)$ on average (using so called hashing of keys and open addressing strategy to resolve hash conflicts; [read more](http://thepythoncorner.com/dev/hash-tables-understanding-dictionaries/)).\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>2</sup></i> A `for` loop over `.keys()`, `.values()` or `.items()` is still $O(n)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `defaultdict` and `Counter`\n",
    "\n",
    "Python's standard library offers two  classes which add some convenience on-top of dictionaries to make your live easier: the `collections.defaultdict` and `collections.Counter` classes.\n",
    "\n",
    "The [`defaultdict`](https://pymotw.com/3/collections/defaultdict.html) takes a function which is called for unknown keys. E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {3: [3]})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# list is a function which returns an empty list\n",
    "list() == []\n",
    "\n",
    "d2 = defaultdict(list)\n",
    "d2[3].append(3)\n",
    "\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider mentioning setdefault, get(k, default)\n",
    "d2.setdefault(4, []).append(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another helpful class is [`Counter`](https://pymotw.com/3/collections/counter.html) which takes a collection of data / an iterable and computes a dictionary which maps each item from the data to the number of its occurrences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 0, 1, 2, 3, 0, 1]\n",
      "\n",
      "Counter({0: 3, 1: 3, 2: 2, 3: 2, 4: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "items = list(range(5)) + list(range(4)) + list(range(2))\n",
    "print(items)\n",
    "print()\n",
    "\n",
    "counter = Counter(items)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets\n",
    "\n",
    "We already used sets in chapter 2 to optimize our profiling example.\n",
    "\n",
    "A set in Python represents a mathematical set. Contrary to lists / tuples:\n",
    "\n",
    "1. There are **no duplicate elements** in a set.\n",
    "2. Set has **no order**; e.g., you can not ask for the first element of set.\n",
    "3. Set **elements must be immutable** (same as for dictionary keys)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that `a` and `b` are Python sets of size $n$ and $m$, respectively; then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Operation | Comment | <div style=\"width:200px\">Complexity</div> | |\n",
    "| - | - | - | - |\n",
    "| `a = set(it)` | create set | $O(\\text{len}(\\text{it}))$ | |\n",
    "| `len(a)`   | size of set | $O(1)$ | |\n",
    "| `a.add(x)` | add element | $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `x in a` | element membership test | $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `a.remove(x)` | remove element | $O(1)$ on average | <i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> |\n",
    "| `a \\| b`, `a.union(b)`  | set union  | $O(n + m)$| |\n",
    "| `a - b`, `a.difference(b)`  | set difference  | $O(n)$| |\n",
    "| `a ^ b`, `a.symmetric_difference(b)` | set symmetric difference  | $O(n + m)$| |\n",
    "| `a & b`, `a.intersection(b)`  | set intersection  | $O(\\min(n, m))$| |\n",
    "\n",
    "<i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"><sup>1</sup></i> As with dictionaries, sets are implemented in Python using hash tables (with dummy values and some optimizations for that), making lookup (membership test), insertion and deletion operations $O(1)$ on average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets can be cleverly used e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def has_only_unique_elements(li):\n",
    "    # O(n)\n",
    "    return len(set(li)) == len(li)\n",
    "\n",
    "\n",
    "def count_duplicate_elements(li):\n",
    "    # O(n)\n",
    "    return len(li) - len(set(li))\n",
    "\n",
    "\n",
    "print(has_only_unique_elements([1, 2, 3]))\n",
    "print(has_only_unique_elements([1, 2, 3, 1]))\n",
    "\n",
    "print(count_duplicate_elements([1, 2, 3, 1, 2, 1, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: `frozenset` to `set` as `tuple` to `list`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Coding exercise [15 min]\n",
    "\n",
    "Given two sentences of words separated with single spaces find a list of words that do not appear in the other sentence and that appear only once in each sentence; for instance:\n",
    "```python\n",
    "s1 = \"you used to code in MATLAB but then you switched to Python\"\n",
    "s2 = \"we used to program in MATLAB but then we switched to Python\"\n",
    "\n",
    "unique_words(s1, s2)\n",
    "```\n",
    "should return:\n",
    "```python\n",
    "['code', 'program']\n",
    "```\n",
    "\n",
    "What is the time and space complexity of your solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['program', 'code']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def unique_words(s1, s2):\n",
    "\n",
    "    # worst-case: one letter words only (string size equals half of the words)\n",
    "    s1_words = s1.split()  # O(n) time and mem\n",
    "    s2_words = s2.split()  # O(m) time and mem\n",
    "\n",
    "    # worst-case: each word occurs exactly once\n",
    "    unique_words = set(s1_words) ^ set(s2_words)  # O(n + m) time and mem\n",
    "\n",
    "    s1_counter = Counter(s1_words)  # O(n) time and mem\n",
    "    s2_counter = Counter(s2_words)  # O(m) time and mem\n",
    "\n",
    "    return [\n",
    "        w\n",
    "        for w in unique_words\n",
    "        if w in s1_counter and s1_counter[w] == 1 or s2_counter[w] == 1\n",
    "    ]  # O(n+m) time and mem\n",
    "\n",
    "    # Total complexity: O(n+m) time and mem\n",
    "\n",
    "\n",
    "s1 = \"you used to code in MATLAB but then you switched to Python\"\n",
    "s2 = \"we used to program in MATLAB but then we switched to Python\"\n",
    "\n",
    "unique_words(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using better algorithms and data structures: examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, that in the Fibonacci's number example, we improved the time complexity of a brute-force recursive method from $O\\left(2^n\\right)$ to $O(n)$ by a memoization-based solution with $O(n)$ space complexity.\n",
    "\n",
    "But we actually can do even a bit better memoizing only the last two Fibonacci's numbers, and, thus, replacing the recursion with a straightforward `for` loop:\n",
    "```\n",
    "iteration 1:   f(2) = f(1) + f(0)\n",
    "iteration 2:   f(3) = f(2) + f(1)\n",
    "...\n",
    "iteration n-1: f(n) = f(n-1) + f(n-2)\n",
    "```\n",
    "\n",
    "Third implementation - sum up in a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fibonacci_loop(n):\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    if n == 1:\n",
    "        return 1\n",
    "\n",
    "    prev_prev_fib = 0\n",
    "    prev_fib = 1\n",
    "    for i in range(2, n + 1):\n",
    "        fib = prev_fib + prev_prev_fib\n",
    "        prev_prev_fib = prev_fib\n",
    "        prev_fib = fib\n",
    "    return fib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 1 µs, total: 11 µs\n",
      "Wall time: 13.1 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9227465"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time fibonacci_loop(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now a $O(n)$ time (`for` loop) and $O(1)$ space (three variables) algorithm.\n",
    "\n",
    "The loop solution not only uses less memory than the memoization-based solution, but is actually also a bit faster in practice (as no time is needed for setting up the cache data structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some ideas for what algorithmic improvements may entail on example of two basic computing problems: sorting and searching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection sort\n",
    "\n",
    "A simple method to sort a list of numbers is the [selection sort algorithm](https://en.wikipedia.org/wiki/Selection_sort):\n",
    "\n",
    "<table>\n",
    "    <tbody>\n",
    "    <tr>\n",
    "        <td style=\"font-size:120%; vertical-align:top; horizontal-align:left; width: 450px\">\n",
    "            <ol>\n",
    "                <li>\n",
    "                    <p>find the smallest number starting at position 0 and swap this with the entry at position 0.</p>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <p>find the smallest number starting at position 1 and swap this with the entry at position 1.</p>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <p>find the smallest number starting at position 2 and swap this with the entry at position 2.</p>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <p>...</p>\n",
    "                </li>\n",
    "            </ol>\n",
    "        </td>\n",
    "        <td style=\"width: 300px;\">\n",
    "            <center>\n",
    "                <img src=\"imgs/selection_sort.jpg\" />\n",
    "                <sub>Source: <a href=\"https://stackoverflow.com/questions/36700830/selection-sort-algorithm\">https://stackoverflow.com/questions/36700830/selection-sort-algorithm</a></sub>\n",
    "            </center>\n",
    "        </td>\n",
    "    </tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 5, 14, 4, 9, 13, 15, 18, 6, 12, 17, 10, 1, 11, 2, 16, 7, 8, 0, 3]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def selection_sort(data):\n",
    "    # as described above\n",
    "    for starting_position in range(len(data) - 1):\n",
    "        index = find_position(data, starting_position)\n",
    "        swap(data, index, starting_position)\n",
    "\n",
    "\n",
    "def find_position(data, starting_position):\n",
    "    # we start with assuming that the value at best_idx is the smallest\n",
    "    # value\n",
    "    best_idx = starting_position\n",
    "    smallest_value = data[starting_position]\n",
    "\n",
    "    # ... and the we iterate over the rest of the list updating best_idx:\n",
    "    for idx in range(starting_position + 1, len(data)):\n",
    "        if data[idx] < smallest_value:\n",
    "            smallest_value = data[idx]\n",
    "            best_idx = idx\n",
    "    return best_idx\n",
    "\n",
    "\n",
    "def swap(data, index_1, index_2):\n",
    "    data[index_1], data[index_2] = data[index_2], data[index_1]\n",
    "\n",
    "\n",
    "random.seed(42)  # for reproducibility\n",
    "\n",
    "data = list(range(20))\n",
    "random.shuffle(data)\n",
    "print(data)\n",
    "\n",
    "selection_sort(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 120%\"><strong>Runtime analysis</strong><sup>*</sup></p>\n",
    "\n",
    "\n",
    "The runtime complexity analysis for this algorithm applied to a list of length $n$ is as follows:\n",
    "\n",
    "We count the operations in each step which depend on $n$ and name the remaining number of operations per step as $n_0$ (e.g. array access, increasing loop counter, swap).\n",
    "\n",
    "- The first step needs $n - 1$ comparisons and $n_0$ other operations\n",
    "- The second step needs $n - 2$ comparisons and $n_0$ other operations\n",
    "- The third step needs $n - 3$ comparisons and $n_0$ other operations\n",
    "- ...\n",
    "-  The $n-1$st step needs $1$ comparisons and $n_0$ other operations.\n",
    "\n",
    "This is in total\n",
    "\n",
    "$$\n",
    "   (n - 1 + n_0)  + (n - 2 + n_0) + \\ldots + (1 + n_0)\n",
    "$$\n",
    "\n",
    "The number of terms in parenthesis is $n-1$ and separating $n_0$ plus rearranging terms leads to\n",
    "\n",
    "$$\n",
    "(n - 1) + (n - 2) + \\ldots + 1 + \\,\\,\\, (n-1) n_0 \n",
    "$$\n",
    "\n",
    "Using some math for summing up the first $n - 1$ natural numbers this is the same as\n",
    "\n",
    "$$\n",
    "\\frac{n (n - 1)}{2} + (n-1) n_0 = \\frac{n^2}{2} + \\frac{n - 1}{2} \\left(1 + 2 n_0 \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "We can ignore the second summand $\\frac{n - 1}{2} \\left(1 + 2 n_0 \\right)$ which is linear in $n$ and grows slower than $n^2$,\n",
    "\n",
    "we further assume that each  operation (depending if it is comparison, swap, ...) has a runtime between $t_0$ and $t_1$ and thus the total runtime $T(n)$ is bounded by\n",
    "\n",
    "$$\n",
    "t_0 \\left\\{ \\frac{n^2}{2} + \\ldots \\right\\} \\le T(n) \\le t_1 \\left\\{ \\frac{n^2}{2} + \\ldots \\right\\}\n",
    "$$\n",
    "\n",
    "Since we omit constants and lower order terms in the $O$ notation we conclude that \n",
    "$$T(n) = O(n^2)$$ for the runtime of selection sort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes we can! Practically important sorting algorithms have a runtime complexity $O(n \\log  n)$ which grows **much slower** than $O(n^2)$. \n",
    "\n",
    "**Note**: $\\log n$ is proportional to the number of digits of $n$ and thus $O(n \\log n)$ is often said to be \"almost linear\".\n",
    "\n",
    "Known standard algorithms from this class are\n",
    "\n",
    "- [Merge sort](https://en.wikipedia.org/wiki/Merge_sort) is a [stable](https://en.wikipedia.org/wiki/Sorting_algorithm#Stability) sorting algorithm, with $O(n \\log n)$ runtime and $O(n)$ memory requirements.</br>\n",
    "  It can sort efficiently data which does fit into memory. Conceptually, you divide data into smaller sub-problems until you reach single elements, which are sorted, and then re-assemble sorted lists by zipping them together simultaneously.\n",
    "\n",
    "<table>\n",
    "<tr><td><img src=\"imgs/merge_sort.png\" width=\"600px\"></td></tr>\n",
    "<tr><td><center><sub>Source: <a href=\"https://en.wikipedia.org/wiki/Merge_sort\">https://en.wikipedia.org/wiki/Merge_sort</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "- [Quick sort](https://en.wikipedia.org/wiki/Quicksort) is not stable, with $O(n \\log n)$ runtime **on average** and $O(\\log n)$ memory requirements.</br>\n",
    "  In corner cases such as presorted data or reversed presorted data the runtime can degrade to $O(n^2)$. Nevertheless quick sort is used often in practice due to its lower memory requirement and lower (hidden) constants in the runtime complexity compared to merge sort. Efficient implementations of quicksort are among the fastest sorting algorithms in practice. \n",
    "- [Timsort](https://en.wikipedia.org/wiki/Timsort)  is stable, with $O(n \\log n)$ worst-case runtime and $O(n)$ worst-case memory requirement.</br>\n",
    "  It is a hybrid algorithm, derived from merge sort and insertion sort ($O(n^2)$ algorithm), designed to perform well in practice.\n",
    "  It has small hidden complexity constants and is fast on naturally occurring (partially) pre-sorted data.</br>\n",
    "  **This is the sorting algorithm used in Python** (since version 2.3 and became meanwhile also the sorting algorithm used in Java and some other programming languages).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selection sort of list of len 1000 took 0.05 seconds\n",
      "selection sort of list of len 2000 took 0.19 seconds\n",
      "selection sort of list of len 4000 took 0.58 seconds\n",
      "\n",
      "timsort of list of len 100000 took 0.03 seconds\n",
      "timsort of list of len 200000 took 0.06 seconds\n",
      "timsort of list of len 400000 took 0.17 seconds\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "k = 500_000\n",
    "\n",
    "random.seed(42)  # for reproducibility\n",
    "\n",
    "for n in 1_000, 2_000, 4_000:\n",
    "    l = [random.randint(0, k) for i in range(n)]\n",
    "\n",
    "    t = time.time()\n",
    "    selection_sort(l)\n",
    "    needed = time.time() - t\n",
    "    print(f\"selection sort of list of len {n} took {needed:.2f} seconds\")\n",
    "\n",
    "print()\n",
    "\n",
    "for n in 100_000, 200_000, 400_000:\n",
    "    l = [random.randint(0, k) for i in range(n)]\n",
    "\n",
    "    t = time.time()\n",
    "    l = sorted(l)\n",
    "    needed = time.time() - t\n",
    "    print(f\"timsort of list of len {n} took {needed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special sorting algorithms\n",
    "\n",
    "One can prove that a **sorting algorithm which is based on comparing items can not be faster than $O(n \\log n)$**.\n",
    "\n",
    "**But**: can you sort items without comparing them?\n",
    "\n",
    "Yes, under some extra assumptions, you actually can sort items without comparing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 120%\">Example: <a href=\"https://en.wikipedia.org/wiki/Counting_sort\"><strong>Counting sort</strong></a></p>\n",
    "\n",
    "Counting sort algorithm **assumes that the items to sort belong to a limited set $S$ of $N$ items with known order**; e.g. the numbers $0 \\ldots N -1$. If the maximum item is not given, you can always first find it in $O(n)$ time.\n",
    "\n",
    "We continue to show case how this sort works using numbers.\n",
    "\n",
    "The idea is as follows:\n",
    "\n",
    "1. Initialise array of counts for every item in $S$ with `0` values.\n",
    "1. Run once over the input numbers increasing count for every seen item.\n",
    "2. Iterate over the $S$ counts array, from smallest to largest number, and append each number to the result as many times as it was seen in the input.\n",
    "\n",
    "E.g the list `[3, 2, 1, 0, 1, 3, 5, 1, 2, 3]` would result in the following counts and output lists:\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/algorithms-counting_sort.png\" width=\"500px\"></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 2, 2, 3, 3, 3, 5]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assume domain S is integer numbers\n",
    "def counting_sort(numbers):\n",
    "    # find N = max(S)\n",
    "    # O(n) time, O(1) space\n",
    "    N = max(numbers) + 1\n",
    "\n",
    "    # count items\n",
    "    # O(n) time, O(N) space\n",
    "    counts = [0] * N\n",
    "    for number in numbers:\n",
    "        counts[number] += 1\n",
    "\n",
    "    # output items\n",
    "    # O(N) time, O(n) space\n",
    "    result = []\n",
    "    for i in range(N):\n",
    "        result.extend([i] * counts[i])\n",
    "\n",
    "    return result\n",
    "\n",
    "    # total: O(n + N) time, O(n + N) memory\n",
    "\n",
    "\n",
    "numbers = [3, 2, 1, 0, 1, 3, 5, 1, 2, 3]\n",
    "\n",
    "counting_sort(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runtime complexity of this algorithm is $O(n + N)$ with a \"hard\" $O(n + N)$ memory requirement. This is only practical with a relatively small $N$;\n",
    "\n",
    "Note: for numbers we actually re-create each item and we don't care that the returned items are copies of the input items; if it's required to return input items, instead of incrementing item counts we can stack items instead (making the counts list occupy $O(n + N)$ memory).\n",
    "\n",
    "Take-home notes:\n",
    "\n",
    "* We **leverage additional knowledge** of the sort domain to get the $O(n)$ performance, at a cost of a potentially much bigger space requirement.\n",
    "* For problems with an already \"fast\" algorithms available, quite often there is a **trade-off between space and runtime**, i.e. reducing runtime requires additional space, and vice-versa.\n",
    "\n",
    "Other notable sorting algorithms which do not rely on comparisons are [bucket sort](https://en.wikipedia.org/wiki/Bucket_sort) and [radix sort](https://en.wikipedia.org/wiki/Radix_sort).\n",
    "\n",
    "All these algorithms do not work on general data and can include a significant memory overhead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### How to sort in Python?\n",
    "\n",
    "The previous explanations served the purpose to introduce runtime analysis and some sorting basics. In practice you should use available sorting algorithms:\n",
    "\n",
    "1. `sorted()` or `list.sort()` for sorting arbitrary data in Python; see [Python Documentation HOWTO on sorting](https://docs.python.org/3/howto/sorting.html#sortinghowto).\n",
    "2. `numpy.sort()` for numerical data, which also offers $O(n)$ radix sort for integers (see [`numpy.sort` documentation](https://numpy.org/doc/stable/reference/generated/numpy.sort.html)).\n",
    "\n",
    "Implementing your own sorting algorithm can be fun and insightful but you can not expect that your result will outperform these implementations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching\n",
    "\n",
    "Searching algorithms can be divided into exact and approximate matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact searching\n",
    "\n",
    "Python offers several methods to find an element in a given data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List lookup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "words = [\"ab\", \"def\", \"ghi\", \"xyz\"]\n",
    "\n",
    "print(\"ghi\" in words)\n",
    "print(words.index(\"ghi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both operations are $O(n)$ if $n$ is the size of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary lookup**\n",
    "\n",
    "In case the items in our collection are unique (no duplicates) we can search faster.\n",
    "\n",
    "Either we reduce lookup time by using a dictionary (assuming elements are immutable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "words_indices = {}\n",
    "for position, word in enumerate(words):\n",
    "    words_indices[word] = position\n",
    "\n",
    "print(\"ghi\" in words_indices.keys())\n",
    "print(words_indices[\"ghi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the preprocessing time is $O(n)$ (with $O(n)$ extra memory), but all subsequent lookups are $O(1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set lookup**\n",
    "\n",
    "In case you only want to check for membership, you can also use a set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "words_unique = set(words)\n",
    "print(\"ghi\" in words_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives also $O(1)$ lookup, but has lower memory requirements than a dictionary (by a constant factor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary search**\n",
    "\n",
    "In case your data set is sorted you can use binary search from the `bisect` module from the standard library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import bisect\n",
    "\n",
    "words = sorted(words)\n",
    "\n",
    "\n",
    "def get_index_of(element, sorted_sequence):\n",
    "    i = bisect.bisect_left(sorted_sequence, element)\n",
    "    # all sequence elements in positions:\n",
    "    #     < i are smaller\n",
    "    #    >= i are greater or equal\n",
    "    if i >= len(sorted_sequence) or sorted_sequence[i] != element:\n",
    "        raise ValueError(f'\"{element}\" not found')\n",
    "    return i\n",
    "\n",
    "\n",
    "print(get_index_of(\"ghi\", words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary search algorithm implements the way we would naively search in a phone book or encyclopedia:\n",
    "\n",
    "1. look in the middle,\n",
    "2. if the entry at this position is the entry we are looking for we are done;\n",
    "3. otherwise, if the entry at this position is \"smaller\" than the entry we are looking for, we restrict search to the 2nd half and repeat from 1. with a \"reduced\" collection.\n",
    "4. else, we restrict search to the 1st half and repeat from 1. with this \"reduced\" collection.\n",
    "\n",
    "<table>\n",
    "    <tr><td><img src=\"imgs/algorithms-binary_search.png\" width=\"800px\"></td></tr>\n",
    "</table>\n",
    "\n",
    "Notes:\n",
    "* if the item was not found you end up with a position where the number should have been, if it would be present in the input;\n",
    "* in implementation we do not actually \"reduce the data set\" - we only narrow down lower or upper index to indicate section of the data that is to be used in a recursive call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For run-time analysis we can establish the relation $T(n) = T(n / 2) +  c$ because we can reduce the data set \"virtually\" by a factor of 2 in every step, having some fixed cost $c$ in every iteration. Using the so called [master theorem](https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)) we can conclude that this algorithm has run-time complexity $O(\\log n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "Assuming list of $n$ element as input, this is an exact search summary table sorted by increasing memory requirements (which you can neglect in most situations):\n",
    "\n",
    "| method | runtime &nbsp; &nbsp; | works with duplicates |  requires | returns position |\n",
    "|--------|---------|-----------------------|-----------------|------------------|\n",
    "| list lookup | $O(n)$ | yes |  - | yes |\n",
    "| bisection | $O(\\log n)$ | yes |  data sorting $O(n \\log n)$ | yes |\n",
    "| set | $O(1)$ | no | set construction $O(n)$ | no |\n",
    "| dict| $O(1)$ | no | dict construction $O(n)$ | yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate search\n",
    "\n",
    "Approximate search is used to find elements in a collection which are **close** to a given element. In most cases closeness is measured by an mathematical distance, such as $|a - b$| for numbers or $\\|a - b \\|_p$ ($p = 2$ is euclidean distance) for multi-dimensional data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching numbers\n",
    "\n",
    "**List lookup**\n",
    "\n",
    "Using a list we return numbers within the given proximity tolerance threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.1, 0.2, 0.3, 0.4, 0.0, 0.1, 0.2, 0.3, 0.4]\n"
     ]
    }
   ],
   "source": [
    "numbers = [(i % 5) / 10 for i in range(10)]\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found [0.2, 0.3, 0.2, 0.3] at positions [2, 3, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "def find_approx(numbers, number, tolerance):\n",
    "    \"\"\"find numbers which deviate from `number` by\n",
    "    max distance `tolerance`\"\"\"\n",
    "    positions, matches = [], []\n",
    "    for position, current_number in enumerate(numbers):\n",
    "        if abs(number - current_number) <= tolerance:\n",
    "            positions.append(position)\n",
    "            matches.append(current_number)\n",
    "\n",
    "    return positions, matches\n",
    "\n",
    "\n",
    "positions, matches = find_approx(numbers, 0.25, tolerance=0.1)\n",
    "print(\"found\", matches, \"at positions\", positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime of this approach is $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary search**\n",
    "\n",
    "Using binary search we:\n",
    "\n",
    "1. require the input numbers to be sorted,\n",
    "2. have to adjust the search for the tolerance threshold - we can look for index where minimum `number - tolerance` number would go and return numbers until we get over maximum `number + tolerance` (or vice versa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.1, 0.1, 0.2, 0.2, 0.3, 0.3, 0.4, 0.4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_numbers = sorted(numbers)\n",
    "print(sorted_numbers)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found [0.2, 0.2, 0.3, 0.3] at positions [4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "def find_approx_sorted(sorted_numbers, number, tolerance):\n",
    "    # results will be in the interval\n",
    "    # [number - tolerance, number + tolerance]\n",
    "\n",
    "    # find first element >= number - tolerance:\n",
    "    current_index = bisect.bisect_left(sorted_numbers, number - tolerance)\n",
    "    positions, matches = [], []\n",
    "\n",
    "    # go through subsequent sorted elements while tolerance is kept\n",
    "    while (\n",
    "        current_index < len(sorted_numbers)  # check if not end first!\n",
    "        and abs(sorted_numbers[current_index] - number) <= tolerance\n",
    "    ):\n",
    "        positions.append(current_index)\n",
    "        matches.append(sorted_numbers[current_index])\n",
    "        current_index += 1\n",
    "\n",
    "    return positions, matches\n",
    "\n",
    "\n",
    "positions, matches = find_approx_sorted(sorted_numbers, 0.25, tolerance=0.1)\n",
    "print(\"found\", matches, \"at positions\", positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime of this approach is $O(\\log n + N)$ for sorted data of size $n$ and $N$ matches.\n",
    "\n",
    "In case your data is not sorted from the beginning the complexity of a single search is $O(n \\log n)$ which looks worse than the simple list approach from before. \n",
    "\n",
    "But if you do **many look-ups** $k$ in the same data it can be much faster as data is sorted once; assuming tolerance low enough to make the number of matches in each lookup $N$ negligible, the complexity in multi-lookup case is $O(n \\log n + k \\log n)$, which is linear in $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching n-dimensional data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate search in $n$-dimensional euclidean spaces is a common task in some fields such as machine learning (e.g. the [kNN classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)), [geographic information systems (GIS)](https://en.wikipedia.org/wiki/Geographic_information_system), [computer graphics](https://en.wikipedia.org/wiki/Space_partitioning#In_computer_graphics) and [computer games](https://en.wikipedia.org/wiki/Collision_detection#Spatial_partitioning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard algorithms for n-dimensional point search boil down to building and using special data structures, analogously to dictionary-based exact search of an element's position.\n",
    "\n",
    "The main data structures from this domain are so called [k-d trees](https://en.wikipedia.org/wiki/K-d_tree).\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr><td><img  src=\"imgs/3dtree.png\" width=\"300\" ></td></tr>\n",
    "        <tr><td><center><sub>Source: <a href=\"https://en.wikipedia.org/wiki/K-d_tree\">https://en.wikipedia.org/wiki/K-d_tree</a></sub></center></td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For $n$ data points, which are randomly distributed $k$-d trees offer nearest point lookup in $O(\\log n)$ time and an (axis-parallel) range lookup returning $m$ points in $O\\left(n^{1−\\frac{1}{k}} + m\\right)$ time. Performance **degrades in higher dimensions** and for some particular data distributions. General runtime analysis is difficult.\n",
    "\n",
    "Such data structures also require time to build, e.g. from a Python list or from numpy arrays. Depending on the implementation setting up a $k$-d tree takes $O(n \\log n)$ or $O(n \\log^2 n)$ time: [see also here](https://en.wikipedia.org/wiki/K-d_tree#Complexity).\n",
    "\n",
    "Python implementations of $k$-d trees can be found\n",
    "- in `scipy`: [scipy.spatial.cKDTrees](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.cKDTree.html#scipy-spatial-ckdtree), this implementation also supports multiple core computations for many lookups and matching two data sets for finding pairs of close points.\n",
    "- in `scikit-learn`: [sklearn.neighbors.KDTree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching points\n",
      "[[0.43194502 0.29122914 0.61185289]\n",
      " [0.66252228 0.31171108 0.52006802]\n",
      " [0.50263709 0.57690388 0.49251769]\n",
      " [0.54873379 0.6918952  0.65196126]]\n",
      "actual distances\n",
      "[0.24643016 0.24953745 0.07731201 0.2495816 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# matrix of 100 points in 3d space\n",
    "np.random.seed(42)  # for reproducibility\n",
    "points = np.random.random(size=(100, 3))\n",
    "\n",
    "tree = cKDTree(points)\n",
    "\n",
    "# find points around center which max distance 0.25\n",
    "center = np.array([0.5, 0.5, 0.5])\n",
    "rows = tree.query_ball_point(center, r=0.25)\n",
    "\n",
    "print(\"matching points\")\n",
    "print(points[rows])\n",
    "\n",
    "print(\"actual distances\")\n",
    "print(np.sum((points[rows] - center) ** 2, axis=1) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Coding exercise [15 min]\n",
    "\n",
    "Compute intersection of two lists of integers. Each element of the intersection must appear as many times as it shows in both arrays and order of elements in the results does not matter. For instance:\n",
    "```python\n",
    "ints1 = [1, 2, 2, 1, 3]\n",
    "ints2 = [2, 3, 2]\n",
    "\n",
    "intersect_ints(ints1, ints2)\n",
    "```\n",
    "can return either of:\n",
    "```python\n",
    "[2, 2, 3]\n",
    "[2, 3, 2]\n",
    "[3, 2, 2]\n",
    "```\n",
    "\n",
    "What is the time and space complexity of your solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 3]\n",
      "[2, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def intersect_ints(ints1, ints2):\n",
    "    out = []\n",
    "\n",
    "    # Ensure: n1 <= n2, where n1 = len(ints1), n2 = len(ints2)\n",
    "    # O(1)\n",
    "    ints_short, ints_long = (\n",
    "        (ints1, ints2) if len(ints1) < len(ints2) else (ints2, ints1)\n",
    "    )\n",
    "\n",
    "    # Build counter\n",
    "    # O(n1) time, O(n1) space\n",
    "    seen = Counter(ints_short)\n",
    "\n",
    "    # Check bigger list for elements, updating counter\n",
    "    # O(n2) time, O(n1) space (output)\n",
    "    for num in ints_long:\n",
    "        if seen.get(num, 0) > 0:  # O(1)\n",
    "            out.append(num)  # average O(1); we could also pre-alloc n1 mem\n",
    "            seen[num] -= 1  # O(1)\n",
    "\n",
    "    return out\n",
    "\n",
    "    # total: O(n_max) time, O(n_min) space\n",
    "\n",
    "\n",
    "# Bonus: sorting input to simplify problem is usually a good approach;\n",
    "#        here, it is not only slower but also a bit more complex to code\n",
    "def intersect_ints_sort(ints1, ints2):\n",
    "\n",
    "    # sorting: O(n log n), O(1) space (in-place)\n",
    "    ints1.sort()\n",
    "    ints2.sort()\n",
    "\n",
    "    # \"merge\" sorted lists:\n",
    "    #     O(n_max) time => worst-case: all element of longer list are smaller\n",
    "    #     O(n_min) space (output) => worst-case: smaller list is a subset of longer list\n",
    "    n1, n2 = len(ints1), len(ints2)\n",
    "    i1, i2 = 0, 0\n",
    "    out = []\n",
    "    while i1 < n1 and i2 < n2:\n",
    "        num1, num2 = ints1[i1], ints2[i2]\n",
    "        if num1 < num2:\n",
    "            i1 += 1\n",
    "        elif num2 < num1:\n",
    "            i2 += 1\n",
    "        else:  # equal\n",
    "            out.append(num1)\n",
    "            i1 += 1\n",
    "            i2 += 1\n",
    "    return out\n",
    "\n",
    "    # total: O(n_max log(n_max)) time, O(n_min) space\n",
    "    #\n",
    "    # Note: O(n_max) time, O(n_min) space if input lists are sorted\n",
    "    #       => same as more general counter-based solution\n",
    "\n",
    "\n",
    "ints1 = [1, 2, 2, 1, 3]\n",
    "ints2 = [2, 3, 2]\n",
    "\n",
    "print(intersect_ints(ints1, ints2))\n",
    "print(intersect_ints_sort(ints1, ints2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other data structures\n",
    "\n",
    "We've already mentioned some either Python-specific or more abstract data structures, like Python dictionaries (hash tables) or $k$-d tree. There are many more - cf. [Wikipedia's list of data structures](https://en.wikipedia.org/wiki/List_of_data_structures). We won't cover them here, but it's highly recommended to get familiar with at least few additional basic and common ones data structures:\n",
    "* [Stacks](https://en.wikipedia.org/wiki/Stack_(abstract_data_type)) and [queues](https://en.wikipedia.org/wiki/Queue_(abstract_data_type)),\n",
    "  * cf. [`collections.deque`](https://docs.python.org/3/library/collections.html#collections.deque),\n",
    "* [Trees](https://en.wikipedia.org/wiki/Tree_(data_structure)),\n",
    "  * ([self-balancing](https://en.wikipedia.org/wiki/Self-balancing_binary_search_tree)) [binary search tree](https://en.wikipedia.org/wiki/Binary_search_tree),\n",
    "  * [B-tree](https://en.wikipedia.org/wiki/B-tree),\n",
    "  * [heap](https://en.wikipedia.org/wiki/Heap_(data_structure)),\n",
    "* [Graphs](https://en.wikipedia.org/wiki/Graph_(abstract_data_type)) (adjacency list, adjacency matrix)\n",
    "  * cf. [NetworkX package](https://networkx.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other algorithms\n",
    "\n",
    "We recommend getting familiar with some general **[algorithm design paradigms](https://en.wikipedia.org/wiki/Algorithmic_paradigm)**, that help developing own algorithms; some important and common ones are:\n",
    "* **[Divide-and-conquer algorithms](https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm)**, where you split recursively your problem into independent smaller  problems,\n",
    "    * like Merge sort;\n",
    "* **[Dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming)**, where you also split your problem into same but smaller problems; the difference to divide-and-conquer is that your sub-problems overlap so you need to build-up the final solution,\n",
    "    * like computing the Fibonacci's number;\n",
    "* **[Backtracking](https://en.wikipedia.org/wiki/Backtracking)**, where you \"track\" history of your algorithm steps and return to a previous \"branching\" step (\"backtrack\") as soon as you find the there is no solution in the current branch,\n",
    "    * like in solving Sudoku by picking one (e.g. lowest) of the many remaining alternative numbers, or\n",
    "    * like in a [depth-first search in a graph](https://en.wikipedia.org/wiki/Depth-first_search).\n",
    "* ...\n",
    "\n",
    "The field of algorithms is vast, [wikipedia offers this overview](https://en.wikipedia.org/wiki/List_of_algorithms).\n",
    "\n",
    "Some fields to mention are:\n",
    "\n",
    "- [Graph algorithms](https://en.wikipedia.org/wiki/Travelling_salesman_problem). E.g. [topological sort](https://en.wikipedia.org/wiki/Topological_sorting) is used in workflow engines (e.g. [snakemake](https://snakemake.github.io)) to figure out the order of steps to execute based on their dependencies. The [Traveling salesman problem](https://en.wikipedia.org/wiki/Travelling_salesman_problem) also requires a graph, and the algorithms named [A*](https://en.wikipedia.org/wiki/A*_search_algorithm) and [D*](https://en.wikipedia.org/wiki/D*) have been widely used, resp., in games and in mobile robots and autonomous vehicle navigation. [Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm) finds the shorted path between nodes in a graph, e.g. in navigation systems.\n",
    "- [String search algorithms](https://en.wikipedia.org/wiki/String-searching_algorithm) to find substrings in a (long) string, e.g. the [Boyer-Moore algorithm](https://en.wikipedia.org/wiki/Boyer–Moore_string-search_algorithm)\n",
    "- [Approximate string matching algorithms](https://en.wikipedia.org/wiki/Approximate_string_matching) find strings that match a pattern approximately (rather than exactly) e.g. to compensate for typos.\n",
    "- [Sequence alignment](https://en.wikipedia.org/wiki/Sequence_alignment) algorithms, most known are the [Needleman-Wunsch](https://en.wikipedia.org/wiki/Needleman–Wunsch_algorithm) and [Swith-Waternam](https://en.wikipedia.org/wiki/Smith–Waterman_algorithm#Explanation) algorithms.\n",
    "- [Numerical algorithms](https://en.wikipedia.org/wiki/Numerical_analysis#Areas_of_study) for numerical computations such as curve fitting, solving linear equations, ...\n",
    "- [Optimization algorithms](https://en.wikipedia.org/wiki/Mathematical_optimization) (may overlap with numerical algorithms), e.g. [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent), the [Simplex algorithm](https://en.wikipedia.org/wiki/Simplex_algorithm) or the [BFGS method](https://en.wikipedia.org/wiki/Broyden–Fletcher–Goldfarb–Shanno_algorithm).\n",
    "- [Combinatorical optimisation algorithms](https://en.wikipedia.org/wiki/Combinatorial_optimization) to solve the [Knapsack](https://en.wikipedia.org/wiki/Knapsack_problem) or [Traveling salesman](https://en.wikipedia.org/wiki/Travelling_salesman_problem) problems. Many of these problems require $O(2^n)$ (**WHICH IS VERY VERY BAD**) operations and can be solved pragmatically if approximate solutions are acceptable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dive into some common techniques of reducing memory usage in scientific computing.\n",
    "\n",
    "It's gonna be convenient to measure memory use of a single object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Measuring memory consumption of an object**\n",
    "\n",
    "In addition to `memory-profile` (FIXME: add link) introduced in Section 2 one can measure the size of a specific object.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p><i class=\"fa fa-warning\"></i>&nbsp;\n",
    "       <strong>Beware</strong>:\n",
    "        The standard library offers a function <code>sys.getsizeof</code>, which you should avoid, whenever possible; see: <a href=\"https://nedbatchelder.com/blog/202002/sysgetsizeof_is_not_what_you_want.html\"><em><code>sys.getsizeof</code> is not what you want</em></a>.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "Instead, we can use the external package `pympler` to measure (approximate) size, in bytes, of an object in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "32\n",
      "136\n"
     ]
    }
   ],
   "source": [
    "from pympler.asizeof import asizeof\n",
    "\n",
    "print(asizeof([]))\n",
    "print(asizeof(1))\n",
    "print(asizeof([1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse arrays "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **[sparse matrix or sparse array](https://en.wikipedia.org/wiki/Sparse_matrix)** is a matrix in which most of the elements are zero. The opposite of this is called a **dense matrix / array**.\n",
    "\n",
    "So instead of storing all entries of a matrix it can be more efficient to **store only the positions of the nonzero entries and the corresponding nonzero values**. E.g.\n",
    "\n",
    "`[0, 0, 0, 0, 1.25, 0, 3.4, 0]` could be represented as `[(4, 1.25), (6, 3.4)]`.\n",
    "\n",
    "This technique not only reduces memory usage but can speed up mathematical operations like matrix-vector, matrix-matrix of vector-vector products significantly. E.g. adding up the entries of a sparse vector is $O(n_*)$ with $n_*$ is the number of nonzero entries, compared to $O(n)$ for a dense vector.\n",
    "\n",
    "\n",
    "For some special sparse matrices also [very fast solvers for linear systems are known](https://en.wikipedia.org/wiki/Conjugate_gradient_method).\n",
    "\n",
    "Sparse matrices play a crucial role in the [numerical solution of partial-differential equations](https://en.wikipedia.org/wiki/Finite_element_method), which are used in many simulations e.g. weather forecasts or virtual crash tests. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse vectors and matrices in `scipy`\n",
    "\n",
    "`scipy` offers [different storage formats for sparse matrices](https://docs.scipy.org/doc/scipy/reference/sparse.html), e.g. the [CSR (Compressed Sparse Row matrix)](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)) format. \n",
    "\n",
    "These formats can be used for vectors and matrices, but not for multi dimensional arrays with dimensions $\\gt$ 2.\n",
    "\n",
    "[scipy.sparse.linalg](https://docs.scipy.org/doc/scipy/reference/sparse.linalg.html#module-scipy.sparse.linalg) also offers special implementations of many common numerical algorithms optimized for sparse matrices, such as computing eigenvectors or solving linear systems.\n",
    "\n",
    "Here we showcase how to use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       numpy array size: 3336\n",
      "scipy sparse array size: 1472\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pympler.asizeof import asizeof\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# create a matrix with only few random nonzero entries\n",
    "np.random.seed(43)\n",
    "array = np.zeros((20, 20))\n",
    "for i in range(13):\n",
    "    row, col = np.random.randint(0, 20, size=(2,))\n",
    "    array[row, col] = 100.0 + i\n",
    "\n",
    "print()\n",
    "print(\"       numpy array size:\", asizeof(array))\n",
    "\n",
    "# convert matrix to a compressed sparse row (csr) matrix\n",
    "scipy_sparse_array = csr_matrix(array)\n",
    "print(\"scipy sparse array size:\", asizeof(scipy_sparse_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the code above constructs sparse matrix from a dense matrix, which still takes \"a lot\" of memory. The proper way to setup a sparse matrix from scratch is a bit more cumbersome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy sparse array size: 1472\n",
      "scipy sparse arrays equal: True\n"
     ]
    }
   ],
   "source": [
    "data, rows, cols = [], [], []\n",
    "\n",
    "# create a compressed sparse row (csr) matrix with only few random nonzero entries\n",
    "# prepare a \"sparse\" list of rows, columns and nonzero values\n",
    "np.random.seed(43)\n",
    "for i in range(13):\n",
    "    row, col = np.random.randint(0, 20, size=(2,))\n",
    "    rows.append(row)\n",
    "    cols.append(col)\n",
    "    data.append(100.0 + i)\n",
    "\n",
    "# create the csr matrix from the list of nonzero entries\n",
    "scipy_sparse_array_2 = csr_matrix(\n",
    "    (np.array(data), (np.array(rows), np.array(cols))), shape=(20, 20)\n",
    ")\n",
    "\n",
    "print(\"scipy sparse array size:\", asizeof(scipy_sparse_array_2))\n",
    "\n",
    "# sanity check for equality\n",
    "# Note: it's cheap to check inequality of sparse matrices\n",
    "#       => check number of nonzero elements (nnz) in inequality sparse matrix\n",
    "print(\n",
    "    \"scipy sparse arrays equal:\", (scipy_sparse_array != scipy_sparse_array_2).nnz == 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: The random seed for construction of the entries in the previous example was carefully chosen to avoid duplicate entries in `rows`, `cols`; otherwise, duplicate entries values are added up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "# place number 1, then number 2 at position (0, 0)\n",
    "data = [1, 2]\n",
    "rows = [0, 0]\n",
    "cols = [0, 0]\n",
    "print(\n",
    "    csr_matrix(\n",
    "        (np.array(data), (np.array(rows), np.array(cols))), shape=(2, 2)\n",
    "    ).todense()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scipy` offers conversion between different sparse formats and from and to dense arrays. Sparse arrays can be used in the same way as dense arrays. \n",
    "\n",
    "`scikit-learn` [also supports sparse matrices to represent features](https://scikit-learn.org/stable/modules/feature_extraction.html#sparsity). E.g. encoding texts using [vector space models](https://en.wikipedia.org/wiki/Vector_space_model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse vectors in `pandas`\n",
    "\n",
    "**Warning**: `pandas` support [sparse vectors (1-dimensional arrays)](https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#sparsearray), but not sparse matrices.\n",
    "\n",
    "Contrary to `scipy.sparse` arrays, the \"common value\", also called a \"fill value\", is not required to be `0`, but can be any arbitrary value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. 42.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0. 42.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 42.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 42.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0. 42.  0.  0.  0.  0.]\n",
      "numpy array size: 8120\n"
     ]
    }
   ],
   "source": [
    "# TODO: rename array => vector\n",
    "array = np.zeros((1000,))\n",
    "array[3::23] = 42  # TODO: simplify (ca. 40 non-zero elements)\n",
    "print(array[:100])\n",
    "print(\"numpy array size:\", asizeof(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas sparse array size: 1208\n"
     ]
    }
   ],
   "source": [
    "pandas_sparse_array = pd.arrays.SparseArray(array, fill_value=0)\n",
    "print(\"pandas sparse array size:\", asizeof(pandas_sparse_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `pandas` only supports sparse vectors, the internal format is also more space efficient compared to `scipy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy sparse array size: 1792\n"
     ]
    }
   ],
   "source": [
    "scipy_sparse_array = csr_matrix(array)\n",
    "print(\"scipy sparse array size:\", asizeof(scipy_sparse_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other options\n",
    "\n",
    "The Python package **[`sparse`](https://sparse.pydata.org/en/latest/)** supports sparse data containers which can be used like `numpy` arrays and supports **arbitrary dimensions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appropriate numerical data type in NumPy\n",
    "\n",
    "We will learn more about `numpy` in the next script, but we anticipate that `numpy` arrays consume much less memory than Python lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list size :       40048\n",
      "numpy array size: 8120\n"
     ]
    }
   ],
   "source": [
    "# TODO: two examples => 1. numpy (array) vs python (list), 2. small ints\n",
    "# If you know that all your numbers are less than 2^16, you can only use one byte per number.\n",
    "\n",
    "python_list = list(range(1000))\n",
    "print(\"list size :      \", asizeof(python_list))\n",
    "\n",
    "np_array = np.array(python_list, dtype=int)\n",
    "print(\"numpy array size:\", asizeof(np_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy supports a much greater variety of numerical types than Python does. You can reduce the amount of memory by using these types.\n",
    "\n",
    "In our previous example the default `int` type takes 8 bytes (64 bit), whereas our numbers are $\\lt$ 1000 and thus can be represented using 10 bytes ($2^{10} = 1024$). The smallest suitable `numpy` type is `np.int16`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy array size: 2120\n"
     ]
    }
   ],
   "source": [
    "np_array = np.array(python_list, dtype=np.int16)\n",
    "print(\"numpy array size:\", asizeof(np_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reduced allocated memory already by a factor or ~4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default floating-point type in `numpy` is `numpy.float64`, thus such a number consumes 64 bit or 8 bytes.\n",
    "\n",
    "Choosing `numpy.float32` instead leads to 50% memory reduction **at the cost of reduced accuracy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few words about floating-point accuracy\n",
    "\n",
    "To store real numbers on a computer with finite memory (infinite memory not invented as of 2021), real numbers need to be approximated using so called [floating-point numbers](https://docs.python.org/3/tutorial/floatingpoint.html). See also [this article from Python documentation](https://docs.python.org/3/tutorial/floatingpoint.html) and [this Wikipedia article](https://en.wikipedia.org/wiki/Floating-point_arithmetic).\n",
    "\n",
    "You can see that e.g. $\\pi$ is approximated, we know that $\\sin(\\pi) = 0$, but:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141592653589793\n",
      "0.00000000000000012246467991473532\n"
     ]
    }
   ],
   "source": [
    "print(np.pi)\n",
    "# print(sin(pi)) with 32 digits after decimap point:\n",
    "print(\"%.32f\" % np.sin(np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number $0.000000000000000012246467991473532$ is close to $0$ but not exactly $0$. So **computations on computers are always approximated computations** which can lead to [serious problems](https://en.wikipedia.org/wiki/Floating-point_arithmetic#Incidents)!\n",
    "\n",
    "The different floating-point formats like `np.float64`, `np.float32` and `np.float16` provide different accuracy in terms of relative errors. \n",
    "\n",
    "Let's call $rd(x)$ the representation of a real number $x$ in a given floating-point format. We define the _**maximum representation relative error $\\epsilon$**_, also named [machine epsilon](https://en.wikipedia.org/wiki/Machine_epsilon) as:\n",
    "$$\n",
    "\\epsilon = \\max_x \\frac{|\\mathrm{rd}(x) - x|}{|x|}\n",
    "$$\n",
    "\n",
    "A _**standard form of a scientific notation**_ is such that there is exactly one non-zero decimal digit before the decimal point. For instance, `12_300_000_000` is written in a standard form of a scientific notation as `1.23e10`; a non-standard form would be, e.g., `123e8`.\n",
    "\n",
    "With _**trusted digits**_ we refer to the number of digits **after the decimal point** in the standard form of a scientific notation, that we can trust to be correct.\n",
    "\n",
    "The maximum representation relative error $\\epsilon$ implies trusted digits value for each floating-point format:\n",
    "\n",
    "| format | $\\epsilon$ | trusted digits |\n",
    "| -- | -- | -- |\n",
    "| `np.float64` | $ 1.11 \\times 10^{-16}$ | 15 |\n",
    "| `np.float32` | $ 5.96 \\times 10^{-8}$ | 7 |\n",
    "| `np.float16` | $ 4.88 \\times 10^{-4}$ | 3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "    <strong>Example</strong>\n",
    "\n",
    "Modern high resolution mass spectrometers produce numbers in the range `0 .. ~1000` with 8 significant digits after the decimal point. In case we store $112.34567123$, which is $1.1234567123 \\times 10^2$, in `float32` format we can trust only the first 7 digits after the decimal point; thus, a computer may not distinguish `112.34567123` and `112.34567` if such values are stored in `np.float32` format.\n",
    "    \n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a = np.float32(112.34567123)\n",
    "b = np.float32(112.34567)\n",
    "print(a == b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: A consequence of approximating real numbers by floating-point numbers is that **you must not compare floating-point numbers exactly** using `==` (unless you know what you are doing). Instead, you should check for floating-point numbers closeness in terms of their relative or absolute difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.sin(np.pi) == 0.0)\n",
    "print(np.isclose(np.sin(np.pi), 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also the documentation of [numpy.isclose](https://numpy.org/doc/stable/reference/generated/numpy.isclose.html#numpy.isclose), [math.isclose](https://docs.python.org/dev/library/math.html#math.isclose) and [numpy.allclose](https://numpy.org/doc/stable/reference/generated/numpy.allclose.html#numpy.allclose)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External memory data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**External memory data processing** or **out of memory processing** is done when data are too large to fit into a computer's main memory; most often, data is kept on a disk and read in blocks fitting main memory.\n",
    "\n",
    "E.g. given a huge text file with one number per line we can determine the largest number without loading all values into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote 2000000 lines\n"
     ]
    }
   ],
   "source": [
    "N = 2_000_000\n",
    "\n",
    "# create text file with \"random\" random numbers\n",
    "with open(\"numbers.txt\", \"w\") as fh:\n",
    "    for i in range(N):\n",
    "        print((i + 1111.1) ** 3 % 99999, file=fh)  # TODO: simplify values\n",
    "print(\"wrote\", N, \"lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15M\tnumbers.txt\n"
     ]
    }
   ],
   "source": [
    "# file size\n",
    "!du -h numbers.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the  largest number it is sufficient to inspect line per line:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest number is 99998.9912109375\n"
     ]
    }
   ],
   "source": [
    "maximum = None\n",
    "\n",
    "with open(\"numbers.txt\") as fh:\n",
    "    for line in fh:\n",
    "        number = float(line)\n",
    "        if maximum is None or number > maximum:\n",
    "            maximum = number\n",
    "\n",
    "print(\"largest number is\", maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` supports reading `csv` or similar data files chunk wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0\n",
      "0  14677.631\n",
      "1  21678.561\n",
      "2  35352.091\n",
      "3  55704.221\n",
      "4  82740.951\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for chunk in pd.read_csv(\"numbers.txt\", header=None, chunksize=5):\n",
    "    print(chunk)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used to determine the largest number by reading chunks of `100_000` lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99998.9912109375\n"
     ]
    }
   ],
   "source": [
    "maximum = None\n",
    "for chunk in pd.read_csv(\"numbers.txt\", header=None, chunksize=100_000):\n",
    "    max_in_chunk = float(chunk.max())\n",
    "    if maximum is None or max_in_chunk > maximum:\n",
    "        maximum = max_in_chunk\n",
    "print(maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember**: external memory data processing is suitable for data that does not fit memory; it's faster to process data in-memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Sometimes you might have to run multiple times over the same file to process external memory data. E.g. to scale a file of numbers to the range 0..1 you \n",
    "   1. First iterate over the data to determine the minimal $x_\\min$ and maximal value $x_\\max$.\n",
    "   2. During a second iteration you replace every $x$ by $\\frac{x - x_\\min}{x_\\max - x_\\min}$  and write this number to an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum 0.0\n",
      "maximum 99998.9912109375\n",
      "rescaling done\n",
      "0.14677779067394975\n",
      "0.21678779692790306\n",
      "0.353524476312283\n",
      "0.5570478294347534\n",
      "0.8274178568982095\n",
      "0.1646944714164191\n",
      "0.5689378493800694\n",
      "0.04020781560903253\n",
      "0.5785645464916112\n",
      "0.18406786685244572\n",
      "CPU times: user 4.47 s, sys: 179 ms, total: 4.65 s\n",
      "Wall time: 4.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "minimum = maximum = None\n",
    "\n",
    "with open(\"numbers.txt\") as fh:\n",
    "    for line in fh:\n",
    "        number = float(line)\n",
    "        if maximum is None or number > maximum:\n",
    "            maximum = number\n",
    "        if minimum is None or number < minimum:\n",
    "            minimum = number\n",
    "\n",
    "print(\"minimum\", minimum)\n",
    "print(\"maximum\", maximum)\n",
    "\n",
    "with open(\"numbers.txt\") as fh_in:\n",
    "    with open(\"numbers_scaled.txt\", \"w\") as fh_out:\n",
    "        for line in fh_in:\n",
    "            number = float(line)\n",
    "            scaled = (number - minimum) / (maximum - minimum)\n",
    "            print(scaled, file=fh_out)\n",
    "\n",
    "print(\"rescaling done\")\n",
    "!head numbers_scaled.txt\n",
    "!rm numbers_scaled.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Coding exercise [15 min]\n",
    "\n",
    "Implement a `pandas` chunk-based variant of the scaling of numbers in a file.\n",
    "\n",
    "Note: a read chunk is a data frame; data frames have `.to_csv()` method with a keyword argument `mode`, which when set to `\"a\"` appends to a given file name or handle. Be careful not to write row names.\n",
    "\n",
    "Try different chunk sizes and compare the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum 0.0\n",
      "maximum 99998.9912109375\n",
      "rescaling done\n",
      "0.14677779067394975\n",
      "0.21678779692790306\n",
      "0.353524476312283\n",
      "0.5570478294347534\n",
      "0.8274178568982095\n",
      "0.1646944714164191\n",
      "0.5689378493800694\n",
      "0.04020781560903253\n",
      "0.5785645464916112\n",
      "0.1840678668524457\n",
      "CPU times: user 4.61 s, sys: 343 ms, total: 4.95 s\n",
      "Wall time: 5.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SOLUTION\n",
    "\n",
    "# Python's `open()` does already buffer I/O reads and writes\n",
    "# even if we use a single read/chunk, the data frames overhead is big\n",
    "n_chunks = 20\n",
    "chunksize = 2_000_000 / n_chunks\n",
    "\n",
    "minimum = maximum = None\n",
    "\n",
    "for chunk in pd.read_csv(\"numbers.txt\", header=None, chunksize=chunksize):\n",
    "    max_in_chunk = float(chunk.max())\n",
    "    min_in_chunk = float(chunk.min())\n",
    "    if maximum is None or max_in_chunk > maximum:\n",
    "        maximum = max_in_chunk\n",
    "    if minimum is None or min_in_chunk < minimum:\n",
    "        minimum = min_in_chunk\n",
    "\n",
    "print(\"minimum\", minimum)\n",
    "print(\"maximum\", maximum)\n",
    "\n",
    "\n",
    "for chunk in pd.read_csv(\"numbers.txt\", header=None, chunksize=chunksize):\n",
    "    scaled_chunk = (chunk - minimum) / (maximum - minimum)\n",
    "    scaled_chunk.to_csv(\"numbers_scaled.txt\", header=None, index=None, mode=\"a\")\n",
    "    # Note: you could keep the file handle open for all writes, but it does not matter\n",
    "    # with open(\"numbers_scaled.txt\", \"w\") as fh_out:\n",
    "    #     chunk.to_csv(fh_out, header=None, mode=\"a\")\n",
    "\n",
    "print(\"rescaling done\")\n",
    "!head numbers_scaled.txt\n",
    "!rm numbers_scaled.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Relation to online algorithms*\n",
    "\n",
    "**Online algorithm** is one that can process its input piece-by-piece in a serial fashion; the entire input is not available from the start, and algorithm takes action as soon as new data pieces arrive (in contrast to so called *streaming algorithms*).\n",
    "\n",
    "**Offline algorithm** is given the whole problem data from the start (and is required to output an answer which solves the whole problem).\n",
    "\n",
    "Online algorithms are considered for real-time/continuously provided data, for instance, for data that are sequentially read from a disk. On the other hand, external memory data processing, in general, considers use of external memory also in parallel, in some structured way, if needed (e.g. queries to databases).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory mapped files\n",
    "\n",
    "A convenient technique to keep data on disk, but still working with the data **as if it is stored in memory** are [memory mapped files](https://en.wikipedia.org/wiki/Memory-mapped_file).\n",
    "\n",
    "`numpy` uses [Python `mmap` standard library module](https://docs.python.org/3/library/mmap.html) to support memory mapped files to support work with an array data stored on a disk.\n",
    "\n",
    "Let's create a large matrix array on a disk in the [NumPy array's binary format `.npy`](https://numpy.org/doc/stable/reference/generated/numpy.lib.format.html#module-numpy.lib.format):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.memmap'>\n",
      "-rw-r--r--  1 uweschmitt  staff   763M Nov 12 12:10 large_file.npy\n",
      "<class 'mmap.mmap'>\n",
      "762.94M\n"
     ]
    }
   ],
   "source": [
    "from numpy.lib.format import open_memmap\n",
    "\n",
    "out_of_mem_array = open_memmap(\n",
    "    \"large_file.npy\",\n",
    "    mode=\"w+\",\n",
    "    dtype=np.float64,\n",
    "    shape=(10_000, 10_000),\n",
    ")\n",
    "\n",
    "print(type(out_of_mem_array))\n",
    "\n",
    "!ls -lh large_file.npy\n",
    "\n",
    "# alternatively, ask the underlying mmap object for the memory-mapped file size:\n",
    "print(type(out_of_mem_array._mmap))\n",
    "print(f\"{out_of_mem_array._mmap.size() / (1024 * 1024):.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `10_000 x 10_000` matrix occupies `763M` on hard drive but consumes almost no RAM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     virtual matrix size: 762.94M\n",
      "numpy.memmap object size: 128 bytes\n"
     ]
    }
   ],
   "source": [
    "print(f\"     virtual matrix size: {out_of_mem_array.nbytes / (1024 * 1024):.2f}M\")\n",
    "# pympler.asizeof does not work correctly w/ numpy.memmap objects => using __sizeof__ built-in\n",
    "print(f\"numpy.memmap object size: {out_of_mem_array.__sizeof__()} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: measuring size of `numpy.memmap` objects is not useful - there is no direct way to monitor how much of the underlying file is actually mapped into the memory.\n",
    "\n",
    "We did not initialize any matrix values. By default, it contains only zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beware: just for a demonstration purpose - np.all triggers mapping of\n",
    "#          the whole array to the memory, which misses the whole point\n",
    "np.all(out_of_mem_array == 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this like a regular matrix, but operations will be slowed down due to I/O overhead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_mem_array[9, 9] = 1.234\n",
    "\n",
    "# stay on the safe side: flush changes to disk after done with writing\n",
    "out_of_mem_array.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us open the file in a read-only mode now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.memmap'>\n",
      "float64\n",
      "(10000, 10000)\n"
     ]
    }
   ],
   "source": [
    "out_of_mem_array = open_memmap(\n",
    "    filename=\"large_file.npy\",\n",
    "    mode=\"r\",\n",
    ")\n",
    "print(type(out_of_mem_array))\n",
    "print(out_of_mem_array.dtype)\n",
    "print(out_of_mem_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* we can equivalently use `np.load(..., mmap_mode=...)`;\n",
    "* using `np.memmap` will save only binary-serialized array, without array metadata header (`shape`, `dtype`) - both constitute the `.npy` binary format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can map chunks of array file into RAM and create regular NumPy array out of it, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    1.234]]\n"
     ]
    }
   ],
   "source": [
    "in_mem_array = np.array(out_of_mem_array[:10, :10])\n",
    "print(type(in_mem_array))\n",
    "print(in_mem_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Side note: in general, for an efficient random access to any type of files, instead of reading file chunks into a buffer, you can use the `mmap` standard library module, getting addressing/indexing-based access to file contents (the `mmap` module is also very efficient when reading whole files into the memory; cf. [`mmap` tutorial](https://realpython.com/python-mmap/)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other file formats\n",
    "\n",
    "- **[HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format)** is a general data format designed to store huge amounts of numerical data as a dictionary of `numpy` arrays.\n",
    "    * The [`h5py` package](https://docs.h5py.org/en/stable/index.html) can be used to work with HDF5 files in Python; [cf. with the `pytables` package for some more advanced HDF5 features](http://www.pytables.org/FAQ.html#how-does-pytables-compare-with-the-h5py-project).\n",
    "- **[Apache Parquet](https://en.wikipedia.org/wiki/Apache_Parquet)** is another efficient column/data frame/table-oriented file format\n",
    "    * Use [Parquet files in Python via the `pyarrow` package](https://arrow.apache.org/docs/python/parquet.html), which integrates with both `numpy` and `pandas`.\n",
    "- **[SQLite](https://www.sqlite.org/index.html)** offers a SQL database in a single file. SQL database enables efficient storage, extraction and transformation of subsets of data from many, possibly related, large tables/datasets.\n",
    "    * The [`sqlite3` standard library module](https://docs.python.org/3/library/sqlite3.html) allows to work with SQLite in Python.\n",
    "    * A [built-in `pandas` support for SQL](https://towardsdatascience.com/python-pandas-and-sqlite-a0e2c052456f) can be used to work with SQL tables as data frames; read *[Fast subsets of large datasets with Pandas and SQLite](https://pythonspeed.com/articles/indexing-pandas-sqlite/)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reads\n",
    "\n",
    "- *[Reducing Pandas memory usage #1: lossless compression](https://pythonspeed.com/articles/pandas-load-less-data/)*\n",
    "- *[Reducing Pandas memory usage #2: lossy compression](https://pythonspeed.com/articles/pandas-reduce-memory-lossy/)*\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python generators](https://docs.python.org/3/howto/functional.html#generators) and [Python generator expressions](https://docs.python.org/3/howto/functional.html#generator-expressions-and-list-comprehensions) can produce a stream of data - an item fater an item on demand.\n",
    "\n",
    "Generator expressions look similar to list comprehensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct full list of first 1000 square numbers\n",
    "direct = [i * i for i in range(1000)]\n",
    "\n",
    "# \"lazy list\" of first 1000 square numbers\n",
    "lazy = (i * i for i in range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both behave similar in many situations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(direct))\n",
    "print(sum(lazy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But memory requirements are different:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators, like all iterators, **can be exhausted**. Unless your generator is infinite, you can iterate through it one time only. Once all values have been evaluated, iteration will stop and the for loop will exit. If you used next(), then instead you’ll get an explicit StopIteration exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"   direct list:\", asizeof(direct))\n",
    "print(\"lazy generator:\", asizeof(lazy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator computes the values **on demand**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = (a ** 2 for a in range(3))\n",
    "for number in generator:\n",
    "    print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But generators can only be used **once**. As soon as you iterated once over all entries, the generator is \"exhausted\". So when we iterate over the previous generator again, nothing happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number in generator:\n",
    "    print(number)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators also don't support `len` or indexed access / slicing using `[...]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement more flexible generators which can not be implemented using generator expressions, you can use the `yield` keyword. This is beyond this course - read [Python documnetation HOWTO on generators](https://docs.python.org/3/howto/functional.html#generators) to learn more about this.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
